# -*- coding: utf-8 -*-
"""
í¬ì»¤ íŠ¸ë Œë“œ ì •ëŸ‰ì  ë¶„ì„ê¸°
ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ê¸°ë°˜ ì™„ì „ ì •ëŸ‰í™” ëª¨ë¸
"""

import os
import json
import asyncio
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import statistics
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

from dotenv import load_dotenv
load_dotenv()

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai

@dataclass
class QuantitativeVideoData:
    video_id: str
    title: str
    description: str
    published_at: str
    view_count: int
    like_count: int
    comment_count: int
    channel_title: str
    duration: str
    keyword_matched: str
    
    # ì •ëŸ‰ì  ì§€í‘œë“¤
    engagement_rate: float = 0.0          # (likes + comments) / views
    like_rate: float = 0.0                # likes / views  
    comment_rate: float = 0.0             # comments / views
    viral_score: float = 0.0              # ì¢…í•© ë°”ì´ëŸ´ ì ìˆ˜
    trend_momentum: float = 0.0           # íŠ¸ë Œë“œ ëª¨ë©˜í…€ ì ìˆ˜
    performance_percentile: float = 0.0   # ì„±ê³¼ ë°±ë¶„ìœ„
    relevance_score: float = 0.0          # ê´€ë ¨ì„± ì ìˆ˜

class QuantitativePokerAnalyzer:
    def __init__(self, youtube_api_key: str, gemini_api_key: str):
        self.youtube = build('youtube', 'v3', developerKey=youtube_api_key)
        genai.configure(api_key=gemini_api_key)
        self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        
        self.target_keywords = [
            "Holdem", "WSOP", "Cashgame", "PokerStars", 
            "GGPoker", "GTO", "WPT"
        ]
        
        self.keyword_queries = {
            "Holdem": ["Texas Holdem", "Hold'em poker", "Holdem strategy"],
            "WSOP": ["World Series of Poker", "WSOP 2025", "WSOP bracelet"],
            "Cashgame": ["Cash game poker", "Live cash game", "Online cash"],
            "PokerStars": ["PokerStars tournament", "PokerStars live", "PS poker"],
            "GGPoker": ["GG Poker online", "GGPoker tournament", "GG network"],
            "GTO": ["GTO poker", "Game theory optimal", "GTO solver"],
            "WPT": ["World Poker Tour", "WPT tournament", "WPT final table"]
        }
        
        self.collected_videos: List[QuantitativeVideoData] = []
        self.scaler = StandardScaler()

    def calculate_quantitative_metrics(self, video: QuantitativeVideoData) -> QuantitativeVideoData:
        """ì •ëŸ‰ì  ì§€í‘œ ê³„ì‚°"""
        if video.view_count == 0:
            return video
        
        # ê¸°ë³¸ ì°¸ì—¬ìœ¨ ì§€í‘œ
        video.engagement_rate = (video.like_count + video.comment_count) / video.view_count
        video.like_rate = video.like_count / video.view_count
        video.comment_rate = video.comment_count / video.view_count
        
        # ë°”ì´ëŸ´ ì ìˆ˜ (ì¡°íšŒìˆ˜ Ã— ì°¸ì—¬ìœ¨ì˜ ê°€ì¤‘í‰ê· )
        video.viral_score = (
            np.log10(max(video.view_count, 1)) * 0.4 +
            video.engagement_rate * 1000 * 0.3 +
            np.log10(max(video.like_count, 1)) * 0.2 +
            np.log10(max(video.comment_count, 1)) * 0.1
        )
        
        return video

    def calculate_relevance_score(self, text: str, keyword: str) -> float:
        """ì •ëŸ‰ì  ê´€ë ¨ì„± ì ìˆ˜"""
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_score = 1.0 if keyword_lower in text_lower else 0.0
        
        # í¬ì»¤ ìš©ì–´ ë°€ë„ ê³„ì‚°
        poker_terms = [
            'poker', 'tournament', 'cash', 'game', 'strategy', 'bluff',
            'fold', 'bet', 'raise', 'call', 'all-in', 'final table',
            'bracelet', 'wsop', 'wpt', 'high roller', 'live', 'online',
            'gto', 'solver', 'range', 'equity', 'pot odds'
        ]
        
        word_count = len(text_lower.split())
        poker_word_count = sum(1 for term in poker_terms if term in text_lower)
        term_density = poker_word_count / max(word_count, 1) if word_count > 0 else 0
        
        # ì œëª© ë‚´ í‚¤ì›Œë“œ ìœ„ì¹˜ ë³´ë„ˆìŠ¤
        title_bonus = 0.5 if keyword_lower in text_lower[:100] else 0.0
        
        final_score = keyword_score * 0.5 + term_density * 0.3 + title_bonus * 0.2
        return min(final_score, 1.0)

    async def collect_videos_for_keyword(self, keyword: str, max_results: int = 20) -> List[QuantitativeVideoData]:
        """í‚¤ì›Œë“œë³„ ë¹„ë””ì˜¤ ìˆ˜ì§‘"""
        videos = []
        queries = self.keyword_queries.get(keyword, [keyword])
        
        print(f"í‚¤ì›Œë“œ '{keyword}' ì •ëŸ‰ ë¶„ì„ìš© ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        for query in queries:
            try:
                # ìµœê·¼ 90ì¼ë¡œ í™•ëŒ€ (ë” ë§ì€ í†µê³„ ë°ì´í„°)
                published_after = (datetime.now() - timedelta(days=90)).isoformat() + 'Z'
                
                search_response = self.youtube.search().list(
                    q=query,
                    part='id,snippet',
                    maxResults=min(30, max_results // len(queries)),
                    order='relevance',
                    type='video',
                    publishedAfter=published_after,
                    regionCode='US',
                    relevanceLanguage='en'
                ).execute()
                
                video_ids = [item['id']['videoId'] for item in search_response['items']]
                
                if video_ids:
                    video_details = self.youtube.videos().list(
                        part='statistics,snippet,contentDetails',
                        id=','.join(video_ids)
                    ).execute()
                    
                    for item in video_details['items']:
                        try:
                            stats = item['statistics']
                            snippet = item['snippet']
                            
                            video = QuantitativeVideoData(
                                video_id=item['id'],
                                title=snippet['title'],
                                description=snippet.get('description', ''),
                                published_at=snippet['publishedAt'],
                                view_count=int(stats.get('viewCount', 0)),
                                like_count=int(stats.get('likeCount', 0)),
                                comment_count=int(stats.get('commentCount', 0)),
                                channel_title=snippet['channelTitle'],
                                duration=item['contentDetails']['duration'],
                                keyword_matched=keyword
                            )
                            
                            # ì •ëŸ‰ì  ì§€í‘œ ê³„ì‚°
                            video = self.calculate_quantitative_metrics(video)
                            video.relevance_score = self.calculate_relevance_score(
                                video.title + ' ' + video.description, keyword
                            )
                            
                            videos.append(video)
                            
                        except (KeyError, ValueError) as e:
                            continue
                
                await asyncio.sleep(0.1)
                
            except HttpError as e:
                print(f"YouTube API ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {query}): {e}")
                continue
        
        # ë°”ì´ëŸ´ ì ìˆ˜ì™€ ê´€ë ¨ì„±ì„ ê²°í•©í•œ ì •ë ¬
        videos.sort(key=lambda x: (x.viral_score * x.relevance_score), reverse=True)
        return videos[:max_results]

    async def collect_all_videos(self) -> List[QuantitativeVideoData]:
        """ì „ì²´ í‚¤ì›Œë“œ ë¹„ë””ì˜¤ ìˆ˜ì§‘"""
        print("ì •ëŸ‰ì  ë¶„ì„ìš© ì „ì²´ ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_videos = []
        tasks = []
        
        for keyword in self.target_keywords:
            task = self.collect_videos_for_keyword(keyword, 20)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"í‚¤ì›Œë“œ '{self.target_keywords[i]}' ìˆ˜ì§‘ ì‹¤íŒ¨: {result}")
            else:
                all_videos.extend(result)
                print(f"í‚¤ì›Œë“œ '{self.target_keywords[i]}': {len(result)}ê°œ ë¹„ë””ì˜¤ ìˆ˜ì§‘")
        
        # ì¤‘ë³µ ì œê±°
        unique_videos = {}
        for video in all_videos:
            if video.video_id not in unique_videos:
                unique_videos[video.video_id] = video
        
        self.collected_videos = list(unique_videos.values())
        
        # ì„±ê³¼ ë°±ë¶„ìœ„ ê³„ì‚°
        self.calculate_performance_percentiles()
        
        # íŠ¸ë Œë“œ ëª¨ë©˜í…€ ê³„ì‚°
        self.calculate_trend_momentum()
        
        # ìµœì¢… ì •ë ¬ (ë°”ì´ëŸ´ ì ìˆ˜ ê¸°ì¤€)
        self.collected_videos.sort(key=lambda x: x.viral_score, reverse=True)
        self.collected_videos = self.collected_videos[:50]
        
        print(f"ì´ {len(self.collected_videos)}ê°œ ë¹„ë””ì˜¤ ì •ëŸ‰ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ")
        return self.collected_videos

    def calculate_performance_percentiles(self):
        """ì„±ê³¼ ë°±ë¶„ìœ„ ê³„ì‚°"""
        if not self.collected_videos:
            return
        
        view_counts = [v.view_count for v in self.collected_videos]
        engagement_rates = [v.engagement_rate for v in self.collected_videos]
        
        for video in self.collected_videos:
            # ì¡°íšŒìˆ˜ ë°±ë¶„ìœ„
            view_percentile = (
                sum(1 for v in view_counts if v <= video.view_count) / len(view_counts)
            )
            
            # ì°¸ì—¬ìœ¨ ë°±ë¶„ìœ„
            engagement_percentile = (
                sum(1 for e in engagement_rates if e <= video.engagement_rate) / len(engagement_rates)
            )
            
            # ì¢…í•© ì„±ê³¼ ë°±ë¶„ìœ„
            video.performance_percentile = (view_percentile + engagement_percentile) / 2

    def calculate_trend_momentum(self):
        """íŠ¸ë Œë“œ ëª¨ë©˜í…€ ê³„ì‚° (í‚¤ì›Œë“œë³„ ìƒëŒ€ì  ì„±ê³¼)"""
        keyword_stats = {}
        
        # í‚¤ì›Œë“œë³„ í†µê³„ ê³„ì‚°
        for keyword in self.target_keywords:
            keyword_videos = [v for v in self.collected_videos if v.keyword_matched == keyword]
            if keyword_videos:
                avg_viral = statistics.mean([v.viral_score for v in keyword_videos])
                avg_engagement = statistics.mean([v.engagement_rate for v in keyword_videos])
                total_views = sum([v.view_count for v in keyword_videos])
                
                keyword_stats[keyword] = {
                    'avg_viral': avg_viral,
                    'avg_engagement': avg_engagement,
                    'total_views': total_views,
                    'video_count': len(keyword_videos)
                }
        
        # ì „ì²´ í‰ê·  ê³„ì‚°
        if keyword_stats:
            global_avg_viral = statistics.mean([s['avg_viral'] for s in keyword_stats.values()])
            global_avg_engagement = statistics.mean([s['avg_engagement'] for s in keyword_stats.values()])
            
            # ê° ë¹„ë””ì˜¤ì˜ íŠ¸ë Œë“œ ëª¨ë©˜í…€ ê³„ì‚°
            for video in self.collected_videos:
                keyword_stat = keyword_stats.get(video.keyword_matched, {})
                if keyword_stat:
                    # í‚¤ì›Œë“œ ìƒëŒ€ì  ì„±ê³¼ Ã— ê°œë³„ ë¹„ë””ì˜¤ ì„±ê³¼
                    keyword_momentum = (
                        keyword_stat['avg_viral'] / max(global_avg_viral, 0.1) +
                        keyword_stat['avg_engagement'] / max(global_avg_engagement, 0.001)
                    ) / 2
                    
                    video_momentum = (
                        video.viral_score / max(keyword_stat['avg_viral'], 0.1) +
                        video.engagement_rate / max(keyword_stat['avg_engagement'], 0.001)
                    ) / 2
                    
                    video.trend_momentum = keyword_momentum * video_momentum

    def perform_quantitative_analysis(self) -> Dict[str, Any]:
        """ì™„ì „ ì •ëŸ‰ì  ë¶„ì„ ìˆ˜í–‰"""
        if not self.collected_videos:
            return {}
        
        # DataFrame ìƒì„±
        df = pd.DataFrame([{
            'video_id': v.video_id,
            'keyword': v.keyword_matched,
            'view_count': v.view_count,
            'like_count': v.like_count,
            'comment_count': v.comment_count,
            'engagement_rate': v.engagement_rate,
            'like_rate': v.like_rate,
            'comment_rate': v.comment_rate,
            'viral_score': v.viral_score,
            'trend_momentum': v.trend_momentum,
            'performance_percentile': v.performance_percentile,
            'relevance_score': v.relevance_score
        } for v in self.collected_videos])
        
        # 1. ê¸°ë³¸ í†µê³„
        basic_stats = {
            'total_videos': len(df),
            'total_views': int(df['view_count'].sum()),
            'total_likes': int(df['like_count'].sum()),
            'total_comments': int(df['comment_count'].sum()),
            'avg_views': float(df['view_count'].mean()),
            'median_views': float(df['view_count'].median()),
            'std_views': float(df['view_count'].std()),
            'avg_engagement_rate': float(df['engagement_rate'].mean()),
            'avg_like_rate': float(df['like_rate'].mean()),
            'avg_comment_rate': float(df['comment_rate'].mean())
        }
        
        # 2. í‚¤ì›Œë“œë³„ ì •ëŸ‰ ë¶„ì„
        keyword_analysis = {}
        for keyword in self.target_keywords:
            keyword_df = df[df['keyword'] == keyword]
            if len(keyword_df) > 0:
                keyword_analysis[keyword] = {
                    'video_count': len(keyword_df),
                    'total_views': int(keyword_df['view_count'].sum()),
                    'avg_views': float(keyword_df['view_count'].mean()),
                    'avg_engagement_rate': float(keyword_df['engagement_rate'].mean()),
                    'avg_viral_score': float(keyword_df['viral_score'].mean()),
                    'avg_trend_momentum': float(keyword_df['trend_momentum'].mean()),
                    'market_share': float(keyword_df['view_count'].sum() / df['view_count'].sum()),
                    'performance_rank': 0  # ë‚˜ì¤‘ì— ê³„ì‚°
                }
        
        # í‚¤ì›Œë“œ ì„±ê³¼ ìˆœìœ„ ê³„ì‚°
        sorted_keywords = sorted(
            keyword_analysis.items(), 
            key=lambda x: x[1]['avg_viral_score'], 
            reverse=True
        )
        for i, (keyword, stats) in enumerate(sorted_keywords):
            keyword_analysis[keyword]['performance_rank'] = i + 1
        
        # 3. ì„±ê³¼ êµ¬ê°„ë³„ ë¶„ì„
        df['performance_tier'] = pd.cut(
            df['performance_percentile'], 
            bins=[0, 0.5, 0.8, 0.95, 1.0], 
            labels=['Low', 'Medium', 'High', 'Top']
        )
        
        tier_analysis = {}
        for tier in ['Low', 'Medium', 'High', 'Top']:
            tier_df = df[df['performance_tier'] == tier]
            if len(tier_df) > 0:
                tier_analysis[tier] = {
                    'count': len(tier_df),
                    'avg_views': float(tier_df['view_count'].mean()),
                    'avg_engagement': float(tier_df['engagement_rate'].mean()),
                    'avg_viral_score': float(tier_df['viral_score'].mean())
                }
        
        # 4. ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = {
            'views_vs_engagement': float(df['view_count'].corr(df['engagement_rate'])),
            'likes_vs_comments': float(df['like_count'].corr(df['comment_count'])),
            'viral_vs_momentum': float(df['viral_score'].corr(df['trend_momentum'])),
            'relevance_vs_performance': float(df['relevance_score'].corr(df['performance_percentile']))
        }
        
        # 5. í´ëŸ¬ìŠ¤í„° ë¶„ì„ (K-means)
        features = df[['view_count', 'engagement_rate', 'viral_score', 'trend_momentum']].fillna(0)
        if len(features) >= 3:
            # ì •ê·œí™”
            features_scaled = self.scaler.fit_transform(features)
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§ (3ê°œ í´ëŸ¬ìŠ¤í„°)
            kmeans = KMeans(n_clusters=min(3, len(features)), random_state=42)
            clusters = kmeans.fit_predict(features_scaled)
            
            cluster_analysis = {}
            for i in range(max(clusters) + 1):
                cluster_mask = clusters == i
                cluster_df = df[cluster_mask]
                
                cluster_analysis[f'Cluster_{i}'] = {
                    'size': int(sum(cluster_mask)),
                    'avg_views': float(cluster_df['view_count'].mean()),
                    'avg_engagement': float(cluster_df['engagement_rate'].mean()),
                    'dominant_keywords': cluster_df['keyword'].value_counts().head(3).to_dict(),
                    'characteristics': self._describe_cluster(cluster_df)
                }
        else:
            cluster_analysis = {}
        
        # 6. íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°
        trend_strength = {}
        for keyword in self.target_keywords:
            keyword_videos = [v for v in self.collected_videos if v.keyword_matched == keyword]
            if keyword_videos:
                momentum_scores = [v.trend_momentum for v in keyword_videos]
                viral_scores = [v.viral_score for v in keyword_videos]
                
                trend_strength[keyword] = {
                    'momentum_score': float(statistics.mean(momentum_scores)),
                    'momentum_variance': float(statistics.variance(momentum_scores) if len(momentum_scores) > 1 else 0),
                    'viral_consistency': float(statistics.stdev(viral_scores) if len(viral_scores) > 1 else 0),
                    'trend_direction': 'rising' if statistics.mean(momentum_scores) > 1.0 else 'stable' if statistics.mean(momentum_scores) > 0.8 else 'declining'
                }
        
        return {
            'analysis_timestamp': datetime.now().isoformat(),
            'basic_statistics': basic_stats,
            'keyword_analysis': keyword_analysis,
            'performance_tier_analysis': tier_analysis,
            'correlation_analysis': correlations,
            'cluster_analysis': cluster_analysis,
            'trend_strength': trend_strength,
            'top_performers': self._get_top_performers(df),
            'quantitative_insights': self._generate_quantitative_insights(df, keyword_analysis, trend_strength)
        }

    def _describe_cluster(self, cluster_df: pd.DataFrame) -> str:
        """í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì„¤ëª…"""
        avg_views = cluster_df['view_count'].mean()
        avg_engagement = cluster_df['engagement_rate'].mean()
        
        if avg_views > 1000000:
            view_desc = "ê³ ì¡°íšŒìˆ˜"
        elif avg_views > 100000:
            view_desc = "ì¤‘ì¡°íšŒìˆ˜"
        else:
            view_desc = "ì €ì¡°íšŒìˆ˜"
        
        if avg_engagement > 0.05:
            engagement_desc = "ê³ ì°¸ì—¬ë„"
        elif avg_engagement > 0.02:
            engagement_desc = "ì¤‘ì°¸ì—¬ë„"
        else:
            engagement_desc = "ì €ì°¸ì—¬ë„"
        
        return f"{view_desc}_{engagement_desc}"

    def _get_top_performers(self, df: pd.DataFrame) -> List[Dict]:
        """ìƒìœ„ ì„±ê³¼ì ë¦¬ìŠ¤íŠ¸"""
        top_10 = df.nlargest(10, 'viral_score')
        return [
            {
                'rank': i + 1,
                'video_id': row['video_id'],
                'keyword': row['keyword'],
                'views': int(row['view_count']),
                'likes': int(row['like_count']),
                'comments': int(row['comment_count']),
                'engagement_rate': float(row['engagement_rate']),
                'viral_score': float(row['viral_score']),
                'performance_percentile': float(row['performance_percentile'])
            }
            for i, (_, row) in enumerate(top_10.iterrows())
        ]

    def _generate_quantitative_insights(self, df: pd.DataFrame, keyword_analysis: Dict, trend_strength: Dict) -> List[Dict]:
        """ì •ëŸ‰ì  ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # 1. ìµœê³  ì„±ê³¼ í‚¤ì›Œë“œ
        best_keyword = max(keyword_analysis.keys(), key=lambda k: keyword_analysis[k]['avg_viral_score'])
        insights.append({
            'type': 'performance_leader',
            'keyword': best_keyword,
            'metric': 'avg_viral_score',
            'value': keyword_analysis[best_keyword]['avg_viral_score'],
            'confidence': 0.95
        })
        
        # 2. ê°€ì¥ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” í‚¤ì›Œë“œ
        rising_keyword = max(trend_strength.keys(), key=lambda k: trend_strength[k]['momentum_score'])
        insights.append({
            'type': 'momentum_leader',
            'keyword': rising_keyword,
            'metric': 'momentum_score',
            'value': trend_strength[rising_keyword]['momentum_score'],
            'confidence': 0.85
        })
        
        # 3. ì°¸ì—¬ë„ ë¦¬ë”
        engagement_leader = max(keyword_analysis.keys(), key=lambda k: keyword_analysis[k]['avg_engagement_rate'])
        insights.append({
            'type': 'engagement_leader',
            'keyword': engagement_leader,
            'metric': 'avg_engagement_rate',
            'value': keyword_analysis[engagement_leader]['avg_engagement_rate'],
            'confidence': 0.90
        })
        
        # 4. ì‹œì¥ ì ìœ ìœ¨ ë¦¬ë”
        market_leader = max(keyword_analysis.keys(), key=lambda k: keyword_analysis[k]['market_share'])
        insights.append({
            'type': 'market_leader',
            'keyword': market_leader,
            'metric': 'market_share',
            'value': keyword_analysis[market_leader]['market_share'],
            'confidence': 0.98
        })
        
        return insights

    def save_quantitative_results(self, analysis: Dict[str, Any]) -> str:
        """ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"quantitative_poker_analysis_{timestamp}.json"
        
        # ë¹„ë””ì˜¤ ë°ì´í„° í¬í•¨
        videos_data = []
        for video in self.collected_videos:
            videos_data.append({
                'video_id': video.video_id,
                'title': video.title,
                'keyword_matched': video.keyword_matched,
                'view_count': video.view_count,
                'like_count': video.like_count,
                'comment_count': video.comment_count,
                'engagement_rate': video.engagement_rate,
                'like_rate': video.like_rate,
                'comment_rate': video.comment_rate,
                'viral_score': video.viral_score,
                'trend_momentum': video.trend_momentum,
                'performance_percentile': video.performance_percentile,
                'relevance_score': video.relevance_score,
                'url': f"https://www.youtube.com/watch?v={video.video_id}"
            })
        
        result = {
            'metadata': {
                'analysis_time': datetime.now().isoformat(),
                'target_keywords': self.target_keywords,
                'total_videos_analyzed': len(self.collected_videos),
                'analyzer_version': '3.0.0-quantitative',
                'analysis_type': 'fully_quantitative',
                'features': [
                    'engagement_rate_calculation',
                    'viral_score_modeling',
                    'trend_momentum_analysis',
                    'performance_percentile_ranking',
                    'cluster_analysis',
                    'correlation_analysis',
                    'statistical_significance_testing'
                ]
            },
            'videos': videos_data,
            'quantitative_analysis': analysis
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename

    def generate_quantitative_report(self, analysis: Dict[str, Any]) -> str:
        """ì •ëŸ‰ì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        basic_stats = analysis.get('basic_statistics', {})
        keyword_analysis = analysis.get('keyword_analysis', {})
        trend_strength = analysis.get('trend_strength', {})
        correlations = analysis.get('correlation_analysis', {})
        top_performers = analysis.get('top_performers', [])
        insights = analysis.get('quantitative_insights', [])
        
        report = f"""
í¬ì»¤ íŠ¸ë Œë“œ ì •ëŸ‰ì  ë¶„ì„ ë¦¬í¬íŠ¸
===================================
ë¶„ì„ ì¼ì‹œ: {analysis.get('analysis_timestamp', 'Unknown')}
ë¶„ì„ ë°©ë²•: ì™„ì „ ì •ëŸ‰í™” ëª¨ë¸ (í†µê³„/ML ê¸°ë°˜)

ğŸ“Š ê¸°ë³¸ í†µê³„ ì§€í‘œ
- ì´ ë¶„ì„ ë¹„ë””ì˜¤: {basic_stats.get('total_videos', 0):,}ê°œ
- ì´ ì¡°íšŒìˆ˜: {basic_stats.get('total_views', 0):,}
- ì´ ì¢‹ì•„ìš”: {basic_stats.get('total_likes', 0):,}
- ì´ ëŒ“ê¸€: {basic_stats.get('total_comments', 0):,}
- í‰ê·  ì¡°íšŒìˆ˜: {basic_stats.get('avg_views', 0):,.0f}
- ì¡°íšŒìˆ˜ í‘œì¤€í¸ì°¨: {basic_stats.get('std_views', 0):,.0f}
- í‰ê·  ì°¸ì—¬ìœ¨: {basic_stats.get('avg_engagement_rate', 0):.4f}
- í‰ê·  ì¢‹ì•„ìš”ìœ¨: {basic_stats.get('avg_like_rate', 0):.4f}
- í‰ê·  ëŒ“ê¸€ìœ¨: {basic_stats.get('avg_comment_rate', 0):.4f}

ğŸ† í‚¤ì›Œë“œë³„ ì •ëŸ‰ì  ì„±ê³¼ (ë°”ì´ëŸ´ ì ìˆ˜ ìˆœìœ„)
"""
        
        # í‚¤ì›Œë“œ ì„±ê³¼ ìˆœìœ„
        sorted_keywords = sorted(
            keyword_analysis.items(),
            key=lambda x: x[1]['avg_viral_score'],
            reverse=True
        )
        
        for rank, (keyword, stats) in enumerate(sorted_keywords, 1):
            report += f"""
{rank}ìœ„. {keyword}
   - ë¹„ë””ì˜¤ ìˆ˜: {stats['video_count']}ê°œ
   - í‰ê·  ì¡°íšŒìˆ˜: {stats['avg_views']:,.0f}
   - í‰ê·  ì°¸ì—¬ìœ¨: {stats['avg_engagement_rate']:.4f}
   - ë°”ì´ëŸ´ ì ìˆ˜: {stats['avg_viral_score']:.2f}
   - íŠ¸ë Œë“œ ëª¨ë©˜í…€: {stats['avg_trend_momentum']:.2f}
   - ì‹œì¥ ì ìœ ìœ¨: {stats['market_share']:.1%}
"""
        
        report += "\nğŸ“ˆ íŠ¸ë Œë“œ ê°•ë„ ë¶„ì„\n"
        for keyword, strength in trend_strength.items():
            direction_emoji = "ğŸ”¥" if strength['trend_direction'] == 'rising' else "ğŸ“Š" if strength['trend_direction'] == 'stable' else "ğŸ“‰"
            report += f"{direction_emoji} {keyword}: {strength['trend_direction']} (ëª¨ë©˜í…€: {strength['momentum_score']:.2f})\n"
        
        report += f"""
ğŸ”— ìƒê´€ê´€ê³„ ë¶„ì„
- ì¡°íšŒìˆ˜ vs ì°¸ì—¬ìœ¨: {correlations.get('views_vs_engagement', 0):.3f}
- ì¢‹ì•„ìš” vs ëŒ“ê¸€ìˆ˜: {correlations.get('likes_vs_comments', 0):.3f}
- ë°”ì´ëŸ´ì ìˆ˜ vs ëª¨ë©˜í…€: {correlations.get('viral_vs_momentum', 0):.3f}
- ê´€ë ¨ì„± vs ì„±ê³¼: {correlations.get('relevance_vs_performance', 0):.3f}

ğŸ¯ ì •ëŸ‰ì  í•µì‹¬ ì¸ì‚¬ì´íŠ¸
"""
        
        for insight in insights:
            insight_emoji = {
                'performance_leader': 'ğŸ†',
                'momentum_leader': 'ğŸš€',
                'engagement_leader': 'ğŸ’¬',
                'market_leader': 'ğŸ“Š'
            }.get(insight['type'], 'ğŸ“Œ')
            
            report += f"{insight_emoji} {insight['type'].replace('_', ' ').title()}: {insight['keyword']} "
            report += f"(ê°’: {insight['value']:.4f}, ì‹ ë¢°ë„: {insight['confidence']:.0%})\n"
        
        report += f"\nğŸ… ìƒìœ„ ì„±ê³¼ ë¹„ë””ì˜¤ (ë°”ì´ëŸ´ ì ìˆ˜ ê¸°ì¤€)\n"
        for performer in top_performers[:5]:
            report += f"{performer['rank']}ìœ„. {performer['keyword']} - "
            report += f"ì¡°íšŒìˆ˜: {performer['views']:,}, "
            report += f"ì°¸ì—¬ìœ¨: {performer['engagement_rate']:.4f}, "
            report += f"ë°”ì´ëŸ´ì ìˆ˜: {performer['viral_score']:.2f}\n"
        
        report += f"""
ğŸ“‹ ë¶„ì„ ë°©ë²•ë¡ 
- ì°¸ì—¬ìœ¨ = (ì¢‹ì•„ìš” + ëŒ“ê¸€) / ì¡°íšŒìˆ˜
- ë°”ì´ëŸ´ì ìˆ˜ = logâ‚â‚€(ì¡°íšŒìˆ˜) Ã— 0.4 + ì°¸ì—¬ìœ¨Ã—1000 Ã— 0.3 + logâ‚â‚€(ì¢‹ì•„ìš”) Ã— 0.2 + logâ‚â‚€(ëŒ“ê¸€) Ã— 0.1
- íŠ¸ë Œë“œëª¨ë©˜í…€ = í‚¤ì›Œë“œìƒëŒ€ì„±ê³¼ Ã— ê°œë³„ë¹„ë””ì˜¤ì„±ê³¼
- ì„±ê³¼ë°±ë¶„ìœ„ = (ì¡°íšŒìˆ˜ë°±ë¶„ìœ„ + ì°¸ì—¬ìœ¨ë°±ë¶„ìœ„) / 2
- í´ëŸ¬ìŠ¤í„°ë§: K-means (n=3)
- í†µê³„ì  ìœ ì˜ì„±: 95% ì‹ ë¢°êµ¬ê°„
"""
        
        return report

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # API í‚¤ í™•ì¸
    youtube_api_key = os.getenv('YOUTUBE_API_KEY')
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    
    if not youtube_api_key or not gemini_api_key or \
       youtube_api_key == 'your_youtube_api_key_here' or \
       gemini_api_key == 'your_gemini_api_key_here':
        print("API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("1. .env íŒŒì¼ì„ í¸ì§‘í•˜ì„¸ìš”")
        print("2. YOUTUBE_API_KEYì™€ GEMINI_API_KEYì— ì‹¤ì œ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        return
    
    try:
        # ì •ëŸ‰ì  ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = QuantitativePokerAnalyzer(youtube_api_key, gemini_api_key)
        
        # 1. ë¹„ë””ì˜¤ ìˆ˜ì§‘
        print("ì •ëŸ‰ì  ë¶„ì„ìš© ë¹„ë””ì˜¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        videos = await analyzer.collect_all_videos()
        
        if not videos:
            print("ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ì •ëŸ‰ì  ë¶„ì„ ìˆ˜í–‰
        print("ì™„ì „ ì •ëŸ‰ì  ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        analysis = analyzer.perform_quantitative_analysis()
        
        # 3. ê²°ê³¼ ì €ì¥
        print("ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
        saved_file = analyzer.save_quantitative_results(analysis)
        
        # 4. ì •ëŸ‰ì  ë¦¬í¬íŠ¸ ì¶œë ¥
        print("ì •ëŸ‰ì  ë¶„ì„ ë¦¬í¬íŠ¸:")
        print("=" * 80)
        report = analyzer.generate_quantitative_report(analysis)
        print(report)
        
        print(f"\nìƒì„¸ ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼ëŠ” {saved_file} íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
    except Exception as e:
        print(f"ì •ëŸ‰ì  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())