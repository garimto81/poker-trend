# PokerScout 크롤러 (`poker_crawler.py`) 기술 기획서

## 1. 개요

본 문서는 `poker_crawler.py` 스크립트의 핵심 기능, 작동 로직, 그리고 기술적 특징을 상세히 설명합니다. 이 스크립트의 주 목적은 [PokerScout](https://www.pokerscout.com) 웹사이트에 접속하여 온라인 포커 룸의 트래픽 데이터를 실시간으로 수집하고, 특정 비즈니스 로직에 따라 데이터를 분류한 후, 결과를 로컬 파일 시스템에 저장하는 것입니다.

## 2. 핵심 기술 스택

스크립트는 다음 주요 라이브러리를 사용하여 구현되었습니다.

- **`cloudscraper`**: Cloudflare와 같은 웹사이트 보안 및 안티봇(Anti-Bot) 솔루션을 우회하기 위해 특별히 설계된 HTTP 요청 라이브러리입니다. 일반적인 `requests` 라이브러리 대신 이를 사용하여 안정적으로 웹사이트에 접속합니다.
- **`BeautifulSoup4`**: 스크래핑한 HTML 문서의 구조를 파싱(parsing)하고, 원하는 데이터를 쉽게 탐색 및 추출하기 위해 사용됩니다.
- **`re` (정규식)**: 추출한 데이터(특히 사이트 이름)에서 불필요한 문자를 제거하고 표준화하는 데 사용됩니다.
- **`logging`**: 스크립트 실행 과정의 주요 단계와 오류를 기록하여 디버깅 및 유지보수를 용이하게 합니다.
- **`json`**: 최종적으로 수집 및 처리된 데이터를 구조화된 JSON 파일 형태로 저장하기 위해 사용됩니다.

## 3. 크롤링 프로세스 상세

크롤링은 `LivePokerScoutCrawler` 클래스 내에서 여러 단계로 나뉘어 체계적으로 실행됩니다.

### Step 1: 초기 설정 (Initialization)
- `cloudscraper` 인스턴스를 생성합니다. 이때, 실제 사용자의 웹 브라우저(Chrome on Linux)처럼 보이도록 사용자 에이전트(User-Agent)를 설정하여 차단 가능성을 최소화합니다.
- `'GGNetwork'`, `'GGPoker ON'` 등 GG 포커 계열 사이트 이름을 리스트(`gg_poker_sites`)로 미리 정의합니다. 이 리스트는 이후 데이터 분류의 핵심 기준이 됩니다.

### Step 2: 웹사이트 접속 및 HTML 요청
- `cloudscraper`를 통해 `https://www.pokerscout.com`에 GET 요청을 보냅니다.
- 요청 시간 초과(timeout)를 30초로 설정하고, 응답 상태 코드를 확인하여 접속 실패 시 즉시 오류를 발생시킵니다.

### Step 3: 핵심 데이터 테이블 파싱
- `BeautifulSoup`을 사용하여 응답받은 HTML 전체를 파싱합니다.
- 파싱된 HTML 내에서 `class` 속성이 `rankTable`인 `<table>` 태그를 찾습니다. 이 테이블이 포커 룸 순위 데이터가 담긴 핵심 컨테이너입니다.

### Step 4: 데이터 행(Row) 반복 및 추출
- `rankTable` 내의 모든 행(`<tr>` 태그)을 순회하며 개별 사이트 데이터를 추출합니다. 이 과정은 다음과 같은 세부 로직을 포함합니다.

    1.  **광고 필터링**: 특정 `class`(`cus_top_traffic_coin`)를 가진 행은 광고이므로 건너뜁니다.
    2.  **데이터 필드 식별**: 각 데이터(온라인 플레이어, 캐시 플레이어 등)는 열의 순서가 아닌, `<td>` 태그의 고유 `id` 속성(`online`, `cash`, `peak`, `avg`)을 기준으로 정확하게 찾아냅니다. 이는 웹사이트 구조가 일부 변경되어도 스크립트가 깨지지 않도록 하는 안정적인 방법입니다.
    3.  **데이터 정제**: 추출한 숫자 데이터에서 쉼표(`,`)를 제거하고, `isdigit()` 메서드로 숫자인지 확인한 후 정수(integer)로 변환합니다.
    4.  **데이터 유효성 검증**: 주요 트래픽 수치(`players_online`, `cash_players`, `peak_24h`)가 모두 0인 행은 의미 없는 데이터로 간주하고 건너뜁니다.
    5.  **사이트명 정규화**: 사이트 이름에 포함될 수 있는 특수문자를 정규식을 사용해 제거하여 데이터의 일관성을 유지합니다.
    6.  **비즈니스 로직: GG포커 분류**: 정규화된 사이트 이름이 `Step 1`에서 정의한 `gg_poker_sites` 리스트에 포함되는지 확인하고, `category` 필드에 `'GG_POKER'` 또는 `'COMPETITOR'` 값을 할당합니다.
    7.  **데이터 구조화**: 추출 및 가공된 모든 데이터(`site_name`, `category`, `players_online` 등)와 수집 시간(`collected_at`)을 하나의 딕셔너리(Dictionary)로 묶습니다.

### Step 5: 결과 취합 및 반환
- 각 행에서 생성된 딕셔너리를 하나의 리스트(`collected_data`)에 순서대로 추가합니다.
- 모든 행의 처리가 끝나면, 이 리스트를 최종 결과물로 반환합니다.

## 4. 데이터 분석 및 저장

`analyze_results` 메서드는 크롤링이 완료된 후 실행됩니다.

- **콘솔 출력**: 수집된 데이터를 기반으로 총 사이트 수, GG 포커 사이트 수, 총 플레이어 수 등 요약 통계를 계산하여 콘솔에 출력합니다.
- **파일 저장**: 전체 결과 데이터를 하나의 큰 딕셔너리로 재구성한 후, 실행 시점의 타임스탬프를 포함하는 JSON 파일(`live_crawling_result_YYYYMMDD_HHMMSS.json`)로 저장합니다.
- **핵심 사항**: **이 단계에서 데이터는 Firebase나 다른 원격 데이터베이스가 아닌, 로컬 파일 시스템에만 저장됩니다.** 이것이 바로 서버에 쓰기 작업이 일어나지 않는 원인입니다.

## 5. 오류 처리

- 스크립트의 주요 실행 부분(웹사이트 접속, 데이터 추출 등)은 `try...except` 블록으로 감싸여 있습니다.
- 크롤링 중 네트워크 오류나 웹사이트 구조 변경으로 인한 문제가 발생하더라도 전체 프로그램이 중단되지 않고, 오류 내용을 로깅한 후 실행을 계속하거나 안전하게 종료됩니다.

## 6. 결론

`poker_crawler.py`는 특정 웹사이트에서 데이터를 안정적으로 수집하고, 정의된 비즈니스 규칙에 따라 가공하여, 최종 결과를 **로컬 JSON 파일로 저장**하는 것을 목표로 하는 잘 구조화된 독립 실행형 스크립트입니다. Firebase 연동 로직은 현재 구현되어 있지 않으며, 모든 작업은 로컬 환경 내에서 완결됩니다.
