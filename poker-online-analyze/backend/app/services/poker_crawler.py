#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PokerScout ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ í‘œì‹œ
online_data_collector.pyì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
+ Firebase Firestore ì—°ë™ ê¸°ëŠ¥ ì¶”ê°€ (íš¨ìœ¨ì ì¸ êµ¬ì¡°)
"""

import cloudscraper
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone
import logging
import sys
import re
import os

import firebase_admin
from firebase_admin import credentials, firestore

# Firebase Admin SDK ì´ˆê¸°í™”
try:
    # GitHub Actions í™˜ê²½ê³¼ ë¡œì»¬ í™˜ê²½ ëª¨ë‘ ì§€ì›
    possible_key_paths = [
        # GitHub Actions í™˜ê²½
        os.path.join(os.path.dirname(__file__), '..', '..', 'key', 'firebase-service-account-key.json'),
        # ë¡œì»¬ í™˜ê²½ (backend í´ë” ê¸°ì¤€)
        os.path.join(os.path.dirname(__file__), '..', '..', '..', 'key', 'firebase-service-account-key.json'),
        # í™˜ê²½ ë³€ìˆ˜ë¡œ ì§€ì •ëœ ê²½ë¡œ
        os.environ.get('FIREBASE_KEY_PATH', '')
    ]
    
    key_path = None
    for path in possible_key_paths:
        if path and os.path.exists(path):
            key_path = os.path.abspath(path)
            break
    
    if not key_path:
        raise FileNotFoundError("Firebase ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    cred = credentials.Certificate(key_path)
    if not firebase_admin._apps:
        firebase_admin.initialize_app(cred)
    db = firestore.client()
    logger = logging.getLogger(__name__)
    logger.info(f"Firebase Admin SDKê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (í‚¤ íŒŒì¼: {key_path})")
except Exception as e:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    logger.error(f"Firebase Admin SDK ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    db = None

# ë¡œê¹… ì„¤ì • (Firebase ì´ˆê¸°í™” í›„ ì„¤ì • ë³´ì¥)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def upload_to_firestore_efficiently(data):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    - `sites` ì»¬ë ‰ì…˜: ì‚¬ì´íŠ¸ì˜ ê³ ì • ì •ë³´ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
    - `traffic_logs` í•˜ìœ„ ì»¬ë ‰ì…˜: ì‹œê°„ì— ë”°ë¥¸ íŠ¸ë˜í”½ ë°ì´í„° ì €ì¥
    """
    if not db:
        logger.error("Firestore í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not data:
        logger.warning("ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ë°ì´í„° ì—…ë¡œë“œ ì‹œì‘...")
    
    # Batch Writeë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì‘ì—…ì„ í•œ ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬
    batch = db.batch()
    
    for site_data in data:
        site_name = site_data['site_name']
        collected_at_iso = site_data['collected_at']
        
        # 1. `sites` ì»¬ë ‰ì…˜ ì²˜ë¦¬
        site_ref = db.collection('sites').document(site_name)
        site_info = {
            'site_name': site_name,
            'category': site_data['category'],
            'last_updated_at': firestore.SERVER_TIMESTAMP # ì„œë²„ ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        }
        # set(..., merge=True)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ë¬¸ì„œëŠ” ì—…ë°ì´íŠ¸, ì—†ëŠ” ë¬¸ì„œëŠ” ìƒì„±
        batch.set(site_ref, site_info, merge=True)

        # 2. `traffic_logs` í•˜ìœ„ ì»¬ë ‰ì…˜ ì²˜ë¦¬
        log_ref = site_ref.collection('traffic_logs').document(collected_at_iso)
        traffic_data = {
            'players_online': site_data['players_online'],
            'cash_players': site_data['cash_players'],
            'peak_24h': site_data['peak_24h'],
            'seven_day_avg': site_data['seven_day_avg'],
            # ISO 8601 ë¬¸ìì—´ì„ Firestore íƒ€ì„ìŠ¤íƒ¬í”„ ê°ì²´ë¡œ ë³€í™˜
            'collected_at': datetime.fromisoformat(collected_at_iso).replace(tzinfo=timezone.utc)
        }
        batch.set(log_ref, traffic_data)

    try:
        # Batch ì‘ì—… ì¼ê´„ ì»¤ë°‹
        batch.commit()
        logger.info(f"ì„±ê³µì ìœ¼ë¡œ {len(data)}ê°œ ì‚¬ì´íŠ¸ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"Firestore Batch ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())

class LivePokerScoutCrawler:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'linux', 'mobile': False}
        )
        self.gg_poker_sites = ['GGNetwork', 'GGPoker ON', 'GG Poker', 'GGPoker']
        
    def crawl_pokerscout_data(self):
        logger.info("PokerScout ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œì‘...")
        try:
            response = self.scraper.get('https://www.pokerscout.com', timeout=30)
            response.raise_for_status()
            logger.info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            table = soup.find('table', {'class': 'rankTable'})
            if not table:
                logger.error("PokerScout í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            logger.info("rankTable ë°œê²¬!")
            collected_data = []
            rows = table.find_all('tr')[1:]
            logger.info(f"ë°œê²¬ëœ í–‰ ìˆ˜: {len(rows)}")
            
            for i, row in enumerate(rows):
                try:
                    if 'cus_top_traffic_coin' in row.get('class', []):
                        continue
                    brand_title = row.find('span', {'class': 'brand-title'})
                    if not brand_title:
                        continue
                    site_name = brand_title.get_text(strip=True)
                    if not site_name or len(site_name) < 2:
                        continue
                    
                    players_online, cash_players, peak_24h, seven_day_avg = 0, 0, 0, 0
                    
                    online_td = row.find('td', {'id': 'online'})
                    if online_td and (span := online_td.find('span')) and (text := span.get_text(strip=True).replace(',', '')) and text.isdigit():
                        players_online = int(text)
                    
                    cash_td = row.find('td', {'id': 'cash'})
                    if cash_td and (text := cash_td.get_text(strip=True).replace(',', '')) and text.isdigit():
                        cash_players = int(text)

                    peak_td = row.find('td', {'id': 'peak'})
                    if peak_td and (span := peak_td.find('span')) and (text := span.get_text(strip=True).replace(',', '')) and text.isdigit():
                        peak_24h = int(text)

                    avg_td = row.find('td', {'id': 'avg'})
                    if avg_td and (span := avg_td.find('span')) and (text := span.get_text(strip=True).replace(',', '')) and text.isdigit():
                        seven_day_avg = int(text)

                    if players_online == 0 and cash_players == 0 and peak_24h == 0:
                        continue
                    
                    site_name = re.sub(r'[^\w\s\-\(\)\.&]', '', site_name).strip()
                    category = 'GG_POKER' if site_name in self.gg_poker_sites else 'COMPETITOR'
                    
                    collected_data.append({
                        'site_name': site_name,
                        'category': category,
                        'players_online': players_online,
                        'cash_players': cash_players,
                        'peak_24h': peak_24h,
                        'seven_day_avg': seven_day_avg,
                        'collected_at': datetime.now(timezone.utc).isoformat()
                    })
                except Exception as e:
                    logger.error(f"í–‰ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(collected_data)}ê°œ ì‚¬ì´íŠ¸ ìˆ˜ì§‘")
            return collected_data
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def analyze_and_save(self, data):
        if not data:
            logger.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        logger.info("\n" + "="*60)
        logger.info("í¬ë¡¤ë§ ê²°ê³¼ ë¶„ì„")
        logger.info("="*60)
        
        total_sites = len(data)
        total_players = sum(site['players_online'] for site in data)
        logger.info(f"ì´ ì‚¬ì´íŠ¸ ìˆ˜: {total_sites}ê°œ, ì´ ì˜¨ë¼ì¸ í”Œë ˆì´ì–´: {total_players:,}ëª…")
        
        # ë¡œì»¬ JSON íŒŒì¼ ì €ì¥ (ë°±ì—… ë° ë””ë²„ê¹…ìš©)
        filename = f"live_crawling_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ê²°ê³¼ê°€ ë¡œì»¬ ë°±ì—… íŒŒì¼({filename})ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # GitHub JSON ë°±ì—… íŒŒì¼ ìƒì„± (data í´ë”ì— ì €ì¥)
        self.create_github_backup(data, filename)
        
        # íš¨ìœ¨ì ì¸ êµ¬ì¡°ë¡œ Firestoreì— ì—…ë¡œë“œ
        upload_to_firestore_efficiently(data)
        
        return data
    
    def create_github_backup(self, data, filename):
        """
        GitHub Actions í™˜ê²½ì—ì„œ data í´ë”ì— JSON ë°±ì—… íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            # data ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ëŠ” ê²½ìš°)
            data_dir = os.path.join(os.getcwd(), 'data')
            os.makedirs(data_dir, exist_ok=True)
            
            # GitHub ë°±ì—… íŒŒì¼ ê²½ë¡œ
            github_backup_path = os.path.join(data_dir, filename)
            
            # JSON íŒŒì¼ ì €ì¥
            with open(github_backup_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ”„ GitHub ë°±ì—… íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {github_backup_path}")
            
            # íŒŒì¼ í¬ê¸°ì™€ ìš”ì•½ ì •ë³´ ë¡œê¹…
            file_size = os.path.getsize(github_backup_path)
            logger.info(f"ğŸ“ ë°±ì—… íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            logger.info(f"ğŸ“Š ë°±ì—…ëœ ë°ì´í„°: {len(data)}ê°œ ì‚¬ì´íŠ¸, {sum(site['players_online'] for site in data):,}ëª… í”Œë ˆì´ì–´")
            
        except Exception as e:
            logger.error(f"GitHub ë°±ì—… íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())

if __name__ == '__main__':
    crawler = LivePokerScoutCrawler()
    crawled_data = crawler.crawl_pokerscout_data()
    if crawled_data:
        crawler.analyze_and_save(crawled_data)
