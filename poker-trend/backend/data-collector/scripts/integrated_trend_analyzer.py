#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© YouTube í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°
ì¼ê°„, ì£¼ê°„, ì›”ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í†µí•© ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import logging
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
from collections import Counter, defaultdict
import requests
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
from pathlib import Path
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
REQUIRED_ENV_VARS = ['YOUTUBE_API_KEY', 'SLACK_WEBHOOK_URL', 'GEMINI_API_KEY']
for var in REQUIRED_ENV_VARS:
    if not os.getenv(var):
        logger.error(f"Missing required environment variable: {var}")
        sys.exit(1)

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
youtube = build('youtube', 'v3', developerKey=os.getenv('YOUTUBE_API_KEY'))
slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
gemini_model = genai.GenerativeModel('gemini-1.5-flash')


class IntegratedTrendAnalyzer:
    """í†µí•© í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""
    
    def __init__(self, report_type: str, date_range: int):
        self.report_type = report_type  # daily, weekly, monthly
        self.date_range = date_range    # 1, 7, 30
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ì–´ ì „ìš©, Global ê²€ìƒ‰)
        self.search_terms = [
            'poker',
            'holdem',
            'wsop',
            'wpt',
            'ept',
            'pokerstars',
            'ggpoker',
            'triton poker'
        ]
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        self.categories = {
            'tournament': ['WSOP', 'WPT', 'EPT', 'tournament', 'final table', 'Triton'],
            'cash_game': ['cash game', 'high stakes', 'cash', 'NLH', 'PLO'],
            'online': ['PokerStars', 'GGPoker', 'online poker', 'online'],
            'education': ['strategy', 'tutorial', 'how to', 'tips', 'learn', 'guide'],
            'entertainment': ['highlights', 'funny', 'best', 'epic', 'amazing', 'crazy']
        }
        
        self.all_videos = []
        self.keyword_videos = defaultdict(list)
        self.channel_stats = defaultdict(lambda: {'count': 0, 'views': 0})
        
    def collect_videos(self) -> Dict[str, List[Dict]]:
        """ê° í‚¤ì›Œë“œë³„ë¡œ ìƒìœ„ 5ê°œ ì˜ìƒ ìˆ˜ì§‘"""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=self.date_range)
        
        published_after = start_date.isoformat() + 'Z'
        published_before = end_date.isoformat() + 'Z'
        
        logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        
        for keyword in self.search_terms:
            try:
                # ê° í‚¤ì›Œë“œë³„ë¡œ ì¡°íšŒìˆ˜ ìƒìœ„ 5ê°œ ì˜ìƒ ê²€ìƒ‰
                request = youtube.search().list(
                    q=keyword,
                    part='snippet',
                    type='video',
                    maxResults=5,
                    order='viewCount',
                    publishedAfter=published_after,
                    publishedBefore=published_before,
                    relevanceLanguage='en',  # ì˜ì–´ ì½˜í…ì¸  ìš°ì„ 
                    videoDuration='short' if self.report_type == 'daily' else 'any'
                )
                response = request.execute()
                
                video_ids = [item['id']['videoId'] for item in response.get('items', [])]
                
                if video_ids:
                    # ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    details_request = youtube.videos().list(
                        part='statistics,contentDetails,snippet',
                        id=','.join(video_ids)
                    )
                    details_response = details_request.execute()
                    
                    for item in details_response.get('items', []):
                        video_data = {
                            'video_id': item['id'],
                            'title': item['snippet']['title'],
                            'channel_title': item['snippet']['channelTitle'],
                            'channel_id': item['snippet']['channelId'],
                            'published_at': item['snippet']['publishedAt'],
                            'view_count': int(item['statistics'].get('viewCount', 0)),
                            'like_count': int(item['statistics'].get('likeCount', 0)),
                            'comment_count': int(item['statistics'].get('commentCount', 0)),
                            'duration': item['contentDetails']['duration'],
                            'url': f"https://youtube.com/watch?v={item['id']}",
                            'keyword': keyword,
                            'category': self._categorize_video(item['snippet']['title'], item['snippet'].get('description', ''))
                        }
                        
                        # ì¤‘ë³µ ì œê±°
                        if not any(v['video_id'] == video_data['video_id'] for v in self.all_videos):
                            self.all_videos.append(video_data)
                            self.keyword_videos[keyword].append(video_data)
                            
                            # ì±„ë„ í†µê³„ ì—…ë°ì´íŠ¸
                            channel = video_data['channel_title']
                            self.channel_stats[channel]['count'] += 1
                            self.channel_stats[channel]['views'] += video_data['view_count']
                
                logger.info(f"í‚¤ì›Œë“œ '{keyword}': {len(self.keyword_videos[keyword])}ê°œ ì˜ìƒ ìˆ˜ì§‘")
                
            except HttpError as e:
                logger.error(f"YouTube API ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}): {e}")
                continue
            except Exception as e:
                logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {keyword}): {e}")
                continue
        
        # ì „ì²´ ì˜ìƒì„ ì¡°íšŒìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        self.all_videos.sort(key=lambda x: x['view_count'], reverse=True)
        
        logger.info(f"ì´ {len(self.all_videos)}ê°œ ì˜ìƒ ìˆ˜ì§‘ ì™„ë£Œ")
        return self.keyword_videos
    
    def _categorize_video(self, title: str, description: str) -> str:
        """ì˜ìƒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        text = (title + ' ' + description).lower()
        
        for category, keywords in self.categories.items():
            if any(keyword.lower() in text for keyword in keywords):
                return category
        
        return 'general'
    
    def analyze_trends(self) -> Dict[str, Any]:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        if not self.all_videos:
            return {}
        
        # ê¸°ë³¸ í†µê³„
        total_views = sum(v['view_count'] for v in self.all_videos)
        avg_views = total_views // len(self.all_videos)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_stats = defaultdict(lambda: {'count': 0, 'views': 0})
        for video in self.all_videos:
            cat = video['category']
            category_stats[cat]['count'] += 1
            category_stats[cat]['views'] += video['view_count']
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì¼ê°„ ë¦¬í¬íŠ¸ìš©)
        hourly_uploads = defaultdict(int)
        if self.report_type == 'daily':
            for video in self.all_videos:
                hour = datetime.fromisoformat(video['published_at'].replace('Z', '+00:00')).hour
                hourly_uploads[hour] += 1
        
        # ì„±ì¥ë¥  ê³„ì‚° (ì£¼ê°„/ì›”ê°„ìš©)
        growth_videos = []
        if self.report_type in ['weekly', 'monthly']:
            for video in self.all_videos[:20]:  # ìƒìœ„ 20ê°œë§Œ
                # ê°„ë‹¨í•œ ì„±ì¥ë¥  ê³„ì‚° (ì¼í‰ê·  ì¡°íšŒìˆ˜)
                days_since_upload = (datetime.utcnow() - datetime.fromisoformat(
                    video['published_at'].replace('Z', '+00:00')
                )).days or 1
                daily_views = video['view_count'] / days_since_upload
                
                growth_videos.append({
                    'title': video['title'],
                    'channel': video['channel_title'],
                    'views': video['view_count'],
                    'daily_growth': int(daily_views),
                    'url': video['url']
                })
        
        return {
            'total_videos': len(self.all_videos),
            'total_views': total_views,
            'avg_views': avg_views,
            'category_stats': dict(category_stats),
            'channel_stats': dict(self.channel_stats),
            'hourly_uploads': dict(hourly_uploads),
            'growth_videos': sorted(growth_videos, key=lambda x: x['daily_growth'], reverse=True)[:10]
        }
    
    def generate_ai_insights(self, analysis_data: Dict[str, Any]) -> str:
        """Gemini AIë¥¼ ì‚¬ìš©í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        # ë¦¬í¬íŠ¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸
        prompts = {
            'daily': f"""
ë‹¤ìŒì€ ìµœê·¼ 24ì‹œê°„ YouTube í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°ì…ë‹ˆë‹¤.
ì´ {analysis_data['total_videos']}ê°œ ì˜ìƒ, ì´ ì¡°íšŒìˆ˜ {analysis_data['total_views']:,}íšŒ

ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬: {json.dumps(analysis_data['category_stats'], ensure_ascii=False)}
ìƒìœ„ ì±„ë„: {', '.join([f"{ch} ({stats['count']}ê°œ)" for ch, stats in list(analysis_data['channel_stats'].items())[:5]])}

ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì˜¤ëŠ˜ì˜ í•« í† í”½ 2-3ê°œ
2. ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì±„ë„/ì„ ìˆ˜
3. ë‚´ì¼ ì˜ˆìƒë˜ëŠ” íŠ¸ë Œë“œ
í•œêµ­ì–´ë¡œ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
""",
            'weekly': f"""
ë‹¤ìŒì€ ì§€ë‚œ 7ì¼ê°„ YouTube í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°ì…ë‹ˆë‹¤.
ì´ {analysis_data['total_videos']}ê°œ ì˜ìƒ, ì´ ì¡°íšŒìˆ˜ {analysis_data['total_views']:,}íšŒ

ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬: {json.dumps(analysis_data['category_stats'], ensure_ascii=False)}
ê¸‰ì„±ì¥ ì˜ìƒ: {len(analysis_data.get('growth_videos', []))}ê°œ

ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì´ë²ˆ ì£¼ ê°€ì¥ í° íŠ¸ë Œë“œ ë³€í™”
2. ìƒˆë¡­ê²Œ ë– ì˜¤ë¥´ëŠ” í‚¤ì›Œë“œë‚˜ í† í”½
3. ë‹¤ìŒ ì£¼ ì½˜í…ì¸  ì „ëµ ì œì•ˆ
í•œêµ­ì–´ë¡œ 5-6ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
""",
            'monthly': f"""
ë‹¤ìŒì€ ì§€ë‚œ 30ì¼ê°„ YouTube í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°ì…ë‹ˆë‹¤.
ì´ {analysis_data['total_videos']}ê°œ ì˜ìƒ, ì´ ì¡°íšŒìˆ˜ {analysis_data['total_views']:,}íšŒ

ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬: {json.dumps(analysis_data['category_stats'], ensure_ascii=False)}
ì£¼ìš” ì±„ë„: {', '.join([f"{ch} ({stats['views']:,}íšŒ)" for ch, stats in list(analysis_data['channel_stats'].items())[:5]])}

ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì´ë²ˆ ë‹¬ì˜ ì£¼ìš” íŠ¸ë Œë“œ 3ê°œ
2. ê°€ì¥ ì„±ê³µì ì¸ ì½˜í…ì¸  ìœ í˜•
3. ì¥ê¸°ì ì¸ íŠ¸ë Œë“œ ë°©í–¥ ì˜ˆì¸¡
í•œêµ­ì–´ë¡œ 7-8ë¬¸ì¥ìœ¼ë¡œ ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
        }
        
        try:
            prompt = prompts.get(self.report_type, prompts['daily'])
            response = gemini_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return "AI ë¶„ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def format_slack_message(self, analysis_data: Dict[str, Any], ai_insights: str) -> Dict[str, Any]:
        """Slack ë©”ì‹œì§€ í¬ë§·íŒ…"""
        
        # ë¦¬í¬íŠ¸ íƒ€ì…ë³„ ì œëª©
        titles = {
            'daily': 'ğŸ° í¬ì»¤ íŠ¸ë Œë“œ ì¼ì¼ ë¶„ì„',
            'weekly': 'ğŸ“Š í¬ì»¤ íŠ¸ë Œë“œ ì£¼ê°„ ë¶„ì„',
            'monthly': 'ğŸ“Š í¬ì»¤ íŠ¸ë Œë“œ ì›”ê°„ ì¢…í•© ë¦¬í¬íŠ¸'
        }
        
        # ê¸°ê°„ í…ìŠ¤íŠ¸
        date_ranges = {
            'daily': 'ìµœê·¼ 24ì‹œê°„',
            'weekly': 'ì§€ë‚œ 7ì¼',
            'monthly': 'ì§€ë‚œ 30ì¼'
        }
        
        # ê¸°ë³¸ ì •ë³´
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": titles[self.report_type]
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"ğŸ“… {datetime.now().strftime('%Yë…„ %mì›” %dì¼')} ({['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼'][datetime.now().weekday()]})\n"
                           f"â° ë¶„ì„ ê¸°ê°„: {date_ranges[self.report_type]}\n"
                           f"ğŸ“Š ë¶„ì„ ì˜ìƒ: {analysis_data['total_videos']}ê°œ"
                }
            },
            {
                "type": "divider"
            }
        ]
        
        # AI ì¸ì‚¬ì´íŠ¸
        if ai_insights:
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*ğŸ¤– AI íŠ¸ë Œë“œ ë¶„ì„*\n{ai_insights}"
                }
            })
            blocks.append({"type": "divider"})
        
        # í‚¤ì›Œë“œë³„ TOP ì˜ìƒ
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "*ğŸ“º í‚¤ì›Œë“œë³„ TOP ì˜ìƒ*"
            }
        })
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ ìƒìœ„ 1-2ê°œë§Œ í‘œì‹œ
        for keyword in self.search_terms:
            if keyword in self.keyword_videos and self.keyword_videos[keyword]:
                top_videos = self.keyword_videos[keyword][:2]
                video_text = f"\n*ã€{keyword}ã€‘*\n"
                
                for i, video in enumerate(top_videos, 1):
                    video_text += f"{i}. <{video['url']}|{video['title'][:50]}...>\n"
                    video_text += f"   ì¡°íšŒìˆ˜: {video['view_count']:,} | {video['channel_title']}\n"
                
                blocks.append({
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": video_text
                    }
                })
        
        blocks.append({"type": "divider"})
        
        # ì±„ë„ í†µê³„
        top_channels = sorted(
            self.channel_stats.items(),
            key=lambda x: x[1]['views'],
            reverse=True
        )[:5]
        
        channel_text = "*ğŸ¬ TOP 5 ì±„ë„*\n"
        for i, (channel, stats) in enumerate(top_channels, 1):
            channel_text += f"{i}. {channel}: {stats['count']}ê°œ ì˜ìƒ, {stats['views']:,}íšŒ\n"
        
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": channel_text
            }
        })
        
        # í†µê³„ ìš”ì•½
        blocks.append({"type": "divider"})
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸ“Š í†µê³„ ìš”ì•½*\n"
                       f"â€¢ ì´ ì¡°íšŒìˆ˜: {analysis_data['total_views']:,}íšŒ\n"
                       f"â€¢ í‰ê·  ì¡°íšŒìˆ˜: {analysis_data['avg_views']:,}íšŒ\n"
                       f"â€¢ ê°€ì¥ í™œë°œí•œ ì¹´í…Œê³ ë¦¬: {max(analysis_data['category_stats'].items(), key=lambda x: x[1]['views'])[0]}"
            }
        })
        
        return {
            "blocks": blocks,
            "text": f"{titles[self.report_type]} - {datetime.now().strftime('%Y-%m-%d')}"
        }
    
    def send_to_slack(self, message: Dict[str, Any]) -> bool:
        """Slackìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡"""
        try:
            response = requests.post(
                slack_webhook_url,
                json=message,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code == 200:
                logger.info("Slack ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ")
                return True
            else:
                logger.error(f"Slack ì „ì†¡ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"Slack ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def save_report(self, analysis_data: Dict[str, Any], ai_insights: str):
        """ë¦¬í¬íŠ¸ ë°ì´í„° ì €ì¥"""
        os.makedirs('reports', exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"reports/{self.report_type}_report_{timestamp}.json"
        
        report_data = {
            'report_type': self.report_type,
            'date_range': self.date_range,
            'generated_at': datetime.now().isoformat(),
            'total_videos': len(self.all_videos),
            'analysis': analysis_data,
            'ai_insights': ai_insights,
            'videos': self.all_videos[:50]  # ìƒìœ„ 50ê°œë§Œ ì €ì¥
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info(f"{'='*50}")
        logger.info(f"{self.report_type.upper()} ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
        logger.info(f"{'='*50}")
        
        # 1. ë°ì´í„° ìˆ˜ì§‘
        logger.info("1. YouTube ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        self.collect_videos()
        
        if not self.all_videos:
            logger.error("ìˆ˜ì§‘ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. íŠ¸ë Œë“œ ë¶„ì„
        logger.info("2. íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        analysis_data = self.analyze_trends()
        
        # 3. AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
        logger.info("3. AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
        ai_insights = self.generate_ai_insights(analysis_data)
        
        # 4. ë¦¬í¬íŠ¸ ì €ì¥
        logger.info("4. ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘...")
        self.save_report(analysis_data, ai_insights)
        
        # 5. Slack ì „ì†¡
        logger.info("5. Slack ë©”ì‹œì§€ ìƒì„± ë° ì „ì†¡ ì¤‘...")
        slack_message = self.format_slack_message(analysis_data, ai_insights)
        self.send_to_slack(slack_message)
        
        logger.info(f"{'='*50}")
        logger.info("ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        logger.info(f"{'='*50}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='YouTube í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°')
    parser.add_argument('--report-type', choices=['daily', 'weekly', 'monthly'],
                      default='daily', help='ë¦¬í¬íŠ¸ íƒ€ì…')
    parser.add_argument('--date-range', type=int, default=1,
                      help='ë¶„ì„ ê¸°ê°„ (ì¼ ë‹¨ìœ„)')
    
    args = parser.parse_args()
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸° (GitHub Actionsì—ì„œ ì„¤ì •)
    report_type = os.getenv('REPORT_TYPE', args.report_type)
    date_range = int(os.getenv('DATE_RANGE', args.date_range))
    
    # ë¶„ì„ê¸° ì‹¤í–‰
    analyzer = IntegratedTrendAnalyzer(report_type, date_range)
    analyzer.run()


if __name__ == "__main__":
    main()