#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì›”ê°„ í¬ì»¤ íŠ¸ë Œë“œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸
ë§¤ì›” ì²«ì§¸ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 10ì‹œ ì‹¤í–‰
"""

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import requests
from collections import Counter, defaultdict
from googleapiclient.discovery import build
import google.generativeai as genai

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# API ì´ˆê¸°í™”
youtube = build('youtube', 'v3', developerKey=os.getenv('YOUTUBE_API_KEY'))
slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
gemini_model = genai.GenerativeModel('gemini-pro')


class MonthlyTrendAnalyzer:
    """ì›”ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.month_data = {
            'total_videos': 0,
            'total_views': 0,
            'category_performance': defaultdict(lambda: {'videos': 0, 'views': 0}),
            'top_keywords': Counter(),
            'top_channels': Counter(),
            'viral_videos': [],
            'trend_evolution': [],
            'monthly_insights': []
        }
    
    def analyze_monthly_trends(self) -> Dict[str, Any]:
        """í•œ ë‹¬ê°„ì˜ íŠ¸ë Œë“œ ì¢…í•© ë¶„ì„"""
        logger.info("Starting monthly trend analysis...")
        
        # ì§€ë‚œ 30ì¼ê°„ ë°ì´í„° ìˆ˜ì§‘
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        # ì£¼ì°¨ë³„ ë°ì´í„° ìˆ˜ì§‘
        weekly_data = []
        for week in range(4):
            week_start = start_date + timedelta(weeks=week)
            week_end = week_start + timedelta(days=7)
            week_videos = self._collect_week_data(week_start, week_end)
            weekly_data.append({
                'week': week + 1,
                'videos': week_videos,
                'total': len(week_videos),
                'avg_views': sum(v.get('view_count', 0) for v in week_videos) / len(week_videos) if week_videos else 0
            })
        
        # ì›”ê°„ í†µê³„ ê³„ì‚°
        all_videos = []
        for week in weekly_data:
            all_videos.extend(week['videos'])
        
        self.month_data['total_videos'] = len(all_videos)
        self.month_data['total_views'] = sum(v.get('view_count', 0) for v in all_videos)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼ ë¶„ì„
        self._analyze_categories(all_videos)
        
        # í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
        self._analyze_keywords(all_videos)
        
        # ì±„ë„ ìˆœìœ„ ë¶„ì„
        self._analyze_channels(all_videos)
        
        # ë°”ì´ëŸ´ ì˜ìƒ ì„ ì • (ìƒìœ„ 1%)
        viral_threshold = sorted([v.get('view_count', 0) for v in all_videos], reverse=True)[int(len(all_videos) * 0.01)] if all_videos else 0
        self.month_data['viral_videos'] = [v for v in all_videos if v.get('view_count', 0) >= viral_threshold][:10]
        
        # ì£¼ì°¨ë³„ íŠ¸ë Œë“œ ë³€í™”
        self.month_data['trend_evolution'] = self._analyze_trend_evolution(weekly_data)
        
        # AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
        self.month_data['monthly_insights'] = self._generate_ai_insights()
        
        return self.month_data
    
    def _collect_week_data(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """íŠ¹ì • ì£¼ì˜ ë°ì´í„° ìˆ˜ì§‘"""
        videos = []
        # ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ì–´ ì „ìš©, Global ê²€ìƒ‰)
        # ì„¤ì • ë¬¸ì„œ: docs/SEARCH_KEYWORDS.md
        search_terms = [
            'poker', 'holdem', 'wsop', 'wpt', 
            'ept', 'pokerstars', 'ggpoker', 'triton poker'
        ]
        
        for term in search_terms:
            try:
                request = youtube.search().list(
                    q=term,
                    part='snippet',
                    type='video',
                    maxResults=50,
                    order='viewCount',
                    publishedAfter=start_date.isoformat() + 'Z',
                    publishedBefore=end_date.isoformat() + 'Z'
                )
                response = request.execute()
                
                for item in response.get('items', []):
                    videos.append({
                        'video_id': item['id']['videoId'],
                        'title': item['snippet']['title'],
                        'channel_title': item['snippet']['channelTitle'],
                        'published_at': item['snippet']['publishedAt']
                    })
            except Exception as e:
                logger.error(f"Error collecting data: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ìƒì„¸ ì •ë³´ ì¶”ê°€
        unique_videos = list({v['video_id']: v for v in videos}.values())
        return self._enrich_videos(unique_videos[:100])
    
    def _enrich_videos(self, videos: List[Dict]) -> List[Dict]:
        """ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ì¶”ê°€"""
        if not videos:
            return []
        
        video_ids = [v['video_id'] for v in videos]
        
        try:
            request = youtube.videos().list(
                part='statistics,contentDetails',
                id=','.join(video_ids[:50])
            )
            response = request.execute()
            
            stats_map = {
                item['id']: item['statistics']
                for item in response.get('items', [])
            }
            
            for video in videos:
                if video['video_id'] in stats_map:
                    stats = stats_map[video['video_id']]
                    video['view_count'] = int(stats.get('viewCount', 0))
                    video['like_count'] = int(stats.get('likeCount', 0))
                    video['comment_count'] = int(stats.get('commentCount', 0))
        except Exception as e:
            logger.error(f"Error enriching videos: {e}")
        
        return videos
    
    def _analyze_categories(self, videos: List[Dict]):
        """ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼ ë¶„ì„"""
        categories = {
            'tournament': ['WSOP', 'WPT', 'EPT', 'í† ë„ˆë¨¼íŠ¸'],
            'cash_game': ['cash', 'ìºì‹œ', 'high stakes'],
            'online': ['online', 'ì˜¨ë¼ì¸', 'PokerStars', 'GGPoker'],
            'education': ['strategy', 'ì „ëµ', 'tutorial', 'ê°•ì˜']
        }
        
        for video in videos:
            title_lower = video['title'].lower()
            categorized = False
            
            for cat, keywords in categories.items():
                if any(kw.lower() in title_lower for kw in keywords):
                    self.month_data['category_performance'][cat]['videos'] += 1
                    self.month_data['category_performance'][cat]['views'] += video.get('view_count', 0)
                    categorized = True
                    break
            
            if not categorized:
                self.month_data['category_performance']['other']['videos'] += 1
                self.month_data['category_performance']['other']['views'] += video.get('view_count', 0)
    
    def _analyze_keywords(self, videos: List[Dict]):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        import re
        
        for video in videos:
            # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            words = re.findall(r'\b[a-zA-Zê°€-í£]{3,}\b', video['title'].lower())
            
            # ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'the', 'and', 'for', 'with', 'poker', 'í¬ì»¤'}
            keywords = [w for w in words if w not in stopwords]
            
            # ì¡°íšŒìˆ˜ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
            weight = min(video.get('view_count', 0) / 10000, 10)  # ìµœëŒ€ ê°€ì¤‘ì¹˜ 10
            for keyword in keywords:
                self.month_data['top_keywords'][keyword] += weight
    
    def _analyze_channels(self, videos: List[Dict]):
        """ì±„ë„ë³„ ì„±ê³¼ ë¶„ì„"""
        for video in videos:
            channel = video['channel_title']
            views = video.get('view_count', 0)
            self.month_data['top_channels'][channel] += views
    
    def _analyze_trend_evolution(self, weekly_data: List[Dict]) -> List[Dict]:
        """ì£¼ì°¨ë³„ íŠ¸ë Œë“œ ë³€í™” ë¶„ì„"""
        evolution = []
        
        for week in weekly_data:
            # í•´ë‹¹ ì£¼ì˜ TOP í‚¤ì›Œë“œ
            week_keywords = Counter()
            for video in week['videos']:
                words = video['title'].lower().split()
                week_keywords.update(words)
            
            evolution.append({
                'week': week['week'],
                'total_videos': week['total'],
                'avg_views': week['avg_views'],
                'top_keywords': week_keywords.most_common(5)
            })
        
        return evolution
    
    def _generate_ai_insights(self) -> List[str]:
        """AIë¥¼ í™œìš©í•œ ì›”ê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # ì›”ê°„ ë°ì´í„° ìš”ì•½
            summary = f"""
            ì›”ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°:
            - ì´ ë¶„ì„ ì˜ìƒ: {self.month_data['total_videos']}ê°œ
            - ì´ ì¡°íšŒìˆ˜: {self.month_data['total_views']:,}íšŒ
            - TOP í‚¤ì›Œë“œ: {', '.join([k for k, _ in self.month_data['top_keywords'].most_common(10)])}
            - ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼: {dict(self.month_data['category_performance'])}
            """
            
            prompt = f"""
            ë‹¤ìŒ ì›”ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ 3ê°€ì§€ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”:
            
            {summary}
            
            ê° ì¸ì‚¬ì´íŠ¸ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:
            1. íŠ¸ë Œë“œ íŒ¨í„´ (ë¬´ì—‡ì´ ì£¼ëª©ë°›ì•˜ëŠ”ê°€)
            2. ì›ì¸ ë¶„ì„ (ì™œ ì´ëŸ° íŠ¸ë Œë“œê°€ ë°œìƒí–ˆëŠ”ê°€)
            3. í–¥í›„ ì „ë§ (ë‹¤ìŒ ë‹¬ ì˜ˆìƒ íŠ¸ë Œë“œ)
            
            ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            response = gemini_model.generate_content(prompt)
            return response.text.split('\n\n')[:3]  # 3ê°œ ì¸ì‚¬ì´íŠ¸ë§Œ
            
        except Exception as e:
            logger.error(f"AI insights generation error: {e}")
            return ["AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]


def send_monthly_report(data: Dict[str, Any]):
    """ì›”ê°„ ë¦¬í¬íŠ¸ Slack ì „ì†¡"""
    
    # í˜„ì¬ ì‹œê°„ (í•œêµ­ ì‹œê°„)
    kst_time = datetime.now() + timedelta(hours=9)
    
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"ğŸ“… í¬ì»¤ íŠ¸ë Œë“œ ì›”ê°„ ì¢…í•© ë¦¬í¬íŠ¸ ({kst_time.strftime('%Yë…„ %mì›”')})"
            }
        },
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸ“Š ì›”ê°„ ì¢…í•© í†µê³„*\n" +
                       f"â€¢ ì´ ë¶„ì„ ì˜ìƒ: {data['total_videos']:,}ê°œ\n" +
                       f"â€¢ ì´ ì¡°íšŒìˆ˜: {data['total_views']:,}íšŒ\n" +
                       f"â€¢ í‰ê·  ì¡°íšŒìˆ˜: {data['total_views'] // data['total_videos'] if data['total_videos'] > 0 else 0:,}íšŒ"
            }
        },
        {"type": "divider"},
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "*ğŸ† ì›”ê°„ TOP 10 í‚¤ì›Œë“œ*"
            }
        }
    ]
    
    # TOP í‚¤ì›Œë“œ
    keyword_text = []
    for i, (keyword, score) in enumerate(data['top_keywords'].most_common(10), 1):
        keyword_text.append(f"{i}. `{keyword}` (ì ìˆ˜: {score:.1f})")
    
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": '\n'.join(keyword_text[:5]) + '\n' + '\n'.join(keyword_text[5:])
        }
    })
    
    blocks.append({"type": "divider"})
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼*"
        }
    })
    
    cat_text = []
    for cat, stats in sorted(data['category_performance'].items(), key=lambda x: x[1]['views'], reverse=True):
        avg_views = stats['views'] // stats['videos'] if stats['videos'] > 0 else 0
        emoji = {
            'tournament': 'ğŸ†',
            'cash_game': 'ğŸ’°',
            'online': 'ğŸ’»',
            'education': 'ğŸ“š',
            'other': 'ğŸ®'
        }.get(cat, 'ğŸ“Œ')
        cat_text.append(f"{emoji} *{cat}*: {stats['videos']}ê°œ (í‰ê·  {avg_views:,}íšŒ)")
    
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": '\n'.join(cat_text)
        }
    })
    
    blocks.append({"type": "divider"})
    
    # TOP ì±„ë„
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ¬ TOP 5 ì±„ë„ (ì¡°íšŒìˆ˜ ê¸°ì¤€)*\n" +
                   '\n'.join([f"{i+1}. {ch} ({views:,}íšŒ)" 
                            for i, (ch, views) in enumerate(data['top_channels'].most_common(5))])
        }
    })
    
    blocks.append({"type": "divider"})
    
    # ì£¼ì°¨ë³„ íŠ¸ë Œë“œ ë³€í™”
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ“Š ì£¼ì°¨ë³„ íŠ¸ë Œë“œ ë³€í™”*"
        }
    })
    
    for week in data['trend_evolution']:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*{week['week']}ì£¼ì°¨*: {week['total_videos']}ê°œ ì˜ìƒ, í‰ê·  {int(week['avg_views']):,}íšŒ\n" +
                       f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join([f'`{k[0]}`' for k in week['top_keywords'][:3]])}"
            }
        })
    
    blocks.append({"type": "divider"})
    
    # AI ì¸ì‚¬ì´íŠ¸
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ¤– AI ì›”ê°„ ì¸ì‚¬ì´íŠ¸*"
        }
    })
    
    for insight in data['monthly_insights']:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": insight[:600]  # ê¸¸ì´ ì œí•œ
            }
        })
    
    blocks.append({"type": "divider"})
    
    # ë‹¤ìŒ ë‹¬ ì¶”ì²œ ì „ëµ
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ’¡ ë‹¤ìŒ ë‹¬ ì½˜í…ì¸  ì „ëµ ì¶”ì²œ*\n" +
                   f"1. TOP í‚¤ì›Œë“œ í™œìš©: {', '.join([k for k, _ in data['top_keywords'].most_common(3)])}\n" +
                   f"2. ì£¼ë ¥ ì¹´í…Œê³ ë¦¬: {max(data['category_performance'].items(), key=lambda x: x[1]['views'])[0]}\n" +
                   "3. ë°”ì´ëŸ´ ì˜ìƒ ë²¤ì¹˜ë§ˆí‚¹: ìƒìœ„ 1% ì˜ìƒ ë¶„ì„\n" +
                   "4. ì‹ ê·œ íŠ¸ë Œë“œ ì‹¤í—˜: AI í¬ì»¤, NFT í† ë„ˆë¨¼íŠ¸ ë“±"
        }
    })
    
    # Slack ì „ì†¡
    payload = {
        "blocks": blocks,
        "text": f"ğŸ“… í¬ì»¤ íŠ¸ë Œë“œ ì›”ê°„ ì¢…í•© ë¦¬í¬íŠ¸ ({kst_time.strftime('%Yë…„ %mì›”')})"
    }
    
    try:
        response = requests.post(
            slack_webhook_url,
            json=payload,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code == 200:
            logger.info("Monthly report sent successfully")
        else:
            logger.error(f"Slack webhook error: {response.status_code}")
            
    except Exception as e:
        logger.error(f"Error sending monthly report: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("Starting monthly poker trend analysis...")
    
    try:
        analyzer = MonthlyTrendAnalyzer()
        monthly_data = analyzer.analyze_monthly_trends()
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        os.makedirs('reports', exist_ok=True)
        with open(f'reports/monthly_report_{datetime.now(, encoding='utf-8').strftime("%Y%m")}.json', 'w', encoding='utf-8') as f:
            json.dump(monthly_data, f, ensure_ascii=False, indent=2, default=str)
        
        # Slack ì „ì†¡
        send_monthly_report(monthly_data)
        
        logger.info("Monthly trend analysis completed successfully!")
        
    except Exception as e:
        logger.error(f"Error in monthly analysis: {e}", exc_info=True)
        
        # ì—ëŸ¬ ì•Œë¦¼
        error_payload = {
            "text": f"âš ï¸ ì›”ê°„ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
        }
        requests.post(slack_webhook_url, json=error_payload)
        
        raise


if __name__ == "__main__":
    main()