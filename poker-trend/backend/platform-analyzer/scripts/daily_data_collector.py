#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily Poker Platform Data Collector
ë§¤ì¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ìì²´ íˆìŠ¤í† ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
"""

import os
import sys
import json
import sqlite3
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
from pathlib import Path

# Import analyzer
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from analyze_live_data import LivePokerDataAnalyzer

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('daily_collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DailyDataCollector:
    def __init__(self, db_path: str = "poker_history.db"):
        self.db_path = db_path
        self.analyzer = LivePokerDataAnalyzer()
        self.init_database()
    
    def init_database(self):
        """Initialize SQLite database for storing daily data"""
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”: {self.db_path}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Create daily_data table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS daily_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    site_name TEXT NOT NULL,
                    players_online INTEGER DEFAULT 0,
                    cash_players INTEGER DEFAULT 0,
                    peak_24h INTEGER DEFAULT 0,
                    seven_day_avg INTEGER DEFAULT 0,
                    data_quality TEXT DEFAULT 'normal',
                    created_at TEXT NOT NULL,
                    UNIQUE(date, site_name)
                )
            """)
            
            # Create collection_log table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS collection_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    platforms_collected INTEGER DEFAULT 0,
                    platforms_validated INTEGER DEFAULT 0,
                    platforms_flagged INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'pending',
                    notes TEXT,
                    created_at TEXT NOT NULL
                )
            """)
            
            # Create analysis_results table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date TEXT NOT NULL,
                    analysis_type TEXT NOT NULL,
                    total_online INTEGER DEFAULT 0,
                    total_cash INTEGER DEFAULT 0,
                    active_platforms INTEGER DEFAULT 0,
                    market_leader TEXT,
                    significant_changes INTEGER DEFAULT 0,
                    alert_level TEXT DEFAULT 'none',
                    summary_text TEXT,
                    data_json TEXT,
                    created_at TEXT NOT NULL
                )
            """)
            
            conn.commit()
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
    
    def collect_daily_data(self, target_date: str = None) -> Dict:
        """Collect and store daily data"""
        if target_date is None:
            target_date = datetime.now().strftime('%Y-%m-%d')
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        logger.info(f"ğŸ”„ ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {target_date}")
        
        # Check if data already exists for this date
        if self._data_exists_for_date(target_date):
            logger.warning(f"âš ï¸ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•¨: {target_date}")
            return {'status': 'already_exists', 'date': target_date}
        
        # Collect live data
        raw_data = self.analyzer.crawl_pokerscout_data()
        
        if not raw_data:
            logger.error("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return {'status': 'failed', 'date': target_date}
        
        logger.info(f"âœ… ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘: {len(raw_data)}ê°œ í”Œë«í¼")
        
        # Validate and clean data
        validated_data = self._validate_and_clean_data(raw_data)
        
        # Store in database
        stored_count = self._store_daily_data(target_date, timestamp, validated_data)
        
        # Log collection
        self._log_collection(target_date, timestamp, raw_data, validated_data)
        
        logger.info(f"âœ… ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {stored_count}ê°œ í”Œë«í¼ ì €ì¥")
        
        return {
            'status': 'success',
            'date': target_date,
            'platforms_collected': len(raw_data),
            'platforms_stored': stored_count,
            'timestamp': timestamp
        }
    
    def _data_exists_for_date(self, date: str) -> bool:
        """Check if data already exists for the given date"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM daily_data WHERE date = ?", (date,))
            count = cursor.fetchone()[0]
            return count > 0
    
    def _validate_and_clean_data(self, raw_data: List[Dict]) -> List[Dict]:
        """Validate and clean raw data"""
        logger.info("ğŸ” ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬")
        
        validated_data = []
        flagged_platforms = []
        
        for site in raw_data:
            # Apply validation rules
            original_online = site['players_online']
            validated_online = self.analyzer._validate_online_players(original_online, site['site_name'])
            
            # Determine data quality
            data_quality = 'normal'
            if validated_online != original_online:
                data_quality = 'corrected'
                flagged_platforms.append(site['site_name'])
            
            # Check for suspicious historical data
            if site['seven_day_avg'] == 0 and site['players_online'] > 100:
                data_quality = 'suspicious_history'
            
            # Store validated data
            validated_site = {
                'site_name': site['site_name'],
                'players_online': validated_online,
                'cash_players': site['cash_players'],
                'peak_24h': site['peak_24h'],
                'seven_day_avg': site['seven_day_avg'],  # Keep original for reference
                'data_quality': data_quality,
                'original_online': original_online
            }
            
            validated_data.append(validated_site)
        
        logger.info(f"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validated_data)}ê°œ í”Œë«í¼, {len(flagged_platforms)}ê°œ ìˆ˜ì •ë¨")
        
        if flagged_platforms:
            logger.warning(f"ğŸš¨ ìˆ˜ì •ëœ í”Œë«í¼: {', '.join(flagged_platforms[:5])}")
        
        return validated_data
    
    def _store_daily_data(self, date: str, timestamp: str, data: List[Dict]) -> int:
        """Store validated data in database"""
        logger.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            stored_count = 0
            
            for site in data:
                try:
                    cursor.execute("""
                        INSERT OR REPLACE INTO daily_data 
                        (date, timestamp, site_name, players_online, cash_players, 
                         peak_24h, seven_day_avg, data_quality, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        date,
                        timestamp,
                        site['site_name'],
                        site['players_online'],
                        site['cash_players'],
                        site['peak_24h'],
                        site['seven_day_avg'],
                        site['data_quality'],
                        datetime.now().isoformat()
                    ))
                    stored_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨ - {site['site_name']}: {e}")
            
            conn.commit()
        
        return stored_count
    
    def _log_collection(self, date: str, timestamp: str, raw_data: List[Dict], validated_data: List[Dict]):
        """Log collection process"""
        flagged_count = len([d for d in validated_data if d['data_quality'] != 'normal'])
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO collection_log 
                (date, timestamp, platforms_collected, platforms_validated, 
                 platforms_flagged, status, notes, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                date,
                timestamp,
                len(raw_data),
                len(validated_data),
                flagged_count,
                'completed',
                f"ì •ìƒ ìˆ˜ì§‘: {len(validated_data) - flagged_count}ê°œ, ìˆ˜ì •ë¨: {flagged_count}ê°œ",
                datetime.now().isoformat()
            ))
            
            conn.commit()
    
    def get_historical_data(self, site_name: str, days: int = 7) -> List[Dict]:
        """Get historical data for a specific platform"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT date, players_online, cash_players, peak_24h, data_quality
                FROM daily_data
                WHERE site_name = ?
                ORDER BY date DESC
                LIMIT ?
            """, (site_name, days))
            
            rows = cursor.fetchall()
            
            return [{
                'date': row[0],
                'players_online': row[1],
                'cash_players': row[2],
                'peak_24h': row[3],
                'data_quality': row[4]
            } for row in rows]
    
    def calculate_growth_from_history(self, current_data: List[Dict], days_back: int = 7) -> List[Dict]:
        """Calculate growth rates using our historical data"""
        logger.info(f"ğŸ“Š íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì„±ì¥ë¥  ê³„ì‚° (ì§€ë‚œ {days_back}ì¼)")
        
        growth_results = []
        target_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            for site in current_data:
                site_name = site['site_name']
                current_online = site['players_online']
                
                # Get historical data
                cursor.execute("""
                    SELECT players_online FROM daily_data
                    WHERE site_name = ? AND date = ?
                """, (site_name, target_date))
                
                historical_row = cursor.fetchone()
                
                if historical_row:
                    historical_online = historical_row[0]
                    
                    if historical_online > 0:
                        growth_rate = ((current_online - historical_online) / historical_online) * 100
                        growth_type = 'calculated'
                    else:
                        growth_rate = 0 if current_online == 0 else float('inf')
                        growth_type = 'infinite'
                else:
                    # No historical data - use PokerScout's 7-day avg as fallback
                    pokerscount_avg = site.get('seven_day_avg', 0)
                    if pokerscount_avg > 0:
                        growth_rate = ((current_online - pokerscount_avg) / pokerscount_avg) * 100
                        growth_type = 'fallback'
                    else:
                        growth_rate = 0
                        growth_type = 'no_data'
                
                growth_results.append({
                    'site_name': site_name,
                    'current_online': current_online,
                    'historical_online': historical_row[0] if historical_row else site.get('seven_day_avg', 0),
                    'growth_rate': growth_rate,
                    'growth_type': growth_type,
                    'days_back': days_back,
                    'comparison_date': target_date if historical_row else 'pokerscount_avg'
                })
        
        return growth_results
    
    def get_collection_summary(self, days: int = 7) -> Dict:
        """Get collection summary for recent days"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Get recent collections
            cursor.execute("""
                SELECT date, platforms_collected, platforms_validated, platforms_flagged, status
                FROM collection_log
                ORDER BY date DESC
                LIMIT ?
            """, (days,))
            
            recent_collections = cursor.fetchall()
            
            # Get total data points
            cursor.execute("SELECT COUNT(*) FROM daily_data")
            total_data_points = cursor.fetchone()[0]
            
            # Get unique platforms
            cursor.execute("SELECT COUNT(DISTINCT site_name) FROM daily_data")
            unique_platforms = cursor.fetchone()[0]
            
            # Get date range
            cursor.execute("SELECT MIN(date), MAX(date) FROM daily_data")
            date_range = cursor.fetchone()
            
            return {
                'total_data_points': total_data_points,
                'unique_platforms': unique_platforms,
                'date_range': {
                    'start': date_range[0],
                    'end': date_range[1]
                },
                'recent_collections': [{
                    'date': row[0],
                    'platforms_collected': row[1],
                    'platforms_validated': row[2],
                    'platforms_flagged': row[3],
                    'status': row[4]
                } for row in recent_collections],
                'collection_days': len(recent_collections)
            }

def main():
    print("=" * 80)
    print("ğŸ“… ì¼ì¼ í¬ì»¤ í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘ê¸°")
    print("=" * 80)
    
    collector = DailyDataCollector()
    
    # Collect today's data
    result = collector.collect_daily_data()
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"ìƒíƒœ: {result['status']}")
    print(f"ë‚ ì§œ: {result['date']}")
    
    if result['status'] == 'success':
        print(f"ìˆ˜ì§‘ëœ í”Œë«í¼: {result['platforms_collected']}ê°œ")
        print(f"ì €ì¥ëœ í”Œë«í¼: {result['platforms_stored']}ê°œ")
        print(f"íƒ€ì„ìŠ¤íƒ¬í”„: {result['timestamp']}")
    
    # Show summary
    summary = collector.get_collection_summary()
    
    print(f"\nğŸ“ˆ ìˆ˜ì§‘ í˜„í™©:")
    print(f"ì´ ë°ì´í„° í¬ì¸íŠ¸: {summary['total_data_points']:,}ê°œ")
    print(f"ì¶”ì  í”Œë«í¼: {summary['unique_platforms']}ê°œ")
    print(f"ìˆ˜ì§‘ ê¸°ê°„: {summary['date_range']['start']} ~ {summary['date_range']['end']}")
    print(f"ìµœê·¼ ìˆ˜ì§‘ ì¼ìˆ˜: {summary['collection_days']}ì¼")
    
    print("\n" + "=" * 80)
    print("âœ… ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()