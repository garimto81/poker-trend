# í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸° ê¸°ìˆ  ì•„í‚¤í…ì²˜

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     í”„ë¡ íŠ¸ì—”ë“œ (ëŒ€ì‹œë³´ë“œ)                      â”‚
â”‚                   React + TypeScript + D3.js                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Gateway                           â”‚
â”‚                      FastAPI + Redis                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë°ì´í„° ìˆ˜ì§‘ê¸°   â”‚   â”‚   ë¶„ì„ ì—”ì§„     â”‚   â”‚  ë³´ê³ ì„œ ìƒì„±ê¸°   â”‚
â”‚  Python Scripts  â”‚   â”‚  ML/NLP Models  â”‚   â”‚  Report Builder  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ë°ì´í„°ë² ì´ìŠ¤ ê³„ì¸µ                         â”‚
â”‚              MongoDB (ë¬¸ì„œ) + PostgreSQL (ê´€ê³„í˜•)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1. ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ

### 1.1 ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ëŸ¬

```python
# collectors/reddit_collector.py
import praw
from datetime import datetime
import asyncio
from typing import List, Dict

class RedditCollector:
    def __init__(self, client_id: str, client_secret: str):
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent="PokerTrendAnalyzer/1.0"
        )
        self.target_subreddits = ['poker', 'poker_theory']
        
    async def collect_posts(self, limit: int = 100) -> List[Dict]:
        """Reddit ê²Œì‹œë¬¼ ìˆ˜ì§‘"""
        posts = []
        
        for subreddit_name in self.target_subreddits:
            subreddit = self.reddit.subreddit(subreddit_name)
            
            # ì¸ê¸° ê²Œì‹œë¬¼
            for post in subreddit.hot(limit=limit//2):
                posts.append(self._extract_post_data(post))
            
            # ì‹ ê·œ ê²Œì‹œë¬¼
            for post in subreddit.new(limit=limit//2):
                posts.append(self._extract_post_data(post))
                
        return posts
    
    def _extract_post_data(self, post) -> Dict:
        return {
            'id': post.id,
            'title': post.title,
            'author': str(post.author) if post.author else 'deleted',
            'created_utc': datetime.fromtimestamp(post.created_utc),
            'score': post.score,
            'num_comments': post.num_comments,
            'url': post.url,
            'selftext': post.selftext,
            'subreddit': post.subreddit.display_name,
            'platform': 'reddit',
            'collected_at': datetime.now()
        }
```

### 1.2 íŠ¸ìœ„í„°/X ìˆ˜ì§‘ê¸°

```python
# collectors/twitter_collector.py
import tweepy
from typing import List, Dict

class TwitterCollector:
    def __init__(self, api_key: str, api_secret: str, 
                 access_token: str, access_secret: str):
        auth = tweepy.OAuthHandler(api_key, api_secret)
        auth.set_access_token(access_token, access_secret)
        self.api = tweepy.API(auth, wait_on_rate_limit=True)
        
        self.target_accounts = [
            'DNegreanu', 'phil_hellmuth', 'LexVeldhuis',
            'Stapes', 'JaimeStaples', 'PokerNews'
        ]
        
        self.poker_hashtags = [
            '#poker', '#WSOP', '#WPT', '#pokerlife',
            '#onlinepoker', '#pokerplayer'
        ]
    
    async def collect_tweets(self) -> List[Dict]:
        """íŠ¸ìœ— ìˆ˜ì§‘"""
        tweets = []
        
        # íƒ€ê²Ÿ ê³„ì •ì˜ ìµœì‹  íŠ¸ìœ—
        for username in self.target_accounts:
            user_tweets = self.api.user_timeline(
                screen_name=username, 
                count=10, 
                tweet_mode='extended'
            )
            tweets.extend([self._extract_tweet_data(t) for t in user_tweets])
        
        # í•´ì‹œíƒœê·¸ ê²€ìƒ‰
        for hashtag in self.poker_hashtags:
            search_tweets = self.api.search_tweets(
                q=hashtag, 
                count=20, 
                tweet_mode='extended'
            )
            tweets.extend([self._extract_tweet_data(t) for t in search_tweets])
            
        return tweets
    
    def _extract_tweet_data(self, tweet) -> Dict:
        return {
            'id': tweet.id_str,
            'text': tweet.full_text,
            'author': tweet.user.screen_name,
            'created_at': tweet.created_at,
            'retweet_count': tweet.retweet_count,
            'favorite_count': tweet.favorite_count,
            'platform': 'twitter',
            'collected_at': datetime.now()
        }
```

### 1.3 ì „ë¬¸ ì‚¬ì´íŠ¸ ìŠ¤í¬ë ˆì´í¼

```python
# collectors/specialized_scraper.py
import aiohttp
from bs4 import BeautifulSoup
import asyncio

class PokerNewsScraper:
    def __init__(self):
        self.base_url = "https://www.pokernews.com"
        self.headers = {
            'User-Agent': 'PokerTrendAnalyzer/1.0'
        }
    
    async def scrape_latest_news(self) -> List[Dict]:
        """PokerNews ìµœì‹  ê¸°ì‚¬ ìŠ¤í¬ë ˆì´í•‘"""
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{self.base_url}/news/", 
                headers=self.headers
            ) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                articles = []
                for article in soup.find_all('article', limit=20):
                    articles.append(self._extract_article_data(article))
                    
                return articles
    
    def _extract_article_data(self, article) -> Dict:
        return {
            'title': article.find('h2').text.strip(),
            'url': self.base_url + article.find('a')['href'],
            'summary': article.find('p').text.strip() if article.find('p') else '',
            'published_at': self._parse_date(article.find('time')['datetime']),
            'platform': 'pokernews',
            'collected_at': datetime.now()
        }
```

## 2. ë¶„ì„ ì—”ì§„

### 2.1 NLP ë¶„ì„ ëª¨ë“ˆ

```python
# analysis/nlp_analyzer.py
from transformers import pipeline
import spacy
from typing import List, Dict
from collections import Counter

class NLPAnalyzer:
    def __init__(self):
        # ê°ì„± ë¶„ì„ ëª¨ë¸
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment"
        )
        
        # SpaCy ëª¨ë¸ (ê°œì²´ëª… ì¸ì‹)
        self.nlp = spacy.load("en_core_web_sm")
        
        # í¬ì»¤ ì „ë¬¸ ìš©ì–´ ì‚¬ì „
        self.poker_terms = {
            'strategic': ['GTO', 'exploitative', 'range', 'equity', 'EV'],
            'emotional': ['tilt', 'bad beat', 'cooler', 'suckout', 'rigged'],
            'technical': ['3bet', 'c-bet', 'check-raise', 'float', 'donk']
        }
        
    def analyze_sentiment(self, text: str) -> Dict:
        """ê°ì„± ë¶„ì„"""
        result = self.sentiment_analyzer(text[:512])[0]  # ìµœëŒ€ 512 í† í°
        
        # í¬ì»¤ íŠ¹í™” ê°ì • ë¶„ì„
        poker_emotion = self._detect_poker_emotion(text)
        
        return {
            'sentiment': result['label'],
            'confidence': result['score'],
            'poker_emotion': poker_emotion
        }
    
    def extract_entities(self, text: str) -> Dict:
        """ê°œì²´ëª… ì¶”ì¶œ"""
        doc = self.nlp(text)
        
        entities = {
            'players': [],
            'tournaments': [],
            'sites': [],
            'money': []
        }
        
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                entities['players'].append(ent.text)
            elif ent.label_ == "ORG":
                # í† ë„ˆë¨¼íŠ¸ë‚˜ ì‚¬ì´íŠ¸ êµ¬ë¶„
                if any(term in ent.text.lower() for term in ['wsop', 'wpt', 'ept']):
                    entities['tournaments'].append(ent.text)
                else:
                    entities['sites'].append(ent.text)
            elif ent.label_ == "MONEY":
                entities['money'].append(ent.text)
                
        return entities
    
    def _detect_poker_emotion(self, text: str) -> str:
        """í¬ì»¤ íŠ¹í™” ê°ì • ê°ì§€"""
        text_lower = text.lower()
        
        if any(term in text_lower for term in ['bad beat', 'suckout', 'rigged']):
            return 'frustration'
        elif any(term in text_lower for term in ['ship it', 'bink', 'crushing']):
            return 'excitement'
        elif any(term in text_lower for term in ['tilt', 'tilted', 'steaming']):
            return 'tilt'
        else:
            return 'neutral'
```

### 2.2 íŠ¸ë Œë“œ ìŠ¤ì½”ì–´ë§ ì—”ì§„

```python
# analysis/trend_scorer.py
import numpy as np
from datetime import datetime, timedelta

class TrendScorer:
    def __init__(self):
        self.weights = {
            'engagement_velocity': 0.4,
            'sentiment_intensity': 0.3,
            'narrative_potential': 0.2,
            'visual_availability': 0.1
        }
    
    def calculate_content_potential(self, trend_data: Dict) -> float:
        """ì½˜í…ì¸  ì ì¬ë ¥ ì ìˆ˜ ê³„ì‚°"""
        scores = {
            'engagement_velocity': self._calc_engagement_velocity(trend_data),
            'sentiment_intensity': self._calc_sentiment_intensity(trend_data),
            'narrative_potential': self._calc_narrative_potential(trend_data),
            'visual_availability': self._calc_visual_availability(trend_data)
        }
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = sum(
            scores[key] * self.weights[key] 
            for key in scores
        )
        
        return round(total_score, 2)
    
    def _calc_engagement_velocity(self, data: Dict) -> float:
        """ì°¸ì—¬ ì†ë„ ê³„ì‚° (0-1)"""
        recent_engagement = data.get('recent_engagement', 0)
        previous_engagement = data.get('previous_engagement', 1)
        
        if previous_engagement == 0:
            return 1.0 if recent_engagement > 0 else 0.0
            
        velocity = (recent_engagement - previous_engagement) / previous_engagement
        return min(max(velocity, 0), 1)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
    
    def _calc_sentiment_intensity(self, data: Dict) -> float:
        """ê°ì„± ê°•ë„ ê³„ì‚° (0-1)"""
        sentiment_scores = data.get('sentiment_scores', [])
        if not sentiment_scores:
            return 0.0
            
        # í‘œì¤€í¸ì°¨ê°€ í´ìˆ˜ë¡ ê°•í•œ ê°ì •
        std_dev = np.std(sentiment_scores)
        return min(std_dev * 2, 1)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
```

### 2.3 í† í”½ ëª¨ë¸ë§

```python
# analysis/topic_modeling.py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

class TopicModeler:
    def __init__(self, n_topics=10):
        self.n_topics = n_topics
        self.vectorizer = TfidfVectorizer(
            max_features=100,
            stop_words='english',
            ngram_range=(1, 3)  # 1-3 ë‹¨ì–´ ì¡°í•©
        )
        self.lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42
        )
        
    def extract_topics(self, documents: List[str]) -> List[Dict]:
        """ë¬¸ì„œ ì§‘í•©ì—ì„œ ì£¼ìš” í† í”½ ì¶”ì¶œ"""
        # TF-IDF ë²¡í„°í™”
        doc_term_matrix = self.vectorizer.fit_transform(documents)
        
        # LDA ëª¨ë¸ í•™ìŠµ
        self.lda.fit(doc_term_matrix)
        
        # ê° í† í”½ì˜ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        feature_names = self.vectorizer.get_feature_names_out()
        topics = []
        
        for topic_idx, topic in enumerate(self.lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            
            topics.append({
                'topic_id': topic_idx,
                'keywords': top_words,
                'weight': float(topic.sum())
            })
            
        return topics
```

## 3. ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œ

### 3.1 ì¼ì¼ ë¸Œë¦¬í•‘ ìƒì„±ê¸°

```python
# reporting/daily_briefing.py
from jinja2 import Template
from datetime import datetime
import json

class DailyBriefingGenerator:
    def __init__(self):
        self.template = Template("""
# í¬ì»¤ íŠ¸ë Œë“œ ì¼ì¼ ë¸Œë¦¬í•‘
## {{ date }}

### ğŸ¯ ì˜¤ëŠ˜ì˜ ìŠ¤í¬íŠ¸ë¼ì´íŠ¸
**{{ spotlight.title }}**
- ì½˜í…ì¸  ì ì¬ë ¥: {{ spotlight.score }}/10
- ì¹´í…Œê³ ë¦¬: {{ spotlight.category }}
- í•µì‹¬ ìš”ì•½: {{ spotlight.summary }}

### ğŸ“° Top 3 ì¤‘ì‹œ(Meso) íŠ¸ë Œë“œ
{% for trend in meso_trends %}
{{ loop.index }}. **{{ trend.title }}**
   - ì¶œì²˜: {{ trend.source }}
   - ì°¸ì—¬ë„: {{ trend.engagement }}
   - [ìƒì„¸ë³´ê¸°]({{ trend.url }})
{% endfor %}

### ğŸ® Top 3 ë¯¸ì‹œ(Micro) ì „ëµ í† ë¡ 
{% for discussion in micro_trends %}
{{ loop.index }}. **{{ discussion.topic }}**
   - ì£¼ìš” ë…¼ì : {{ discussion.key_points }}
   - ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘: {{ discussion.sentiment }}
{% endfor %}

### ğŸ”¥ Top 3 ë‚˜ë…¸(Nano) ë°”ì´ëŸ´ ìˆœê°„
{% for viral in nano_trends %}
{{ loop.index }}. **{{ viral.title }}**
   - í”Œë«í¼: {{ viral.platform }}
   - ì¡°íšŒìˆ˜: {{ viral.views }}
   - ê°ì •: {{ viral.emotion }}
{% endfor %}

### ğŸ“Š ë°ì´í„° ëŒ€ì‹œë³´ë“œ
- ì´ ìˆ˜ì§‘ ë°ì´í„°: {{ stats.total_collected }}
- ê¸ì •/ë¶€ì • ë¹„ìœ¨: {{ stats.sentiment_ratio }}
- ê°€ì¥ ì–¸ê¸‰ëœ í”Œë ˆì´ì–´: {{ stats.top_players }}
        """)
    
    def generate_briefing(self, analysis_results: Dict) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ì¼ ë¸Œë¦¬í•‘ ìƒì„±"""
        briefing_data = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'spotlight': self._select_spotlight(analysis_results),
            'meso_trends': self._extract_meso_trends(analysis_results),
            'micro_trends': self._extract_micro_trends(analysis_results),
            'nano_trends': self._extract_nano_trends(analysis_results),
            'stats': self._calculate_stats(analysis_results)
        }
        
        return self.template.render(**briefing_data)
```

### 3.2 ì½˜í…ì¸  ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±ê¸°

```python
# reporting/content_matrix.py
class ContentMatrixGenerator:
    def __init__(self):
        self.content_formats = [
            '60ì´ˆ ë‰´ìŠ¤ í”Œë˜ì‹œ',
            'í•«í…Œì´í¬/ë¦¬ì•¡ì…˜',
            'ì „ëµ í•´ì„¤',
            'TOP 5 ì¹´ìš´íŠ¸ë‹¤ìš´',
            'ë¯¸ì‹  íƒ€íŒŒ',
            'Before & After'
        ]
        
        self.format_templates = {
            '60ì´ˆ ë‰´ìŠ¤ í”Œë˜ì‹œ': {
                'hook': 'ì†ë³´! {event}',
                'structure': 'ìœ¡í•˜ì›ì¹™ ìš”ì•½',
                'cta': 'ë” ìì„¸í•œ ë‚´ìš©ì€...'
            },
            'í•«í…Œì´í¬/ë¦¬ì•¡ì…˜': {
                'hook': 'ì´ê±´ ì •ë§ {emotion}!',
                'structure': 'ê°œì¸ ì˜ê²¬ + ê·¼ê±°',
                'cta': 'ì—¬ëŸ¬ë¶„ ìƒê°ì€?'
            },
            # ... ê¸°íƒ€ í…œí”Œë¦¿
        }
    
    def generate_content_ideas(self, trend: Dict) -> List[Dict]:
        """íŠ¸ë Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½˜í…ì¸  ì•„ì´ë””ì–´ ìƒì„±"""
        ideas = []
        
        for format_name in self.content_formats:
            if self._is_format_suitable(trend, format_name):
                idea = {
                    'format': format_name,
                    'title': self._generate_title(trend, format_name),
                    'hook': self._generate_hook(trend, format_name),
                    'script_outline': self._generate_outline(trend, format_name),
                    'estimated_duration': self._estimate_duration(format_name)
                }
                ideas.append(idea)
                
        return ideas
```

## 4. API ë° ëŒ€ì‹œë³´ë“œ

### 4.1 FastAPI ë°±ì—”ë“œ

```python
# api/main.py
from fastapi import FastAPI, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI(title="Poker Trend Analyzer API")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/api/trends/latest")
async def get_latest_trends():
    """ìµœì‹  íŠ¸ë Œë“œ ì¡°íšŒ"""
    # MongoDBì—ì„œ ìµœì‹  ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    trends = await db.trends.find().sort("created_at", -1).limit(10).to_list()
    return {"trends": trends}

@app.get("/api/briefing/today")
async def get_today_briefing():
    """ì˜¤ëŠ˜ì˜ ë¸Œë¦¬í•‘ ì¡°íšŒ"""
    briefing = await db.briefings.find_one(
        {"date": datetime.now().strftime("%Y-%m-%d")}
    )
    return briefing

@app.post("/api/analyze/trigger")
async def trigger_analysis(background_tasks: BackgroundTasks):
    """ìˆ˜ë™ ë¶„ì„ íŠ¸ë¦¬ê±°"""
    background_tasks.add_task(run_analysis_pipeline)
    return {"message": "Analysis triggered"}

@app.get("/api/content/suggestions/{trend_id}")
async def get_content_suggestions(trend_id: str):
    """íŠ¸ë Œë“œ ê¸°ë°˜ ì½˜í…ì¸  ì œì•ˆ"""
    trend = await db.trends.find_one({"_id": trend_id})
    if not trend:
        raise HTTPException(404, "Trend not found")
    
    matrix_generator = ContentMatrixGenerator()
    suggestions = matrix_generator.generate_content_ideas(trend)
    return {"suggestions": suggestions}
```

### 4.2 React ëŒ€ì‹œë³´ë“œ

```typescript
// dashboard/src/components/TrendDashboard.tsx
import React, { useState, useEffect } from 'react';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip } from 'recharts';

interface Trend {
  id: string;
  title: string;
  score: number;
  category: string;
  sentiment: string;
  engagement: number;
}

const TrendDashboard: React.FC = () => {
  const [trends, setTrends] = useState<Trend[]>([]);
  const [selectedTrend, setSelectedTrend] = useState<Trend | null>(null);
  
  useEffect(() => {
    fetchLatestTrends();
    const interval = setInterval(fetchLatestTrends, 60000); // 1ë¶„ë§ˆë‹¤ ê°±ì‹ 
    return () => clearInterval(interval);
  }, []);
  
  const fetchLatestTrends = async () => {
    const response = await fetch('/api/trends/latest');
    const data = await response.json();
    setTrends(data.trends);
  };
  
  return (
    <div className="dashboard">
      <h1>í¬ì»¤ íŠ¸ë Œë“œ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ</h1>
      
      <div className="trend-grid">
        {trends.map(trend => (
          <TrendCard 
            key={trend.id}
            trend={trend}
            onClick={() => setSelectedTrend(trend)}
          />
        ))}
      </div>
      
      {selectedTrend && (
        <TrendDetail 
          trend={selectedTrend}
          onClose={() => setSelectedTrend(null)}
        />
      )}
    </div>
  );
};
```

## 5. ìë™í™” ë° ìŠ¤ì¼€ì¤„ë§

### 5.1 ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬

```python
# scheduler/data_collector_scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import asyncio

class DataCollectionScheduler:
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.collectors = {
            'reddit': RedditCollector(),
            'twitter': TwitterCollector(),
            'pokernews': PokerNewsScraper()
        }
        
    def setup_schedules(self):
        """ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # 5ë¶„ë§ˆë‹¤ ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘
        self.scheduler.add_job(
            self.collect_social_media,
            'interval',
            minutes=5,
            id='social_media_collection'
        )
        
        # 30ë¶„ë§ˆë‹¤ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìŠ¤í¬ë ˆì´í•‘
        self.scheduler.add_job(
            self.scrape_news_sites,
            'interval',
            minutes=30,
            id='news_scraping'
        )
        
        # ë§¤ì¼ ì˜¤ì „ 8ì‹œ ì¼ì¼ ë¸Œë¦¬í•‘ ìƒì„±
        self.scheduler.add_job(
            self.generate_daily_briefing,
            'cron',
            hour=8,
            minute=0,
            id='daily_briefing'
        )
        
    async def collect_social_media(self):
        """ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„° ìˆ˜ì§‘"""
        tasks = []
        for platform, collector in self.collectors.items():
            if platform in ['reddit', 'twitter']:
                tasks.append(collector.collect_data())
        
        results = await asyncio.gather(*tasks)
        await self.save_to_database(results)
```

## 6. ë°°í¬ ë° ìš´ì˜

### 6.1 Docker ì»¨í…Œì´ë„ˆí™”

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# SpaCy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
RUN python -m spacy download en_core_web_sm

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY . .

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONUNBUFFERED=1

# ì‹¤í–‰
CMD ["python", "main.py"]
```

### 6.2 Docker Compose ì„¤ì •

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MONGODB_URL=mongodb://mongodb:27017/poker_trends
      - REDIS_URL=redis://redis:6379
    depends_on:
      - mongodb
      - redis
    
  mongodb:
    image: mongo:5.0
    volumes:
      - mongo_data:/data/db
    ports:
      - "27017:27017"
    
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    
  dashboard:
    build: ./dashboard
    ports:
      - "3000:3000"
    depends_on:
      - api

volumes:
  mongo_data:
```

## 7. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 7.1 ë¡œê¹… ì„¤ì •

```python
# utils/logging_config.py
import logging
from pythonjsonlogger import jsonlogger

def setup_logging():
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •"""
    logHandler = logging.StreamHandler()
    formatter = jsonlogger.JsonFormatter()
    logHandler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.addHandler(logHandler)
    logger.setLevel(logging.INFO)
    
    return logger

# ì‚¬ìš© ì˜ˆì‹œ
logger = setup_logging()
logger.info("trend_detected", extra={
    "trend_id": "123",
    "score": 8.5,
    "category": "tournament",
    "platform": "reddit"
})
```

### 7.2 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
trends_collected = Counter(
    'poker_trends_collected_total',
    'Total number of trends collected',
    ['platform', 'category']
)

analysis_duration = Histogram(
    'poker_analysis_duration_seconds',
    'Time spent analyzing trends'
)

active_trends = Gauge(
    'poker_active_trends',
    'Number of currently active trends',
    ['category']
)
```

ì´ ê¸°ìˆ  ì•„í‚¤í…ì²˜ëŠ” í™•ì¥ ê°€ëŠ¥í•˜ê³  ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ë¶„ì„, ë³´ê³ ì„œ ìƒì„±ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìë™í™”í•©ë‹ˆë‹¤.