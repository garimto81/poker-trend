"""
í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸° - íŠ¹ì • í‚¤ì›Œë“œ ê¸°ë°˜ YouTube ë¶„ì„
ì‚¬ìš©ì ì§€ì • í‚¤ì›Œë“œ: Holdem, WSOP, Cashgame, PokerStars, GGPoker, GTO, WPT
"""

import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import asyncio
from concurrent.futures import ThreadPoolExecutor

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
    import google.generativeai as genai
    import pandas as pd
    import logging
except ImportError as e:
    print(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("pip install google-api-python-client google-generativeai pandas")
    exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('poker_trend_analyzer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class VideoData:
    """ë¹„ë””ì˜¤ ë°ì´í„° êµ¬ì¡°"""
    video_id: str
    title: str
    description: str
    published_at: str
    view_count: int
    like_count: int
    comment_count: int
    channel_title: str
    duration: str
    keyword_matched: str
    relevance_score: float = 0.0

@dataclass
class TrendInsight:
    """íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸ êµ¬ì¡°"""
    trend_title: str
    trend_description: str
    confidence_score: float
    supporting_videos: List[str]
    trend_category: str  # 'emerging', 'stable', 'declining'
    keywords: List[str]
    impact_level: str  # 'nano', 'micro', 'meso', 'macro'

class SpecificKeywordTrendAnalyzer:
    """íŠ¹ì • í‚¤ì›Œë“œ ê¸°ë°˜ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""
    
    def __init__(self, youtube_api_key: str, gemini_api_key: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            youtube_api_key: YouTube Data API v3 í‚¤
            gemini_api_key: Gemini AI API í‚¤
        """
        self.youtube = build('youtube', 'v3', developerKey=youtube_api_key)
        genai.configure(api_key=gemini_api_key)
        self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        
        # ì‚¬ìš©ì ì§€ì • í‚¤ì›Œë“œ
        self.target_keywords = [
            "Holdem", "WSOP", "Cashgame", "PokerStars", 
            "GGPoker", "GTO", "WPT"
        ]
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥
        self.keyword_queries = {
            "Holdem": ["Texas Holdem", "Hold'em poker", "Holdem strategy"],
            "WSOP": ["World Series of Poker", "WSOP 2024", "WSOP bracelet"],
            "Cashgame": ["Cash game poker", "Live cash game", "Online cash"],
            "PokerStars": ["PokerStars tournament", "PokerStars live", "PS poker"],
            "GGPoker": ["GG Poker online", "GGPoker tournament", "GG network"],
            "GTO": ["GTO poker", "Game theory optimal", "GTO solver"],
            "WPT": ["World Poker Tour", "WPT tournament", "WPT final table"]
        }
        
        self.collected_videos: List[VideoData] = []
        
    async def collect_videos_for_keyword(self, keyword: str, max_results: int = 50) -> List[VideoData]:
        """
        íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•´ YouTube ë¹„ë””ì˜¤ ìˆ˜ì§‘
        
        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            max_results: ìˆ˜ì§‘í•  ìµœëŒ€ ë¹„ë””ì˜¤ ìˆ˜
            
        Returns:
            ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        videos = []
        queries = self.keyword_queries.get(keyword, [keyword])
        
        for query in queries:
            try:
                logger.info(f"í‚¤ì›Œë“œ '{query}' ê²€ìƒ‰ ì‹œì‘...")
                
                # ìµœì‹  1ê°œì›” ë‚´ ë¹„ë””ì˜¤ ê²€ìƒ‰
                published_after = (datetime.now() - timedelta(days=30)).isoformat() + 'Z'
                
                search_response = self.youtube.search().list(
                    q=query,
                    part='id,snippet',
                    maxResults=min(20, max_results // len(queries)),
                    order='relevance',
                    type='video',
                    publishedAfter=published_after,
                    regionCode='US',
                    relevanceLanguage='en'
                ).execute()
                
                video_ids = [item['id']['videoId'] for item in search_response['items']]
                
                if video_ids:
                    # ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                    video_details = self.youtube.videos().list(
                        part='statistics,snippet,contentDetails',
                        id=','.join(video_ids)
                    ).execute()
                    
                    for item in video_details['items']:
                        try:
                            stats = item['statistics']
                            snippet = item['snippet']
                            
                            video = VideoData(
                                video_id=item['id'],
                                title=snippet['title'],
                                description=snippet.get('description', ''),
                                published_at=snippet['publishedAt'],
                                view_count=int(stats.get('viewCount', 0)),
                                like_count=int(stats.get('likeCount', 0)),
                                comment_count=int(stats.get('commentCount', 0)),
                                channel_title=snippet['channelTitle'],
                                duration=item['contentDetails']['duration'],
                                keyword_matched=keyword,
                                relevance_score=self._calculate_relevance_score(
                                    snippet['title'] + ' ' + snippet.get('description', ''),
                                    keyword
                                )
                            )
                            videos.append(video)
                            
                        except (KeyError, ValueError) as e:
                            logger.warning(f"ë¹„ë””ì˜¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
                
                # API ì œí•œ ê³ ë ¤ ëŒ€ê¸°
                await asyncio.sleep(0.1)
                
            except HttpError as e:
                logger.error(f"YouTube API ì˜¤ë¥˜ (í‚¤ì›Œë“œ: {query}): {e}")
                continue
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        videos.sort(key=lambda x: x.relevance_score, reverse=True)
        return videos[:max_results]
    
    def _calculate_relevance_score(self, text: str, keyword: str) -> float:
        """
        í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            keyword: ê¸°ì¤€ í‚¤ì›Œë“œ
            
        Returns:
            ê´€ë ¨ì„± ì ìˆ˜ (0.0-1.0)
        """
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        # ê¸°ë³¸ ì ìˆ˜: í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        base_score = 0.5 if keyword_lower in text_lower else 0.0
        
        # ì¶”ê°€ ì ìˆ˜ ìš”ì¸ë“¤
        poker_terms = [
            'poker', 'tournament', 'cash', 'game', 'strategy', 'bluff',
            'fold', 'bet', 'raise', 'call', 'all-in', 'final table'
        ]
        
        bonus_score = sum(0.05 for term in poker_terms if term in text_lower)
        
        # ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ ì‹œ ì¶”ê°€ ì ìˆ˜
        title_bonus = 0.3 if keyword_lower in text_lower[:100] else 0.0
        
        return min(1.0, base_score + bonus_score + title_bonus)
    
    async def collect_all_videos(self) -> List[VideoData]:
        """
        ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ë¹„ë””ì˜¤ ìˆ˜ì§‘
        
        Returns:
            ì „ì²´ ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("ì „ì²´ í‚¤ì›Œë“œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_videos = []
        tasks = []
        
        # ë¹„ë™ê¸°ë¡œ ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰
        for keyword in self.target_keywords:
            task = self.collect_videos_for_keyword(keyword, 50)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"í‚¤ì›Œë“œ '{self.target_keywords[i]}' ìˆ˜ì§‘ ì‹¤íŒ¨: {result}")
            else:
                all_videos.extend(result)
                logger.info(f"í‚¤ì›Œë“œ '{self.target_keywords[i]}': {len(result)}ê°œ ë¹„ë””ì˜¤ ìˆ˜ì§‘")
        
        # ì¤‘ë³µ ì œê±° (video_id ê¸°ì¤€)
        unique_videos = {}
        for video in all_videos:
            if video.video_id not in unique_videos:
                unique_videos[video.video_id] = video
        
        self.collected_videos = list(unique_videos.values())
        logger.info(f"ì´ {len(self.collected_videos)}ê°œ ê³ ìœ  ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ìƒìœ„ 50ê°œë§Œ ì„ íƒ (ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€)
        self.collected_videos.sort(key=lambda x: x.relevance_score, reverse=True)
        self.collected_videos = self.collected_videos[:50]
        
        return self.collected_videos
    
    def prepare_gemini_prompt(self, videos: List[VideoData]) -> str:
        """
        Gemini AI ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        
        Args:
            videos: ë¶„ì„í•  ë¹„ë””ì˜¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Gemini AI í”„ë¡¬í”„íŠ¸
        """
        # ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        video_texts = []
        for i, video in enumerate(videos, 1):
            video_text = f"""
ë¹„ë””ì˜¤ #{i}:
- ì œëª©: {video.title}
- ì„¤ëª…: {video.description[:300]}...
- ì±„ë„: {video.channel_title}
- ì¡°íšŒìˆ˜: {video.view_count:,}
- ì¢‹ì•„ìš”: {video.like_count:,}
- í‚¤ì›Œë“œ: {video.keyword_matched}
- ê²Œì‹œì¼: {video.published_at}
"""
            video_texts.append(video_text)
        
        videos_content = "\n".join(video_texts)
        
        prompt = f"""
ë‹¹ì‹ ì€ í¬ì»¤ ì—…ê³„ì˜ ì „ë¬¸ íŠ¸ë Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ 50ê°œ YouTube ë¹„ë””ì˜¤ì˜ ì œëª©ê³¼ ì„¤ëª…ì„ ë¶„ì„í•˜ì—¬ í˜„ì¬ í¬ì»¤ ì»¤ë®¤ë‹ˆí‹°ì˜ íŠ¸ë Œë“œë¥¼ ì¶”ë¡ í•´ì£¼ì„¸ìš”.

ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ: {', '.join(self.target_keywords)}

YouTube ë¹„ë””ì˜¤ ë°ì´í„°:
{videos_content}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. **íŠ¸ë Œë“œ ì‹ë³„**: 1-3ê°œì˜ ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì‹ë³„
2. **íŠ¸ë Œë“œ ë¶„ë¥˜**: ê° íŠ¸ë Œë“œë¥¼ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜
   - Nano (ë°”ì´ëŸ´, ìˆ˜ì‹œê°„-ìˆ˜ì¼)
   - Micro (ì „ëµ/ê¸°ìˆ , ê°€ë³€ì )
   - Meso (í† ë„ˆë¨¼íŠ¸/ì¸ë¬¼, ìˆ˜ì¼-ìˆ˜ì£¼)
   - Macro (ì‚°ì—… ë³€í™”, 6-24ê°œì›”)

3. **ë¶„ì„ ê²°ê³¼ JSON í˜•ì‹**:
```json
{{
  "analysis_date": "{datetime.now().isoformat()}",
  "total_videos_analyzed": {len(videos)},
  "trends": [
    {{
      "trend_title": "íŠ¸ë Œë“œ ì œëª©",
      "trend_description": "íŠ¸ë Œë“œ ìƒì„¸ ì„¤ëª… (2-3ë¬¸ì¥)",
      "confidence_score": 0.85,
      "supporting_videos": ["video_id1", "video_id2"],
      "trend_category": "emerging/stable/declining",
      "keywords": ["ê´€ë ¨í‚¤ì›Œë“œ1", "ê´€ë ¨í‚¤ì›Œë“œ2"],
      "impact_level": "nano/micro/meso/macro",
      "content_potential": "ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ",
      "recommended_action": "ì½˜í…ì¸  ì œì‘ ê¶Œì¥ì‚¬í•­"
    }}
  ],
  "keyword_insights": {{
    "most_trending": "ê°€ì¥ íŠ¸ë Œë”©í•œ í‚¤ì›Œë“œ",
    "emerging_themes": ["ìƒˆë¡œ ë– ì˜¤ë¥´ëŠ” ì£¼ì œë“¤"],
    "declining_themes": ["ì¤„ì–´ë“œëŠ” ì£¼ì œë“¤"]
  }},
  "content_recommendations": [
    "ì¦‰ì‹œ ì œì‘ ê°€ëŠ¥í•œ ì½˜í…ì¸  ì•„ì´ë””ì–´ 3-5ê°œ"
  ]
}}
```

íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì— ì£¼ì˜í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
- ì¡°íšŒìˆ˜ì™€ ì¢‹ì•„ìš” ìˆ˜ê°€ ë†’ì€ ë¹„ë””ì˜¤ì˜ ê³µí†µì 
- ìµœê·¼ ê²Œì‹œëœ ë¹„ë””ì˜¤ë“¤ì˜ ì£¼ì œ ë³€í™”
- ê° í‚¤ì›Œë“œë³„ ì½˜í…ì¸ ì˜ ì„±í–¥ê³¼ íŠ¸ë Œë“œ
- í¬ì»¤ ì»¤ë®¤ë‹ˆí‹°ì˜ ê´€ì‹¬ì‚¬ ë³€í™”
- ì½˜í…ì¸  ì œì‘ìë“¤ì´ ì£¼ëª©í•˜ëŠ” ì´ìŠˆ

ë¶„ì„ ê²°ê³¼ë¥¼ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        return prompt
    
    async def analyze_with_gemini(self, videos: List[VideoData]) -> Dict[str, Any]:
        """
        Gemini AIë¥¼ ì‚¬ìš©í•œ íŠ¸ë Œë“œ ë¶„ì„
        
        Args:
            videos: ë¶„ì„í•  ë¹„ë””ì˜¤ ë°ì´í„°
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("Gemini AI íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        
        prompt = self.prepare_gemini_prompt(videos)
        
        try:
            response = self.gemini_model.generate_content(prompt)
            
            # JSON ì‘ë‹µ íŒŒì‹±
            response_text = response.text
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` ì œê±°)
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
            else:
                json_text = response_text
            
            analysis_result = json.loads(json_text)
            
            logger.info(f"íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ: {len(analysis_result.get('trends', []))}ê°œ íŠ¸ë Œë“œ ì‹ë³„")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Gemini AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # í´ë°±: ê¸°ë³¸ ë¶„ì„ ê²°ê³¼
            return {
                "analysis_date": datetime.now().isoformat(),
                "total_videos_analyzed": len(videos),
                "trends": [
                    {
                        "trend_title": "ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ",
                        "trend_description": f"Gemini AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                        "confidence_score": 0.0,
                        "supporting_videos": [],
                        "trend_category": "error",
                        "keywords": self.target_keywords,
                        "impact_level": "unknown",
                        "content_potential": "ë‚®ìŒ",
                        "recommended_action": "ìˆ˜ë™ ë¶„ì„ í•„ìš”"
                    }
                ],
                "keyword_insights": {
                    "most_trending": "ë¶„ì„ ë¶ˆê°€",
                    "emerging_themes": [],
                    "declining_themes": []
                },
                "content_recommendations": ["ì˜¤ë¥˜ í•´ê²° í›„ ì¬ë¶„ì„ í•„ìš”"]
            }
    
    def save_results(self, videos: List[VideoData], analysis: Dict[str, Any], 
                    filename_prefix: str = "poker_trend_analysis") -> str:
        """
        ë¶„ì„ ê²°ê³¼ ì €ì¥
        
        Args:
            videos: ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ ë°ì´í„°
            analysis: Gemini AI ë¶„ì„ ê²°ê³¼
            filename_prefix: íŒŒì¼ëª… ì ‘ë‘ì‚¬
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        
        # ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        videos_dict = []
        for video in videos:
            videos_dict.append({
                'video_id': video.video_id,
                'title': video.title,
                'description': video.description,
                'published_at': video.published_at,
                'view_count': video.view_count,
                'like_count': video.like_count,
                'comment_count': video.comment_count,
                'channel_title': video.channel_title,
                'duration': video.duration,
                'keyword_matched': video.keyword_matched,
                'relevance_score': video.relevance_score,
                'url': f"https://www.youtube.com/watch?v={video.video_id}"
            })
        
        result = {
            'metadata': {
                'analysis_time': datetime.now().isoformat(),
                'target_keywords': self.target_keywords,
                'total_videos_collected': len(videos),
                'analyzer_version': '1.0.0'
            },
            'videos': videos_dict,
            'gemini_analysis': analysis
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename
    
    def generate_summary_report(self, analysis: Dict[str, Any]) -> str:
        """
        ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            analysis: Gemini AI ë¶„ì„ ê²°ê³¼
            
        Returns:
            ìš”ì•½ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
        """
        report = f"""
ğŸ¯ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸
ë¶„ì„ ì¼ì‹œ: {analysis.get('analysis_date', 'Unknown')}
ë¶„ì„ ë¹„ë””ì˜¤ ìˆ˜: {analysis.get('total_videos_analyzed', 0)}ê°œ

ğŸ“ˆ ì‹ë³„ëœ íŠ¸ë Œë“œ:
"""
        
        trends = analysis.get('trends', [])
        for i, trend in enumerate(trends, 1):
            report += f"""
{i}. {trend.get('trend_title', 'Unknown')}
   - ì¹´í…Œê³ ë¦¬: {trend.get('trend_category', 'unknown')} ({trend.get('impact_level', 'unknown')} ë ˆë²¨)
   - ì‹ ë¢°ë„: {trend.get('confidence_score', 0):.2f}
   - ì„¤ëª…: {trend.get('trend_description', 'No description')}
   - ì½˜í…ì¸  ì ì¬ë ¥: {trend.get('content_potential', 'unknown')}
   - ì¶”ì²œ ì•¡ì…˜: {trend.get('recommended_action', 'No action')}
"""
        
        keyword_insights = analysis.get('keyword_insights', {})
        report += f"""
ğŸ”¥ í‚¤ì›Œë“œ ì¸ì‚¬ì´íŠ¸:
- ê°€ì¥ íŠ¸ë Œë”©: {keyword_insights.get('most_trending', 'Unknown')}
- ë– ì˜¤ë¥´ëŠ” ì£¼ì œ: {', '.join(keyword_insights.get('emerging_themes', []))}
- ì¤„ì–´ë“œëŠ” ì£¼ì œ: {', '.join(keyword_insights.get('declining_themes', []))}

ğŸ’¡ ì½˜í…ì¸  ì œì‘ ê¶Œì¥ì‚¬í•­:
"""
        
        recommendations = analysis.get('content_recommendations', [])
        for i, rec in enumerate(recommendations, 1):
            report += f"{i}. {rec}\n"
        
        return report

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    from dotenv import load_dotenv
    load_dotenv()
    
    # API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    youtube_api_key = os.getenv('YOUTUBE_API_KEY')
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    
    if not youtube_api_key or not gemini_api_key or \
       youtube_api_key == 'your_youtube_api_key_here' or \
       gemini_api_key == 'your_gemini_api_key_here':
        print("API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("1. .env íŒŒì¼ì„ í¸ì§‘í•˜ì„¸ìš”")
        print("2. YOUTUBE_API_KEYì™€ GEMINI_API_KEYì— ì‹¤ì œ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        print("\nAPI í‚¤ ìƒì„± ë°©ë²•:")
        print("YouTube API: https://console.developers.google.com/")
        print("Gemini AI: https://makersuite.google.com/app/apikey")
        return
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = SpecificKeywordTrendAnalyzer(youtube_api_key, gemini_api_key)
    
    try:
        # 1. ë¹„ë””ì˜¤ ìˆ˜ì§‘
        print("ğŸ” í‚¤ì›Œë“œë³„ YouTube ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì¤‘...")
        videos = await analyzer.collect_all_videos()
        
        print(f"âœ… ì´ {len(videos)}ê°œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # 2. Gemini AI ë¶„ì„
        print("ğŸ¤– Gemini AI íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        analysis = await analyzer.analyze_with_gemini(videos)
        
        # 3. ê²°ê³¼ ì €ì¥
        print("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
        saved_file = analyzer.save_results(videos, analysis)
        
        # 4. ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        print("=" * 60)
        summary = analyzer.generate_summary_report(analysis)
        print(summary)
        
        print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼ëŠ” {saved_file} íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())