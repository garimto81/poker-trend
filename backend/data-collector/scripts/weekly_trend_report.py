#!/usr/bin/env python3
"""
ì£¼ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸
ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 10ì‹œ ì‹¤í–‰ (ì²«ì§¸ì£¼ ì œì™¸)
"""

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import requests
from collections import Counter, defaultdict
from googleapiclient.discovery import build
import google.generativeai as genai

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# API ì´ˆê¸°í™”
youtube = build('youtube', 'v3', developerKey=os.getenv('YOUTUBE_API_KEY'))
slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
gemini_model = genai.GenerativeModel('gemini-pro')


class WeeklyTrendAnalyzer:
    """ì£¼ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.week_data = {
            'total_videos': 0,
            'total_views': 0,
            'daily_trends': [],
            'top_keywords': Counter(),
            'category_performance': defaultdict(lambda: {'videos': 0, 'views': 0}),
            'best_times': defaultdict(int),
            'emerging_trends': [],
            'weekly_insights': []
        }
    
    def analyze_weekly_trends(self) -> Dict[str, Any]:
        """ì¼ì£¼ì¼ê°„ì˜ íŠ¸ë Œë“œ ë¶„ì„"""
        logger.info("Starting weekly trend analysis...")
        
        # ì§€ë‚œ 7ì¼ê°„ ë°ì´í„° ìˆ˜ì§‘
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        
        # ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘
        all_videos = []
        for day in range(7):
            day_date = start_date + timedelta(days=day)
            day_videos = self._collect_daily_data(day_date)
            
            self.week_data['daily_trends'].append({
                'date': day_date.strftime('%Y-%m-%d'),
                'day_name': day_date.strftime('%A'),
                'video_count': len(day_videos),
                'total_views': sum(v.get('view_count', 0) for v in day_videos),
                'avg_views': sum(v.get('view_count', 0) for v in day_videos) / len(day_videos) if day_videos else 0
            })
            
            all_videos.extend(day_videos)
        
        # ì£¼ê°„ í†µê³„ ê³„ì‚°
        self.week_data['total_videos'] = len(all_videos)
        self.week_data['total_views'] = sum(v.get('view_count', 0) for v in all_videos)
        
        # ìƒì„¸ ë¶„ì„
        self._analyze_keywords(all_videos)
        self._analyze_categories(all_videos)
        self._analyze_upload_times(all_videos)
        self._identify_emerging_trends(all_videos)
        
        # AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
        self.week_data['weekly_insights'] = self._generate_ai_insights()
        
        return self.week_data
    
    def _collect_daily_data(self, date: datetime) -> List[Dict]:
        """íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„° ìˆ˜ì§‘"""
        videos = []
        search_terms = self._get_dynamic_keywords()
        
        start_time = date.isoformat() + 'Z'
        end_time = (date + timedelta(days=1)).isoformat() + 'Z'
        
        for term in search_terms[:10]:  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            try:
                request = youtube.search().list(
                    q=term,
                    part='snippet',
                    type='video',
                    maxResults=20,
                    order='viewCount',
                    publishedAfter=start_time,
                    publishedBefore=end_time
                )
                response = request.execute()
                
                for item in response.get('items', []):
                    videos.append({
                        'video_id': item['id']['videoId'],
                        'title': item['snippet']['title'],
                        'channel_title': item['snippet']['channelTitle'],
                        'published_at': item['snippet']['publishedAt'],
                        'description': item['snippet']['description'][:200]
                    })
            except Exception as e:
                logger.error(f"Error collecting data for {term}: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ìƒì„¸ ì •ë³´ ì¶”ê°€
        unique_videos = list({v['video_id']: v for v in videos}.values())
        return self._enrich_videos(unique_videos[:50])
    
    def _get_dynamic_keywords(self) -> List[str]:
        """ë™ì  í‚¤ì›Œë“œ ë¡œë“œ"""
        try:
            keywords_file = 'data/dynamic_keywords.json'
            if os.path.exists(keywords_file):
                with open(keywords_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data['base_keywords'] + data.get('trending_keywords', [])
        except Exception as e:
            logger.error(f"Error loading keywords: {e}")
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ
        return ['poker', 'í¬ì»¤', 'holdem', 'í™€ë¤', 'WSOP', 'WPT']
    
    def _enrich_videos(self, videos: List[Dict]) -> List[Dict]:
        """ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ì¶”ê°€"""
        if not videos:
            return []
        
        video_ids = [v['video_id'] for v in videos]
        
        try:
            request = youtube.videos().list(
                part='statistics',
                id=','.join(video_ids[:50])
            )
            response = request.execute()
            
            stats_map = {
                item['id']: item['statistics']
                for item in response.get('items', [])
            }
            
            for video in videos:
                if video['video_id'] in stats_map:
                    stats = stats_map[video['video_id']]
                    video['view_count'] = int(stats.get('viewCount', 0))
                    video['like_count'] = int(stats.get('likeCount', 0))
                    video['comment_count'] = int(stats.get('commentCount', 0))
                    
                    # ì°¸ì—¬ìœ¨ ê³„ì‚°
                    views = video['view_count']
                    if views > 0:
                        video['engagement_rate'] = ((video['like_count'] + video['comment_count']) / views) * 100
                    else:
                        video['engagement_rate'] = 0
                        
                    # ì—…ë¡œë“œ ì‹œê°„ ë¶„ì„
                    published_time = datetime.fromisoformat(video['published_at'].replace('Z', '+00:00'))
                    video['upload_hour'] = published_time.hour
                    
        except Exception as e:
            logger.error(f"Error enriching videos: {e}")
        
        return videos
    
    def _analyze_keywords(self, videos: List[Dict]):
        """í‚¤ì›Œë“œ ë¶„ì„"""
        import re
        
        for video in videos:
            # ì œëª©ê³¼ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            text = f"{video['title']} {video.get('description', '')}"
            words = re.findall(r'\b[a-zA-Zê°€-í£]{3,}\b', text.lower())
            
            # ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'the', 'and', 'for', 'with', 'this', 'that', 'have', 'from'}
            keywords = [w for w in words if w not in stopwords and len(w) > 3]
            
            # ì¡°íšŒìˆ˜ë¡œ ê°€ì¤‘ì¹˜
            weight = min(video.get('view_count', 0) / 1000, 10)
            for keyword in keywords:
                self.week_data['top_keywords'][keyword] += weight
    
    def _analyze_categories(self, videos: List[Dict]):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„"""
        categories = {
            'tournament': ['WSOP', 'WPT', 'EPT', 'í† ë„ˆë¨¼íŠ¸', 'final table'],
            'cash_game': ['cash', 'ìºì‹œ', 'high stakes'],
            'online': ['online', 'ì˜¨ë¼ì¸', 'PokerStars', 'GGPoker'],
            'education': ['strategy', 'ì „ëµ', 'tutorial', 'ê°•ì˜', 'tips'],
            'entertainment': ['funny', 'ì¬ë¯¸', 'fail', 'compilation', 'highlights']
        }
        
        for video in videos:
            title_lower = video['title'].lower()
            desc_lower = video.get('description', '').lower()
            
            for cat, keywords in categories.items():
                if any(kw.lower() in title_lower or kw.lower() in desc_lower for kw in keywords):
                    self.week_data['category_performance'][cat]['videos'] += 1
                    self.week_data['category_performance'][cat]['views'] += video.get('view_count', 0)
                    break
    
    def _analyze_upload_times(self, videos: List[Dict]):
        """ì—…ë¡œë“œ ì‹œê°„ëŒ€ ë¶„ì„"""
        for video in videos:
            hour = video.get('upload_hour', 0)
            engagement = video.get('engagement_rate', 0)
            self.week_data['best_times'][hour] += engagement
    
    def _identify_emerging_trends(self, videos: List[Dict]):
        """ìƒˆë¡­ê²Œ ë– ì˜¤ë¥´ëŠ” íŠ¸ë Œë“œ ê°ì§€"""
        # ì´ë²ˆ ì£¼ í‚¤ì›Œë“œì™€ ì§€ë‚œ ì£¼ í‚¤ì›Œë“œ ë¹„êµ
        current_keywords = set([k for k, _ in self.week_data['top_keywords'].most_common(20)])
        
        # ì´ì „ ì£¼ ë°ì´í„° ë¡œë“œ (ìˆë‹¤ë©´)
        try:
            prev_report = f'reports/weekly_report_{(datetime.now() - timedelta(weeks=1)).strftime("%Y%W")}.json'
            if os.path.exists(prev_report):
                with open(prev_report, 'r', encoding='utf-8') as f:
                    prev_data = json.load(f)
                prev_keywords = set([k for k, _ in prev_data.get('top_keywords', {}).items()][:20])
                
                # ìƒˆë¡œìš´ í‚¤ì›Œë“œ
                self.week_data['emerging_trends'] = list(current_keywords - prev_keywords)[:10]
        except Exception as e:
            logger.error(f"Error loading previous report: {e}")
    
    def _generate_ai_insights(self) -> List[str]:
        """AI ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # ì£¼ê°„ ë°ì´í„° ìš”ì•½
            summary = f"""
            ì£¼ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°:
            - ì´ ì˜ìƒ: {self.week_data['total_videos']}ê°œ
            - ì´ ì¡°íšŒìˆ˜: {self.week_data['total_views']:,}íšŒ
            - TOP í‚¤ì›Œë“œ: {', '.join([k for k, _ in self.week_data['top_keywords'].most_common(10)])}
            - ì¹´í…Œê³ ë¦¬ ì„±ê³¼: {dict(self.week_data['category_performance'])}
            - ìƒˆë¡œìš´ íŠ¸ë Œë“œ: {', '.join(self.week_data['emerging_trends'][:5])}
            """
            
            prompt = f"""
            ë‹¤ìŒ ì£¼ê°„ í¬ì»¤ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ 3ê°œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
            
            {summary}
            
            ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ì´ë²ˆ ì£¼ ê°€ì¥ ì£¼ëª©í•  íŠ¸ë Œë“œ
            2. ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼ ë¶„ì„
            3. ë‹¤ìŒ ì£¼ ì½˜í…ì¸  ì œì‘ ì¶”ì²œ
            
            ê° ì¸ì‚¬ì´íŠ¸ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.
            """
            
            response = gemini_model.generate_content(prompt)
            return response.text.split('\n\n')[:3]
            
        except Exception as e:
            logger.error(f"AI insights error: {e}")
            return ["AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]


def send_weekly_report(data: Dict[str, Any]):
    """ì£¼ê°„ ë¦¬í¬íŠ¸ Slack ì „ì†¡"""
    
    kst_time = datetime.now() + timedelta(hours=9)
    week_start = (kst_time - timedelta(days=7)).strftime('%m/%d')
    week_end = kst_time.strftime('%m/%d')
    
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"ğŸ“Š í¬ì»¤ íŠ¸ë Œë“œ ì£¼ê°„ ë¦¬í¬íŠ¸ ({week_start} - {week_end})"
            }
        },
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸ“ˆ ì£¼ê°„ ìš”ì•½*\n" +
                       f"â€¢ ì´ ë¶„ì„ ì˜ìƒ: {data['total_videos']:,}ê°œ\n" +
                       f"â€¢ ì´ ì¡°íšŒìˆ˜: {data['total_views']:,}íšŒ\n" +
                       f"â€¢ ì¼í‰ê·  ì˜ìƒ: {data['total_videos'] // 7}ê°œ"
            }
        },
        {"type": "divider"}
    ]
    
    # ì¼ë³„ íŠ¸ë Œë“œ
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ“… ì¼ë³„ íŠ¸ë Œë“œ*"
        }
    })
    
    daily_text = []
    for day in data['daily_trends']:
        daily_text.append(f"â€¢ {day['date']} ({day['day_name'][:3]}): {day['video_count']}ê°œ, í‰ê·  {int(day['avg_views']):,}íšŒ")
    
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": '\n'.join(daily_text)
        }
    })
    
    blocks.append({"type": "divider"})
    
    # TOP í‚¤ì›Œë“œ
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ”¥ ì£¼ê°„ TOP 10 í‚¤ì›Œë“œ*\n" +
                   ', '.join([f"`{k}`" for k, _ in data['top_keywords'].most_common(10)])
        }
    })
    
    # ìƒˆë¡œìš´ íŠ¸ë Œë“œ
    if data['emerging_trends']:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ğŸŒŸ ìƒˆë¡­ê²Œ ë– ì˜¤ë¥´ëŠ” í‚¤ì›Œë“œ*\n{', '.join([f'`{k}`' for k in data['emerging_trends'][:5]])}"
            }
        })
    
    blocks.append({"type": "divider"})
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼*"
        }
    })
    
    cat_fields = []
    for cat, stats in sorted(data['category_performance'].items(), key=lambda x: x[1]['views'], reverse=True):
        if stats['videos'] > 0:
            avg_views = stats['views'] // stats['videos']
            emoji = {
                'tournament': 'ğŸ†',
                'cash_game': 'ğŸ’°', 
                'online': 'ğŸ’»',
                'education': 'ğŸ“š',
                'entertainment': 'ğŸ­'
            }.get(cat, 'ğŸ“Œ')
            cat_fields.append({
                "type": "mrkdwn",
                "text": f"{emoji} *{cat}*\n{stats['videos']}ê°œ (í‰ê·  {avg_views:,}íšŒ)"
            })
    
    blocks.append({
        "type": "section",
        "fields": cat_fields[:4]  # ìµœëŒ€ 4ê°œë§Œ í‘œì‹œ
    })
    
    blocks.append({"type": "divider"})
    
    # ìµœì  ì—…ë¡œë“œ ì‹œê°„
    best_hours = sorted(data['best_times'].items(), key=lambda x: x[1], reverse=True)[:3]
    if best_hours:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "*â° ìµœì  ì—…ë¡œë“œ ì‹œê°„ (KST)*\n" +
                       '\n'.join([f"{i+1}. {(h[0]+9)%24}:00 (ì°¸ì—¬ìœ¨ {h[1]:.1f})" for i, h in enumerate(best_hours)])
            }
        })
    
    blocks.append({"type": "divider"})
    
    # AI ì¸ì‚¬ì´íŠ¸
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ¤– AI ì£¼ê°„ ì¸ì‚¬ì´íŠ¸*"
        }
    })
    
    for insight in data['weekly_insights']:
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": insight[:500]
            }
        })
    
    blocks.append({"type": "divider"})
    
    # ë‹¤ìŒ ì£¼ ì¶”ì²œ
    blocks.append({
        "type": "section",
        "text": {
            "type": "mrkdwn",
            "text": "*ğŸ’¡ ë‹¤ìŒ ì£¼ ì½˜í…ì¸  ì¶”ì²œ*\n" +
                   f"1. í‚¤ì›Œë“œ í™œìš©: {', '.join([k for k, _ in data['top_keywords'].most_common(3)])}\n" +
                   f"2. ì£¼ë ¥ ì¹´í…Œê³ ë¦¬: {max(data['category_performance'].items(), key=lambda x: x[1]['views'])[0] if data['category_performance'] else 'N/A'}\n" +
                   f"3. ì—…ë¡œë“œ ì‹œê°„: {(best_hours[0][0]+9)%24 if best_hours else 20}:00 KST\n" +
                   f"4. ì‹ ê·œ íŠ¸ë Œë“œ: {', '.join(data['emerging_trends'][:3]) if data['emerging_trends'] else 'ì§€ì† ê´€ì°° í•„ìš”'}"
        }
    })
    
    # Slack ì „ì†¡
    payload = {
        "blocks": blocks,
        "text": f"ğŸ“Š í¬ì»¤ íŠ¸ë Œë“œ ì£¼ê°„ ë¦¬í¬íŠ¸ ({week_start} - {week_end})"
    }
    
    try:
        response = requests.post(
            slack_webhook_url,
            json=payload,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code == 200:
            logger.info("Weekly report sent successfully")
        else:
            logger.error(f"Slack webhook error: {response.status_code}")
            
    except Exception as e:
        logger.error(f"Error sending weekly report: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("Starting weekly poker trend analysis...")
    
    try:
        analyzer = WeeklyTrendAnalyzer()
        weekly_data = analyzer.analyze_weekly_trends()
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        os.makedirs('reports', exist_ok=True)
        report_file = f'reports/weekly_report_{datetime.now().strftime("%Y%W")}.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(weekly_data, f, ensure_ascii=False, indent=2, default=str)
        
        # Slack ì „ì†¡
        send_weekly_report(weekly_data)
        
        logger.info("Weekly trend analysis completed successfully!")
        
    except Exception as e:
        logger.error(f"Error in weekly analysis: {e}", exc_info=True)
        
        # ì—ëŸ¬ ì•Œë¦¼
        error_payload = {
            "text": f"âš ï¸ ì£¼ê°„ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
        }
        requests.post(slack_webhook_url, json=error_payload)
        
        raise


if __name__ == "__main__":
    main()