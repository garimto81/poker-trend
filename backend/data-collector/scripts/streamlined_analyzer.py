#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Í∞ÑÏÜåÌôîÎêú Ìè¨Ïª§ Ìä∏Î†åÎìú Î∂ÑÏÑùÍ∏∞
- Ïñ∏Ïñ¥/Íµ≠Í∞Ä Í∞êÏßÄ (Î≤àÏó≠ Ï†úÏô∏Î°ú ÏÜçÎèÑ Ìñ•ÏÉÅ)
- Í∞ÑÍ≤∞Ìïú ÏáºÏ∏† ÏïÑÏù¥ÎîîÏñ¥ 1Í∞ú
- ÏùºÍ¥Ñ Slack ÏóÖÎ°úÎìú
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai
from googleapiclient.discovery import build
import requests
import re

# ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú
env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StreamlinedAnalyzer:
    def __init__(self):
        self.youtube = build('youtube', 'v3', developerKey=os.getenv('YOUTUBE_API_KEY'))
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        self.slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
        
        self.keywords = [
            'poker', 'holdem', 'wsop', 'wpt', 'ept', 
            'pokerstars', 'ggpoker', 'triton poker'
        ]
    
    def detect_language_and_country(self, title, description, channel_title):
        """Ïñ∏Ïñ¥ Î∞è Íµ≠Í∞Ä Í∞êÏßÄ (Î≤àÏó≠ ÏóÜÏùå)"""
        text_sample = f"{title} {description[:100]} {channel_title}"
        
        # Ïñ∏Ïñ¥ Ìå®ÌÑ¥ Í∞êÏßÄ
        if re.search(r'[\u0B80-\u0BFF]', text_sample):  # Tamil
            return "Tamil", "India"
        elif re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text_sample):  # Japanese
            return "Japanese", "Japan"
        elif re.search(r'[\uAC00-\uD7AF]', text_sample):  # Korean
            return "Korean", "South Korea"
        elif re.search(r'[\u4E00-\u9FFF]', text_sample):  # Chinese
            return "Chinese", "China"
        elif re.search(r'[\u0400-\u04FF]', text_sample):  # Russian
            return "Russian", "Russia"
        elif re.search(r'espa√±ol|fran√ßais|portugu√™s', text_sample.lower()):
            return "Spanish/French/Portuguese", "Europe/Latin America"
        else:
            return "English", "Global"
    
    def collect_streamlined_data(self, max_results=5):
        """Í∞ÑÏÜåÌôîÎêú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        logger.info("Starting streamlined video data collection...")
        all_videos = []
        
        for keyword in self.keywords:
            try:
                request = self.youtube.search().list(
                    part='snippet',
                    q=keyword,
                    type='video',
                    maxResults=max_results,
                    order='viewCount',
                    publishedAfter='2025-08-04T00:00:00Z'
                )
                
                response = request.execute()
                videos = response.get('items', [])
                
                for video in videos:
                    video_id = video['id']['videoId']
                    snippet = video['snippet']
                    
                    # Ïñ∏Ïñ¥ Î∞è Íµ≠Í∞Ä Í∞êÏßÄ (Î≤àÏó≠ ÏóÜÏùå)
                    title = snippet.get('title', '')
                    description = snippet.get('description', '')
                    channel_title = snippet.get('channelTitle', '')
                    
                    detected_language, detected_country = self.detect_language_and_country(
                        title, description, channel_title
                    )
                    
                    # Í∏∞Î≥∏ Ï†ïÎ≥¥
                    video_data = {
                        'video_id': video_id,
                        'title': title,
                        'description': description[:200],  # 200ÏûêÎ°ú Ï†úÌïú
                        'channel_title': channel_title,
                        'published_at': snippet.get('publishedAt', ''),
                        'upload_date': snippet.get('publishedAt', '')[:10],
                        'keyword': keyword,
                        'url': f"https://youtube.com/watch?v={video_id}",
                        'language': detected_language,
                        'country': detected_country
                    }
                    
                    # ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
                    details = self.youtube.videos().list(
                        part='statistics,contentDetails',
                        id=video_id
                    ).execute()
                    
                    if details['items']:
                        stats = details['items'][0]['statistics']
                        content = details['items'][0]['contentDetails']
                        
                        video_data.update({
                            'view_count': int(stats.get('viewCount', 0)),
                            'like_count': int(stats.get('likeCount', 0)),
                            'comment_count': int(stats.get('commentCount', 0)),
                            'duration': content.get('duration', ''),
                        })
                    
                    all_videos.append(video_data)
                
                logger.info(f"Collected {len(videos)} videos for keyword: {keyword}")
                
            except Exception as e:
                logger.error(f"Error collecting data for {keyword}: {e}")
        
        # Ï§ëÎ≥µ Ï†úÍ±∞
        unique_videos = []
        seen_ids = set()
        for video in all_videos:
            if video['video_id'] not in seen_ids:
                unique_videos.append(video)
                seen_ids.add(video['video_id'])
        
        logger.info(f"Total unique videos collected: {len(unique_videos)}")
        return unique_videos
    
    def create_focused_analysis_prompt(self, top_videos):
        """ÏßëÏ§ëÎêú Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        
        video_summary = []
        for i, video in enumerate(top_videos[:5], 1):
            engagement_rate = round((video.get('like_count', 0) / max(video.get('view_count', 1), 1) * 100), 2)
            video_summary.append(f"""
{i}ÏúÑ: "{video.get('title', '')}" ({video.get('language', '')})
Ï°∞ÌöåÏàò: {video.get('view_count', 0):,} | Ï∞∏Ïó¨Ïú®: {engagement_rate}% | Íµ≠Í∞Ä: {video.get('country', '')}
Ï±ÑÎÑê: {video.get('channel_title', '')} | ÌÇ§ÏõåÎìú: {video.get('keyword', '')}
""")
        
        videos_text = "\n".join(video_summary)
        
        prompt = f"""
Ìè¨Ïª§ Ìä∏Î†åÎìú Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞ÄÎ°úÏÑú Îã§Ïùå TOP 5 ÏòÅÏÉÅÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏ÏôÄ ÏµúÍ≥†Ïùò ÏáºÏ∏† ÏïÑÏù¥ÎîîÏñ¥ 1Í∞úÎ•º Ï†úÏïàÌïòÏÑ∏Ïöî.

=== TOP 5 ÏòÅÏÉÅ Îç∞Ïù¥ÌÑ∞ ===
{videos_text}

=== Í∞ÑÍ≤∞Ìïú Î∂ÑÏÑù ÏöîÏ≤≠ ===

**1. ÌïµÏã¨ Ìä∏Î†åÎìú (Í∞Å 1Ï§Ñ)**
- Í∞ÄÏû• Ï£ºÎ™©Ìï†ÎßåÌïú Ìå®ÌÑ¥:
- Ïñ∏Ïñ¥/ÏßÄÏó≠Î≥Ñ ÌäπÏßï:
- Í≥†Ï∞∏Ïó¨Ïú® ÏΩòÌÖêÏ∏† ÌäπÏßï:

**2. Ìè¨Ïª§ Ìå¨ Í¥ÄÏã¨ÏÇ¨ TOP 3 (Í∞Å 1Ï§Ñ)**
- 1ÏúÑ Í¥ÄÏã¨ÏÇ¨:
- 2ÏúÑ Í¥ÄÏã¨ÏÇ¨:
- 3ÏúÑ Í¥ÄÏã¨ÏÇ¨:

**3. ÏµúÍ≥†Ïùò ÏáºÏ∏† ÏïÑÏù¥ÎîîÏñ¥ 1Í∞ú**
Ï†úÎ™©: [Îß§Î†•Ï†ÅÏù∏ Ï†úÎ™©]
Ïª®ÏÖâ: [30Ï¥à Ïä§ÌÜ†Î¶¨ - 2Ï§Ñ]
ÌÉÄÍ≤ü: [ÎåÄÏÉÅ ÏãúÏ≤≠Ïûê]
ÏòàÏÉÅ ÏÑ±Í≥º: [Ï°∞ÌöåÏàò ÏòàÏ∏° + Í∑ºÍ±∞ 1Ï§Ñ]
Ìï¥ÏãúÌÉúÍ∑∏: [5Í∞ú]

Î™®Îì† ÎãµÎ≥ÄÏùÄ Í∞ÑÍ≤∞ÌïòÍ≥† Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
"""
        return prompt
    
    def generate_focused_insights(self, videos):
        """ÏßëÏ§ëÎêú AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        logger.info("Generating focused AI insights...")
        
        try:
            top_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:5]
            prompt = self.create_focused_analysis_prompt(top_videos)
            
            response = self.gemini_model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"AI insight generation failed: {e}")
            return "AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§."
    
    def create_final_slack_report(self, videos, ai_insights):
        """ÏµúÏ¢Ö Slack Î¶¨Ìè¨Ìä∏ (Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏôÑÎ£å ÌõÑ ÏùºÍ¥Ñ Ï†ÑÏÜ°)"""
        top_videos = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:5]
        total_views = sum(v.get('view_count', 0) for v in videos)
        
        # Ïñ∏Ïñ¥Î≥Ñ ÌÜµÍ≥Ñ
        language_stats = {}
        for video in top_videos:
            lang = video.get('language', 'Unknown')
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        lang_summary = ", ".join([f"{lang}({count})" for lang, count in language_stats.items()])
        
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": "üé∞ Streamlined Poker Trend Report"
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M')}\nüìä Total: {len(videos)} videos | {total_views:,} views\nüåç Languages: {lang_summary}\n‚ö° Enhanced: Language detection, Country identification"
                }
            },
            {
                "type": "divider"
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": "*üèÜ TOP 5 VIDEOS WITH LANGUAGE ANALYSIS*"
                }
            }
        ]
        
        # TOP 5 ÏòÅÏÉÅ ÏÉÅÏÑ∏ Ï†ïÎ≥¥
        for i, video in enumerate(top_videos, 1):
            title = video.get('title', '')[:50] + "..." if len(video.get('title', '')) > 50 else video.get('title', '')
            channel = video.get('channel_title', '')
            views = video.get('view_count', 0)
            likes = video.get('like_count', 0)
            language = video.get('language', '')
            country = video.get('country', '')
            keyword = video.get('keyword', '')
            url = video.get('url', '')
            
            engagement = round((likes / max(views, 1) * 100), 2)
            
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{i}. <{url}|{title}>*\nüì∫ {channel} | üéØ {keyword}\nüåç {language} ({country})\nüìä {views:,} views ‚Ä¢ üëç {likes:,} ‚Ä¢ üìà {engagement}%"
                }
            })
        
        # AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÎØ∏Î¶¨Î≥¥Í∏∞ (400Ïûê Ï†úÌïú)
        insights_preview = ai_insights[:400] + "..." if len(ai_insights) > 400 else ai_insights
        
        blocks.extend([
            {
                "type": "divider"
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": "*ü§ñ FOCUSED AI INSIGHTS & BEST SHORTS IDEA*"
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"```{insights_preview}```"
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": "üìã Complete analysis with detailed insights saved to report files"
                    }
                ]
            }
        ])
        
        return {"blocks": blocks}
    
    def send_slack_notification(self, message):
        """Slack ÏïåÎ¶º Ï†ÑÏÜ°"""
        try:
            response = requests.post(self.slack_webhook, json=message, timeout=30)
            if response.status_code == 200:
                logger.info("Slack notification sent successfully")
                return True
            else:
                logger.error(f"Slack notification failed: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"Slack notification error: {e}")
            return False
    
    def run_streamlined_analysis(self):
        """Í∞ÑÏÜåÌôîÎêú Î∂ÑÏÑù Ïã§Ìñâ"""
        logger.info("=" * 60)
        logger.info("STREAMLINED POKER TREND ANALYSIS")
        logger.info("=" * 60)
        
        # 1. Îç∞Ïù¥ÌÑ∞ ÏàòÏßë (Ïñ∏Ïñ¥/Íµ≠Í∞Ä Í∞êÏßÄ)
        videos = self.collect_streamlined_data()
        
        # 2. AI Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
        ai_insights = self.generate_focused_insights(videos)
        
        # 3. TOP 5 Ï∂îÏ∂ú
        top_5 = sorted(videos, key=lambda x: x.get('view_count', 0), reverse=True)[:5]
        
        # 4. Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "analysis_type": "streamlined_with_language_detection",
            "total_videos": len(videos),
            "data_fields": [
                "title", "language", "country", "view_count", 
                "like_count", "comment_count", "channel_title", "keyword"
            ],
            "ai_insights": ai_insights,
            "top_5_videos": top_5,
            "all_videos": videos
        }
        
        # 5. ÌååÏùº Ï†ÄÏû•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = Path(__file__).parent / 'reports' / f'streamlined_analysis_{timestamp}.json'
        insights_path = Path(__file__).parent / 'reports' / f'focused_insights_{timestamp}.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        with open(insights_path, 'w', encoding='utf-8') as f:
            f.write("STREAMLINED POKER TREND ANALYSIS\n")
            f.write("=" * 50 + "\n\n")
            f.write("TOP 5 VIDEOS WITH LANGUAGE ANALYSIS:\n")
            f.write("-" * 40 + "\n")
            for i, video in enumerate(top_5, 1):
                f.write(f"{i}. {video.get('title', '')}\n")
                f.write(f"   Language: {video.get('language', '')} ({video.get('country', '')})\n")
                f.write(f"   Views: {video.get('view_count', 0):,} | Keyword: {video.get('keyword', '')}\n\n")
            
            f.write("\nFOCUSED AI INSIGHTS:\n")
            f.write("=" * 30 + "\n")
            f.write(ai_insights)
        
        logger.info(f"Report saved: {report_path}")
        logger.info(f"Insights saved: {insights_path}")
        
        # 6. **Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏôÑÎ£å ÌõÑ** ÏùºÍ¥Ñ Slack Ï†ÑÏÜ°
        logger.info("Sending comprehensive Slack report (after all data processed)...")
        slack_message = self.create_final_slack_report(videos, ai_insights)
        slack_success = self.send_slack_notification(slack_message)
        
        # 7. Í≤∞Í≥º ÏöîÏïΩ
        logger.info("=" * 60)
        logger.info("STREAMLINED ANALYSIS COMPLETED")
        logger.info("=" * 60)
        logger.info(f"‚úÖ Videos analyzed: {len(videos)}")
        logger.info(f"‚úÖ Languages detected: {len(set(v.get('language', '') for v in top_5))}")
        logger.info(f"‚úÖ Focused insights: {'Generated' if ai_insights else 'Failed'}")
        logger.info(f"‚úÖ Slack report: {'Sent' if slack_success else 'Failed'}")
        
        # TOP 5 Í∞ÑÎã® ÏöîÏïΩ
        logger.info("\nüèÜ TOP 5 SUMMARY:")
        for i, video in enumerate(top_5, 1):
            logger.info(f"{i}. {video.get('view_count', 0):,} views - {video.get('language', '')} - {video.get('title', '')[:40]}...")
        
        return report_data

if __name__ == "__main__":
    analyzer = StreamlinedAnalyzer()
    analyzer.run_streamlined_analysis()