// í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ í”Œë«í¼ - ì„±ëŠ¥ ë° ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
const { test, expect } = require('@playwright/test');
const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs');

test.describe('Performance and Error Scenario Tests', () => {
  const testResultsDir = path.join(__dirname, '..', 'test-results');
  let performanceResults = {
    benchmarks: [],
    errorHandling: [],
    loadTests: [],
    memoryUsage: []
  };

  test.beforeAll(async () => {
    if (!fs.existsSync(testResultsDir)) {
      fs.mkdirSync(testResultsDir, { recursive: true });
    }
  });

  test.afterAll(async () => {
    // ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
    const resultPath = path.join(testResultsDir, 'performance-test-results.json');
    fs.writeFileSync(resultPath, JSON.stringify(performanceResults, null, 2));
    
    console.log(`ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: ${resultPath}`);
  });

  test('ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸', async () => {
    const performanceScript = `
import time
import json
import psutil
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

class PerformanceBenchmark:
    def __init__(self):
        self.benchmark_results = {}
        self.system_metrics = {}
    
    def measure_execution_time(self, func, *args, **kwargs):
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        start_cpu = time.process_time()
        
        try:
            result = func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        end_time = time.time()
        end_cpu = time.process_time()
        
        return {
            "result": result,
            "success": success,
            "error": error,
            "wall_time": end_time - start_time,
            "cpu_time": end_cpu - start_cpu,
            "timestamp": datetime.now().isoformat()
        }
    
    def simulate_pokernews_collection(self, article_count=100):
        """PokerNews ìˆ˜ì§‘ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜"""
        articles = []
        processing_times = []
        
        for i in range(article_count):
            start = time.time()
            
            # ê¸°ì‚¬ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” RSS íŒŒì‹±, AI ë¶„ì„ ë“±)
            article = {
                "id": f"article_{i}",
                "title": f"Poker News Article {i}",
                "content": f"Content for article {i}" * 50,  # ê¸´ ë‚´ìš© ì‹œë®¬ë ˆì´ì…˜
                "processed_at": datetime.now().isoformat()
            }
            
            # ì²˜ë¦¬ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.01)  # 10ms ì²˜ë¦¬ ì‹œê°„
            
            articles.append(article)
            processing_times.append(time.time() - start)
        
        return {
            "total_articles": len(articles),
            "avg_processing_time": sum(processing_times) / len(processing_times),
            "max_processing_time": max(processing_times),
            "min_processing_time": min(processing_times),
            "throughput": len(articles) / sum(processing_times) if processing_times else 0
        }
    
    def simulate_youtube_analysis(self, video_count=200):
        """YouTube ë¶„ì„ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜"""
        videos = []
        analysis_times = []
        
        for i in range(video_count):
            start = time.time()
            
            # ë¹„ë””ì˜¤ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
            video_analysis = {
                "video_id": f"video_{i}",
                "title": f"Poker Video {i}",
                "views": 1000 + (i * 100),
                "analysis": {
                    "sentiment_score": 7.5 + (i % 3),
                    "topic_relevance": 0.8 + (i % 10) / 100,
                    "engagement_prediction": "high" if i % 3 == 0 else "medium"
                },
                "processed_at": datetime.now().isoformat()
            }
            
            # AI ë¶„ì„ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.02)  # 20ms AI ë¶„ì„ ì‹œê°„
            
            videos.append(video_analysis)
            analysis_times.append(time.time() - start)
        
        return {
            "total_videos": len(videos),
            "avg_analysis_time": sum(analysis_times) / len(analysis_times),
            "max_analysis_time": max(analysis_times),
            "throughput": len(videos) / sum(analysis_times) if analysis_times else 0
        }
    
    def simulate_platform_data_processing(self, data_points=1000):
        """í”Œë«í¼ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜"""
        data_processing_times = []
        processed_data = []
        
        for i in range(data_points):
            start = time.time()
            
            # ë°ì´í„° í¬ì¸íŠ¸ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            data_point = {
                "timestamp": datetime.now().isoformat(),
                "player_id": f"player_{i}",
                "game_type": "cash" if i % 2 == 0 else "tournament",
                "stakes": ["low", "medium", "high"][i % 3],
                "duration": 3600 + (i * 60),  # ê²Œì„ ì‹œê°„ (ì´ˆ)
                "profit_loss": (i - data_points/2) * 10  # ì†ìµ
            }
            
            # ë°ì´í„° ê²€ì¦ ë° ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.001)  # 1ms ì²˜ë¦¬ ì‹œê°„
            
            processed_data.append(data_point)
            data_processing_times.append(time.time() - start)
        
        return {
            "total_data_points": len(processed_data),
            "avg_processing_time": sum(data_processing_times) / len(data_processing_times),
            "throughput": len(processed_data) / sum(data_processing_times) if data_processing_times else 0,
            "data_size_mb": len(str(processed_data)) / (1024 * 1024)
        }
    
    def run_concurrent_load_test(self, max_workers=5):
        """ë™ì‹œì„± ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        tasks = [
            ("pokernews", self.simulate_pokernews_collection, 50),
            ("youtube", self.simulate_youtube_analysis, 100),
            ("platform", self.simulate_platform_data_processing, 500),
            ("pokernews_heavy", self.simulate_pokernews_collection, 100),
            ("youtube_heavy", self.simulate_youtube_analysis, 200)
        ]
        
        results = {}
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹œì‘
            future_to_task = {
                executor.submit(func, count): (name, func, count)
                for name, func, count in tasks
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_task):
                name, func, count = future_to_task[future]
                try:
                    result = future.result()
                    results[name] = {
                        "success": True,
                        "result": result,
                        "task_count": count
                    }
                except Exception as e:
                    results[name] = {
                        "success": False,
                        "error": str(e),
                        "task_count": count
                    }
        
        total_time = time.time() - start_time
        
        return {
            "concurrent_tasks": len(tasks),
            "total_execution_time": total_time,
            "successful_tasks": sum(1 for r in results.values() if r["success"]),
            "task_results": results
        }
    
    def monitor_system_resources(self, duration=10):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        measurements = []
        
        for i in range(duration):
            measurement = {
                "timestamp": datetime.now().isoformat(),
                "cpu_percent": psutil.cpu_percent(interval=1),
                "memory_percent": psutil.virtual_memory().percent,
                "memory_used_mb": psutil.virtual_memory().used / (1024 * 1024),
                "disk_io_read_mb": psutil.disk_io_counters().read_bytes / (1024 * 1024) if psutil.disk_io_counters() else 0,
                "disk_io_write_mb": psutil.disk_io_counters().write_bytes / (1024 * 1024) if psutil.disk_io_counters() else 0
            }
            measurements.append(measurement)
        
        return {
            "duration_seconds": duration,
            "measurements": measurements,
            "avg_cpu_percent": sum(m["cpu_percent"] for m in measurements) / len(measurements),
            "avg_memory_percent": sum(m["memory_percent"] for m in measurements) / len(measurements),
            "max_memory_used_mb": max(m["memory_used_mb"] for m in measurements)
        }
    
    def run_full_performance_test(self):
        """ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ í”Œë«í¼ - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # 1. ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\\n1ï¸âƒ£ ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        
        # PokerNews ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        pokernews_result = self.measure_execution_time(self.simulate_pokernews_collection, 100)
        print(f"   PokerNews (100 articles): {pokernews_result['wall_time']:.2f}ì´ˆ")
        self.benchmark_results["pokernews"] = pokernews_result
        
        # YouTube ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        youtube_result = self.measure_execution_time(self.simulate_youtube_analysis, 200)
        print(f"   YouTube (200 videos): {youtube_result['wall_time']:.2f}ì´ˆ")
        self.benchmark_results["youtube"] = youtube_result
        
        # Platform ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        platform_result = self.measure_execution_time(self.simulate_platform_data_processing, 1000)
        print(f"   Platform (1000 data points): {platform_result['wall_time']:.2f}ì´ˆ")
        self.benchmark_results["platform"] = platform_result
        
        # 2. ë™ì‹œì„± ë¶€í•˜ í…ŒìŠ¤íŠ¸
        print("\\n2ï¸âƒ£ ë™ì‹œì„± ë¶€í•˜ í…ŒìŠ¤íŠ¸")
        concurrent_result = self.run_concurrent_load_test(max_workers=5)
        print(f"   ë™ì‹œ ì‘ì—… ìˆ˜: {concurrent_result['concurrent_tasks']}")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {concurrent_result['total_execution_time']:.2f}ì´ˆ")
        print(f"   ì„±ê³µí•œ ì‘ì—…: {concurrent_result['successful_tasks']}/{concurrent_result['concurrent_tasks']}")
        self.benchmark_results["concurrent_load"] = concurrent_result
        
        # 3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        print("\\n3ï¸âƒ£ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ (10ì´ˆ)")
        resource_monitoring = self.monitor_system_resources(10)
        print(f"   í‰ê·  CPU ì‚¬ìš©ë¥ : {resource_monitoring['avg_cpu_percent']:.1f}%")
        print(f"   í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {resource_monitoring['avg_memory_percent']:.1f}%")
        print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {resource_monitoring['max_memory_used_mb']:.1f}MB")
        self.benchmark_results["system_resources"] = resource_monitoring
        
        # 4. ì„±ëŠ¥ í‰ê°€
        print("\\n4ï¸âƒ£ ì„±ëŠ¥ í‰ê°€")
        
        # ì²˜ë¦¬ëŸ‰ ê¸°ì¤€ (ì´ˆë‹¹ ì²˜ë¦¬ ê±´ìˆ˜)
        pokernews_throughput = pokernews_result["result"]["throughput"] if pokernews_result["success"] else 0
        youtube_throughput = youtube_result["result"]["throughput"] if youtube_result["success"] else 0
        platform_throughput = platform_result["result"]["throughput"] if platform_result["success"] else 0
        
        print(f"   PokerNews ì²˜ë¦¬ëŸ‰: {pokernews_throughput:.1f} articles/sec")
        print(f"   YouTube ì²˜ë¦¬ëŸ‰: {youtube_throughput:.1f} videos/sec")  
        print(f"   Platform ì²˜ë¦¬ëŸ‰: {platform_throughput:.1f} data points/sec")
        
        # ì „ë°˜ì  ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        performance_score = 0
        if pokernews_result["success"] and pokernews_result["wall_time"] < 5:
            performance_score += 25
        if youtube_result["success"] and youtube_result["wall_time"] < 10:
            performance_score += 25
        if platform_result["success"] and platform_result["wall_time"] < 3:
            performance_score += 25
        if concurrent_result["successful_tasks"] == concurrent_result["concurrent_tasks"]:
            performance_score += 25
        
        print(f"\\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ì ìˆ˜: {performance_score}/100")
        
        if performance_score >= 80:
            print("âœ… ì„±ëŠ¥ ë“±ê¸‰: EXCELLENT")
        elif performance_score >= 60:
            print("âš ï¸ ì„±ëŠ¥ ë“±ê¸‰: GOOD")
        else:
            print("âŒ ì„±ëŠ¥ ë“±ê¸‰: NEEDS IMPROVEMENT")
        
        return {
            "performance_score": performance_score,
            "benchmark_results": self.benchmark_results
        }

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
try:
    import psutil
    benchmark = PerformanceBenchmark()
    results = benchmark.run_full_performance_test()
    print("\\nì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸: COMPLETED")
    
except ImportError:
    print("âš ï¸ psutil ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    print("ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # psutil ì—†ì´ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    benchmark = PerformanceBenchmark()
    
    # ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
    print("ğŸš€ ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    pokernews_result = benchmark.simulate_pokernews_collection(50)
    youtube_result = benchmark.simulate_youtube_analysis(100) 
    platform_result = benchmark.simulate_platform_data_processing(500)
    
    print(f"PokerNews ì²˜ë¦¬ëŸ‰: {pokernews_result['throughput']:.1f} articles/sec")
    print(f"YouTube ì²˜ë¦¬ëŸ‰: {youtube_result['throughput']:.1f} videos/sec")
    print(f"Platform ì²˜ë¦¬ëŸ‰: {platform_result['throughput']:.1f} data points/sec")
    
    print("\\nê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: COMPLETED")
`;
    
    const tempScript = path.join(testResultsDir, 'temp_performance_test.py');
    fs.writeFileSync(tempScript, performanceScript);
    
    const result = await runPythonScript(tempScript, []);
    
    expect(result.output).toContain('ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸');
    expect(result.output).toContain('COMPLETED');
    
    // ì„±ëŠ¥ ê²°ê³¼ ê¸°ë¡
    performanceResults.benchmarks.push({
      name: 'system_performance_benchmark',
      success: result.code === 0,
      output: result.output
    });
    
    // ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if (fs.existsSync(tempScript)) {
      fs.unlinkSync(tempScript);
    }
  });

  test('ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸', async () => {
    const errorScenarioScript = `
import json
import random
import time
from datetime import datetime

class ErrorScenarioTester:
    def __init__(self):
        self.error_scenarios = {
            "api_failure": {
                "description": "API ì„œë¹„ìŠ¤ ì‹¤íŒ¨",
                "probability": 0.1,
                "recovery_strategies": ["retry", "fallback", "cache"]
            },
            "network_timeout": {
                "description": "ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ",
                "probability": 0.15,
                "recovery_strategies": ["retry", "timeout_extension"]
            },
            "rate_limit": {
                "description": "API ìš”ì²­ ì œí•œ ì´ˆê³¼",
                "probability": 0.05,
                "recovery_strategies": ["backoff", "queue", "alternative_source"]
            },
            "data_corruption": {
                "description": "ë°ì´í„° ì†ìƒ",
                "probability": 0.02,
                "recovery_strategies": ["validation", "backup_restore", "skip"]
            },
            "memory_pressure": {
                "description": "ë©”ëª¨ë¦¬ ë¶€ì¡±",
                "probability": 0.03,
                "recovery_strategies": ["garbage_collection", "batch_processing", "disk_cache"]
            },
            "dependency_failure": {
                "description": "ì™¸ë¶€ ì˜ì¡´ì„± ì‹¤íŒ¨",
                "probability": 0.08,
                "recovery_strategies": ["circuit_breaker", "alternative_service", "degraded_mode"]
            }
        }
        
        self.recovery_success_rates = {
            "retry": 0.8,
            "fallback": 0.9,
            "cache": 0.7,
            "timeout_extension": 0.6,
            "backoff": 0.85,
            "queue": 0.75,
            "alternative_source": 0.8,
            "validation": 0.95,
            "backup_restore": 0.9,
            "skip": 1.0,
            "garbage_collection": 0.7,
            "batch_processing": 0.8,
            "disk_cache": 0.85,
            "circuit_breaker": 0.9,
            "alternative_service": 0.7,
            "degraded_mode": 0.95
        }
    
    def simulate_component_operation(self, component, operation_count=100):
        """êµ¬ì„±ìš”ì†Œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
        results = {
            "component": component,
            "total_operations": operation_count,
            "successful_operations": 0,
            "failed_operations": 0,
            "errors_encountered": {},
            "recovery_attempts": {},
            "total_recovery_time": 0
        }
        
        for i in range(operation_count):
            # ì—ëŸ¬ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜
            error_occurred = None
            for error_type, config in self.error_scenarios.items():
                if random.random() < config["probability"]:
                    error_occurred = error_type
                    break
            
            if error_occurred:
                # ì—ëŸ¬ ê¸°ë¡
                if error_occurred not in results["errors_encountered"]:
                    results["errors_encountered"][error_occurred] = 0
                results["errors_encountered"][error_occurred] += 1
                
                # ë³µêµ¬ ì‹œë„
                recovery_success = self.attempt_recovery(error_occurred)
                
                if recovery_success["success"]:
                    results["successful_operations"] += 1
                    results["total_recovery_time"] += recovery_success["recovery_time"]
                    
                    if error_occurred not in results["recovery_attempts"]:
                        results["recovery_attempts"][error_occurred] = {"success": 0, "failure": 0}
                    results["recovery_attempts"][error_occurred]["success"] += 1
                else:
                    results["failed_operations"] += 1
                    if error_occurred not in results["recovery_attempts"]:
                        results["recovery_attempts"][error_occurred] = {"success": 0, "failure": 0}
                    results["recovery_attempts"][error_occurred]["failure"] += 1
            else:
                # ì •ìƒ ì‘ì—…
                results["successful_operations"] += 1
        
        # ì„±ê³µë¥  ê³„ì‚°
        results["success_rate"] = results["successful_operations"] / results["total_operations"]
        results["avg_recovery_time"] = (
            results["total_recovery_time"] / 
            sum(r["success"] for r in results["recovery_attempts"].values())
            if any(results["recovery_attempts"].values()) else 0
        )
        
        return results
    
    def attempt_recovery(self, error_type):
        """ë³µêµ¬ ì‹œë„ ì‹œë®¬ë ˆì´ì…˜"""
        config = self.error_scenarios[error_type]
        strategies = config["recovery_strategies"]
        
        # ë³µêµ¬ ì „ëµ ì„ íƒ (ì²« ë²ˆì§¸ ì „ëµ ì‹œë„)
        chosen_strategy = strategies[0]
        success_rate = self.recovery_success_rates[chosen_strategy]
        
        # ë³µêµ¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì „ëµì— ë”°ë¼ ë‹¤ë¦„)
        recovery_time_map = {
            "retry": random.uniform(0.5, 2.0),
            "fallback": random.uniform(0.1, 0.5),
            "cache": random.uniform(0.05, 0.2),
            "backoff": random.uniform(1.0, 5.0),
            "validation": random.uniform(0.2, 1.0),
            "circuit_breaker": random.uniform(0.01, 0.1),
        }
        
        recovery_time = recovery_time_map.get(chosen_strategy, random.uniform(0.5, 2.0))
        
        # ë³µêµ¬ ì„±ê³µ ì—¬ë¶€ ê²°ì •
        success = random.random() < success_rate
        
        return {
            "success": success,
            "strategy_used": chosen_strategy,
            "recovery_time": recovery_time,
            "error_type": error_type
        }
    
    def run_stress_test(self, duration_minutes=5):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        components = ["pokernews", "youtube", "platform", "slack"]
        operations_per_minute = 50  # ë¶„ë‹¹ ì‘ì—… ìˆ˜
        
        total_operations = duration_minutes * operations_per_minute
        
        print(f"ğŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ({duration_minutes}ë¶„, {total_operations} ì‘ì—…)")
        
        stress_results = {}
        
        for component in components:
            print(f"\\n{component.upper()} ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸:")
            component_result = self.simulate_component_operation(component, total_operations)
            
            print(f"  ì„±ê³µë¥ : {component_result['success_rate']:.2%}")
            print(f"  ì—ëŸ¬ ë°œìƒ: {component_result['failed_operations']}íšŒ")
            print(f"  í‰ê·  ë³µêµ¬ ì‹œê°„: {component_result['avg_recovery_time']:.2f}ì´ˆ")
            
            if component_result["errors_encountered"]:
                print("  ë°œìƒí•œ ì—ëŸ¬:")
                for error_type, count in component_result["errors_encountered"].items():
                    print(f"    {error_type}: {count}íšŒ")
            
            stress_results[component] = component_result
        
        return stress_results
    
    def test_cascading_failures(self):
        """ì—°ì‡„ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        print("\\nâ›“ï¸ ì—°ì‡„ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        
        # ì—°ì‡„ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤: YouTube API ì‹¤íŒ¨ â†’ PokerNews ê³¼ë¶€í•˜ â†’ Slack ì•Œë¦¼ í­ì£¼
        cascade_scenario = {
            "initial_failure": "youtube_api_failure",
            "cascade_effects": [
                {
                    "component": "pokernews",
                    "effect": "increased_load",
                    "severity_multiplier": 2.0
                },
                {
                    "component": "slack",
                    "effect": "notification_flood", 
                    "severity_multiplier": 3.0
                },
                {
                    "component": "platform",
                    "effect": "delayed_processing",
                    "severity_multiplier": 1.5
                }
            ]
        }
        
        cascade_results = {
            "scenario": cascade_scenario,
            "timeline": [],
            "recovery_time": 0,
            "total_impact": 0
        }
        
        # T+0: ì´ˆê¸° ì¥ì•  ë°œìƒ
        cascade_results["timeline"].append({
            "time": 0,
            "event": "YouTube API ì‹¤íŒ¨ ë°œìƒ",
            "component": "youtube",
            "impact_level": "high"
        })
        
        # T+30ì´ˆ: ì—°ì‡„ íš¨ê³¼ ì‹œì‘
        cascade_results["timeline"].append({
            "time": 30,
            "event": "PokerNews ì²˜ë¦¬ëŸ‰ ì¦ê°€ë¡œ ì¸í•œ ë¶€í•˜",
            "component": "pokernews", 
            "impact_level": "medium"
        })
        
        # T+60ì´ˆ: ì•Œë¦¼ í­ì£¼
        cascade_results["timeline"].append({
            "time": 60,
            "event": "Slack ì•Œë¦¼ ì‹œìŠ¤í…œ ê³¼ë¶€í•˜",
            "component": "slack",
            "impact_level": "high"
        })
        
        # T+90ì´ˆ: í”Œë«í¼ ì§€ì—°
        cascade_results["timeline"].append({
            "time": 90,
            "event": "í”Œë«í¼ ë¶„ì„ ì§€ì—° ë°œìƒ",
            "component": "platform",
            "impact_level": "medium"
        })
        
        # ë³µêµ¬ ì‹œë®¬ë ˆì´ì…˜
        recovery_strategies = [
            {"time": 120, "action": "YouTube API ë³µêµ¬", "success": True},
            {"time": 180, "action": "PokerNews ë¶€í•˜ ì •ìƒí™”", "success": True},
            {"time": 240, "action": "Slack ì•Œë¦¼ ëŒ€ê¸°ì—´ ì •ë¦¬", "success": True},
            {"time": 300, "action": "ì „ì²´ ì‹œìŠ¤í…œ ì •ìƒí™”", "success": True}
        ]
        
        cascade_results["recovery_actions"] = recovery_strategies
        cascade_results["recovery_time"] = 300  # 5ë¶„
        
        print("  ì—°ì‡„ ì¥ì•  íƒ€ì„ë¼ì¸:")
        for event in cascade_results["timeline"]:
            print(f"    T+{event['time']}ì´ˆ: {event['event']} ({event['impact_level']})")
        
        print("  ë³µêµ¬ ê³¼ì •:")
        for action in recovery_strategies:
            status = "âœ“" if action["success"] else "âœ—"
            print(f"    T+{action['time']}ì´ˆ: {action['action']} {status}")
        
        print(f"  ì´ ë³µêµ¬ ì‹œê°„: {cascade_results['recovery_time']}ì´ˆ")
        
        return cascade_results
    
    def generate_error_report(self, test_results):
        """ì—ëŸ¬ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "test_summary": {
                "components_tested": len(test_results),
                "total_operations": sum(r["total_operations"] for r in test_results.values()),
                "overall_success_rate": sum(r["success_rate"] for r in test_results.values()) / len(test_results),
                "total_errors": sum(r["failed_operations"] for r in test_results.values())
            },
            "component_details": test_results,
            "recommendations": []
        }
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
        for component, result in test_results.items():
            if result["success_rate"] < 0.95:
                report["recommendations"].append(
                    f"{component}: ì„±ê³µë¥  {result['success_rate']:.2%}ë¡œ ê°œì„  í•„ìš”"
                )
            
            if result["avg_recovery_time"] > 2.0:
                report["recommendations"].append(
                    f"{component}: í‰ê·  ë³µêµ¬ ì‹œê°„ {result['avg_recovery_time']:.2f}ì´ˆë¡œ ìµœì í™” í•„ìš”"
                )
        
        return report
    
    def run_full_error_test(self):
        """ì „ì²´ ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ’¥ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ í”Œë«í¼ - ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # 1. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        stress_results = self.run_stress_test(duration_minutes=2)  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 2ë¶„
        
        # 2. ì—°ì‡„ ì¥ì•  í…ŒìŠ¤íŠ¸
        cascade_results = self.test_cascading_failures()
        
        # 3. ë³´ê³ ì„œ ìƒì„±
        error_report = self.generate_error_report(stress_results)
        
        print("\\nğŸ“Š ì—ëŸ¬ í…ŒìŠ¤íŠ¸ ì¢…í•© ê²°ê³¼:")
        print(f"  ì „ì²´ ì„±ê³µë¥ : {error_report['test_summary']['overall_success_rate']:.2%}")
        print(f"  ì´ ì—ëŸ¬ ë°œìƒ: {error_report['test_summary']['total_errors']}íšŒ")
        print(f"  í…ŒìŠ¤íŠ¸í•œ êµ¬ì„±ìš”ì†Œ: {error_report['test_summary']['components_tested']}ê°œ")
        
        if error_report["recommendations"]:
            print("\\nğŸ”§ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            for rec in error_report["recommendations"]:
                print(f"  â€¢ {rec}")
        else:
            print("\\nâœ… ëª¨ë“  êµ¬ì„±ìš”ì†Œê°€ ìš°ìˆ˜í•œ ë‚´ê²°í•¨ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        
        return {
            "stress_results": stress_results,
            "cascade_results": cascade_results,
            "error_report": error_report
        }

# ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
error_tester = ErrorScenarioTester()
test_results = error_tester.run_full_error_test()

print("\\nì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸: COMPLETED")
`;
    
    const tempScript = path.join(testResultsDir, 'temp_error_scenarios.py');
    fs.writeFileSync(tempScript, errorScenarioScript);
    
    const result = await runPythonScript(tempScript, []);
    
    expect(result.output).toContain('ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸');
    expect(result.output).toContain('COMPLETED');
    
    // ì—ëŸ¬ ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡
    performanceResults.errorHandling.push({
      name: 'error_scenarios_and_recovery',
      success: result.code === 0,
      output: result.output
    });
    
    // ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if (fs.existsSync(tempScript)) {
      fs.unlinkSync(tempScript);
    }
  });

  test('ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ í…ŒìŠ¤íŠ¸', async () => {
    const memoryTestScript = `
import time
import gc
from datetime import datetime

class MemoryLeakTester:
    def __init__(self):
        self.memory_snapshots = []
        self.resource_usage = []
    
    def simulate_memory_intensive_operation(self, iterations=1000):
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜"""
        large_datasets = []
        
        for i in range(iterations):
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
            dataset = {
                "id": i,
                "data": list(range(1000)),  # 1000ê°œ ì •ìˆ˜ ë°°ì—´
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "size": 1000,
                    "type": "simulation_data",
                    "content": "x" * 100  # 100ì ë¬¸ìì—´
                }
            }
            large_datasets.append(dataset)
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % 100 == 0:
                # ì¼ë¶€ ë°ì´í„° ì •ë¦¬ (ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œì˜ ì •ë¦¬ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜)
                if len(large_datasets) > 500:
                    # ì˜¤ë˜ëœ ë°ì´í„° 50% ì •ë¦¬
                    remove_count = len(large_datasets) // 2
                    large_datasets = large_datasets[remove_count:]
                
                # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
        
        return {
            "iterations_completed": iterations,
            "final_dataset_size": len(large_datasets),
            "memory_management": "active_cleanup_applied"
        }
    
    def simulate_file_handle_management(self, file_operations=100):
        """íŒŒì¼ í•¸ë“¤ ê´€ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
        file_handles = []
        operations_log = []
        
        for i in range(file_operations):
            operation_type = ["read", "write", "append"][i % 3]
            
            # íŒŒì¼ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            file_info = {
                "handle_id": f"file_{i}",
                "operation": operation_type,
                "timestamp": datetime.now().isoformat(),
                "size": 1024 * (i + 1),  # ì ì§„ì  í¬ê¸° ì¦ê°€
                "status": "open"
            }
            
            file_handles.append(file_info)
            operations_log.append({
                "operation": f"{operation_type}_file_{i}",
                "timestamp": datetime.now().isoformat()
            })
            
            # íŒŒì¼ í•¸ë“¤ ì •ë¦¬ (20ê°œë§ˆë‹¤)
            if i > 0 and i % 20 == 0:
                # ì˜¤ë˜ëœ í•¸ë“¤ ì •ë¦¬
                cleanup_count = min(10, len(file_handles))
                for j in range(cleanup_count):
                    if j < len(file_handles):
                        file_handles[j]["status"] = "closed"
                
                # ì‹¤ì œë¡œ ë‹«íŒ í•¸ë“¤ ì œê±°
                file_handles = [f for f in file_handles if f["status"] == "open"]
                
                operations_log.append({
                    "operation": f"cleanup_{cleanup_count}_handles",
                    "timestamp": datetime.now().isoformat()
                })
        
        return {
            "total_operations": file_operations,
            "open_handles": len([f for f in file_handles if f["status"] == "open"]),
            "cleanup_operations": len([op for op in operations_log if "cleanup" in op["operation"]]),
            "operations_log": operations_log
        }
    
    def simulate_connection_pool_management(self, connections=50):
        """ì—°ê²° í’€ ê´€ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
        connection_pool = []
        connection_stats = {
            "created": 0,
            "reused": 0,
            "closed": 0,
            "max_pool_size": 10
        }
        
        for i in range(connections):
            # ì—°ê²°ì´ í•„ìš”í•œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            if len(connection_pool) < connection_stats["max_pool_size"]:
                # ìƒˆ ì—°ê²° ìƒì„±
                connection = {
                    "id": f"conn_{connection_stats['created']}",
                    "created_at": datetime.now().isoformat(),
                    "last_used": datetime.now().isoformat(),
                    "usage_count": 1,
                    "status": "active"
                }
                connection_pool.append(connection)
                connection_stats["created"] += 1
            else:
                # ê¸°ì¡´ ì—°ê²° ì¬ì‚¬ìš©
                if connection_pool:
                    reused_conn = connection_pool[0]
                    reused_conn["last_used"] = datetime.now().isoformat()
                    reused_conn["usage_count"] += 1
                    connection_stats["reused"] += 1
                    
                    # ì—°ê²°ì„ í’€ì˜ ëìœ¼ë¡œ ì´ë™ (LRU ì‹œë®¬ë ˆì´ì…˜)
                    connection_pool.append(connection_pool.pop(0))
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ìœ íœ´ ì—°ê²° ì •ë¦¬
            if i % 15 == 0:
                current_time = time.time()
                for conn in connection_pool[:]:
                    # 5ì´ˆ ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì—°ê²° ì •ë¦¬ (ì‹œë®¬ë ˆì´ì…˜)
                    if conn["usage_count"] < 2:  # ì‚¬ìš©ëŸ‰ì´ ì ì€ ì—°ê²°
                        connection_pool.remove(conn)
                        connection_stats["closed"] += 1
        
        return {
            "total_connection_requests": connections,
            "final_pool_size": len(connection_pool),
            "connections_created": connection_stats["created"],
            "connections_reused": connection_stats["reused"],
            "connections_closed": connection_stats["closed"],
            "reuse_rate": connection_stats["reused"] / connections if connections > 0 else 0
        }
    
    def run_resource_leak_detection(self):
        """ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸")
        print("=" * 40)
        
        test_results = {}
        
        # 1. ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
        print("\\n1ï¸âƒ£ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
        memory_result = self.simulate_memory_intensive_operation(500)
        print(f"   ë°˜ë³µ ì‘ì—…: {memory_result['iterations_completed']}íšŒ")
        print(f"   ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {memory_result['final_dataset_size']}ê°œ")
        print(f"   ë©”ëª¨ë¦¬ ì •ë¦¬: {memory_result['memory_management']}")
        test_results["memory_management"] = memory_result
        
        # 2. íŒŒì¼ í•¸ë“¤ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
        print("\\n2ï¸âƒ£ íŒŒì¼ í•¸ë“¤ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
        file_result = self.simulate_file_handle_management(100)
        print(f"   íŒŒì¼ ì‘ì—…: {file_result['total_operations']}íšŒ")
        print(f"   ì—´ë¦° í•¸ë“¤: {file_result['open_handles']}ê°œ")
        print(f"   ì •ë¦¬ ì‘ì—…: {file_result['cleanup_operations']}íšŒ")
        test_results["file_handle_management"] = file_result
        
        # 3. ì—°ê²° í’€ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
        print("\\n3ï¸âƒ£ ì—°ê²° í’€ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
        connection_result = self.simulate_connection_pool_management(100)
        print(f"   ì—°ê²° ìš”ì²­: {connection_result['total_connection_requests']}íšŒ")
        print(f"   í’€ í¬ê¸°: {connection_result['final_pool_size']}ê°œ")
        print(f"   ì¬ì‚¬ìš©ë¥ : {connection_result['reuse_rate']:.2%}")
        print(f"   ì •ë¦¬ëœ ì—°ê²°: {connection_result['connections_closed']}ê°œ")
        test_results["connection_pool_management"] = connection_result
        
        # 4. ì „ì²´ í‰ê°€
        print("\\n4ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ í‰ê°€")
        
        memory_score = 100 if memory_result["final_dataset_size"] < 600 else 50
        file_score = 100 if file_result["open_handles"] < 20 else 50
        connection_score = 100 if connection_result["reuse_rate"] > 0.5 else 50
        
        total_score = (memory_score + file_score + connection_score) / 3
        
        print(f"   ë©”ëª¨ë¦¬ ê´€ë¦¬: {'âœ…' if memory_score == 100 else 'âš ï¸'} ({memory_score}ì )")
        print(f"   íŒŒì¼ í•¸ë“¤: {'âœ…' if file_score == 100 else 'âš ï¸'} ({file_score}ì )")
        print(f"   ì—°ê²° í’€: {'âœ…' if connection_score == 100 else 'âš ï¸'} ({connection_score}ì )")
        print(f"   ì¢…í•© ì ìˆ˜: {total_score:.0f}/100")
        
        if total_score >= 80:
            print("\\nâœ… ë¦¬ì†ŒìŠ¤ ê´€ë¦¬: EXCELLENT")
        elif total_score >= 60:
            print("\\nâš ï¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬: GOOD")
        else:
            print("\\nâŒ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬: NEEDS IMPROVEMENT")
        
        return {
            "total_score": total_score,
            "test_results": test_results
        }

# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
memory_tester = MemoryLeakTester()
leak_test_results = memory_tester.run_resource_leak_detection()

print("\\në©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ í…ŒìŠ¤íŠ¸: COMPLETED")
`;
    
    const tempScript = path.join(testResultsDir, 'temp_memory_test.py');
    fs.writeFileSync(tempScript, memoryTestScript);
    
    const result = await runPythonScript(tempScript, []);
    
    expect(result.output).toContain('ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ íƒì§€ í…ŒìŠ¤íŠ¸');
    expect(result.output).toContain('COMPLETED');
    
    // ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
    performanceResults.memoryUsage.push({
      name: 'memory_leak_and_resource_management',
      success: result.code === 0,
      output: result.output
    });
    
    // ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if (fs.existsSync(tempScript)) {
      fs.unlinkSync(tempScript);
    }
  });

  test('ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë° í™•ì¥ì„± ê²€ì¦', async () => {
    const loadTestScript = `
import time
import threading
import queue
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

class LoadTester:
    def __init__(self):
        self.load_test_results = {}
        self.concurrent_users = [1, 5, 10, 20, 50]
        self.test_duration = 30  # 30ì´ˆ í…ŒìŠ¤íŠ¸
    
    def simulate_user_workflow(self, user_id, test_queue):
        """ë‹¨ì¼ ì‚¬ìš©ì ì›Œí¬í”Œë¡œìš° ì‹œë®¬ë ˆì´ì…˜"""
        user_results = {
            "user_id": user_id,
            "operations": [],
            "total_time": 0,
            "successful_operations": 0,
            "failed_operations": 0
        }
        
        start_time = time.time()
        operation_count = 0
        
        while time.time() - start_time < self.test_duration:
            operation_start = time.time()
            
            # í¬ì»¤ íŠ¸ë Œë“œ ì¡°íšŒ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            try:
                # YouTube íŠ¸ë Œë“œ ì¡°íšŒ (50ms ~ 200ms)
                time.sleep(0.05 + (operation_count % 4) * 0.0375)
                
                # PokerNews ë¶„ì„ (100ms ~ 300ms)
                time.sleep(0.1 + (operation_count % 3) * 0.067)
                
                # í”Œë«í¼ ë°ì´í„° ì¡°íšŒ (30ms ~ 150ms) 
                time.sleep(0.03 + (operation_count % 5) * 0.024)
                
                operation_time = time.time() - operation_start
                user_results["operations"].append({
                    "operation_id": operation_count,
                    "time": operation_time,
                    "success": True
                })
                user_results["successful_operations"] += 1
                
            except Exception as e:
                operation_time = time.time() - operation_start
                user_results["operations"].append({
                    "operation_id": operation_count,
                    "time": operation_time,
                    "success": False,
                    "error": str(e)
                })
                user_results["failed_operations"] += 1
            
            operation_count += 1
            
            # ì‚¬ìš©ì ê°„ ìš”ì²­ ê°„ê²© (0.5ì´ˆ ~ 2ì´ˆ)
            time.sleep(0.5 + (user_id % 3) * 0.5)
        
        user_results["total_time"] = time.time() - start_time
        user_results["operations_per_second"] = len(user_results["operations"]) / user_results["total_time"]
        
        test_queue.put(user_results)
        return user_results
    
    def run_concurrent_load_test(self, concurrent_users):
        """ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        print(f"\\nğŸ“ˆ ë™ì‹œ ì‚¬ìš©ì {concurrent_users}ëª… ë¶€í•˜ í…ŒìŠ¤íŠ¸")
        
        test_queue = queue.Queue()
        start_time = time.time()
        
        # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [
                executor.submit(self.simulate_user_workflow, user_id, test_queue)
                for user_id in range(concurrent_users)
            ]
            
            # ëª¨ë“  ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ ì™„ë£Œ ëŒ€ê¸°
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"   ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        total_test_time = time.time() - start_time
        
        # ê²°ê³¼ ìˆ˜ì§‘
        user_results = []
        while not test_queue.empty():
            user_results.append(test_queue.get())
        
        # í†µê³„ ê³„ì‚°
        if user_results:
            total_operations = sum(len(u["operations"]) for u in user_results)
            successful_operations = sum(u["successful_operations"] for u in user_results)
            failed_operations = sum(u["failed_operations"] for u in user_results)
            avg_response_time = sum(
                sum(op["time"] for op in u["operations"]) for u in user_results
            ) / total_operations if total_operations > 0 else 0
            
            throughput = total_operations / total_test_time if total_test_time > 0 else 0
            success_rate = successful_operations / total_operations if total_operations > 0 else 0
            
            test_result = {
                "concurrent_users": concurrent_users,
                "test_duration": total_test_time,
                "total_operations": total_operations,
                "successful_operations": successful_operations,
                "failed_operations": failed_operations,
                "success_rate": success_rate,
                "avg_response_time": avg_response_time,
                "throughput_ops_per_sec": throughput,
                "user_results": user_results
            }
            
            print(f"   ì´ ì‘ì—…: {total_operations}ê°œ")
            print(f"   ì„±ê³µë¥ : {success_rate:.2%}")
            print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.3f}ì´ˆ")
            print(f"   ì²˜ë¦¬ëŸ‰: {throughput:.1f} ops/sec")
            
            return test_result
        else:
            return {
                "concurrent_users": concurrent_users,
                "error": "No user results collected"
            }
    
    def analyze_scalability(self, load_test_results):
        """í™•ì¥ì„± ë¶„ì„"""
        print("\\nğŸ“Š í™•ì¥ì„± ë¶„ì„")
        print("=" * 40)
        
        scalability_metrics = {}
        
        print("ì‚¬ìš©ì ìˆ˜ë³„ ì„±ëŠ¥ ì§€í‘œ:")
        print("ì‚¬ìš©ì | ì²˜ë¦¬ëŸ‰(ops/sec) | ì‘ë‹µì‹œê°„(ms) | ì„±ê³µë¥ ")
        print("-" * 50)
        
        for result in load_test_results:
            if "error" not in result:
                users = result["concurrent_users"]
                throughput = result["throughput_ops_per_sec"]
                response_time = result["avg_response_time"] * 1000  # msë¡œ ë³€í™˜
                success_rate = result["success_rate"] * 100  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                
                print(f"   {users:2d}  |     {throughput:6.1f}     |    {response_time:6.1f}    | {success_rate:5.1f}%")
                
                scalability_metrics[users] = {
                    "throughput": throughput,
                    "response_time": response_time,
                    "success_rate": success_rate
                }
        
        # í™•ì¥ì„± ì ìˆ˜ ê³„ì‚°
        if len(scalability_metrics) > 1:
            users_list = sorted(scalability_metrics.keys())
            
            # ì²˜ë¦¬ëŸ‰ í™•ì¥ì„± (ì´ìƒì ìœ¼ë¡œëŠ” ì„ í˜• ì¦ê°€)
            throughput_scalability = (
                scalability_metrics[users_list[-1]]["throughput"] / 
                scalability_metrics[users_list[0]]["throughput"]
            ) / (users_list[-1] / users_list[0])
            
            # ì‘ë‹µì‹œê°„ ì•ˆì •ì„± (ë„ˆë¬´ ë§ì´ ì¦ê°€í•˜ë©´ ì•ˆë¨)
            response_time_stability = (
                scalability_metrics[users_list[0]]["response_time"] /
                scalability_metrics[users_list[-1]]["response_time"]
            )
            
            # ì„±ê³µë¥  ì•ˆì •ì„±
            min_success_rate = min(m["success_rate"] for m in scalability_metrics.values())
            success_rate_stability = min_success_rate / 100
            
            print(f"\\ní™•ì¥ì„± ì§€í‘œ:")
            print(f"  ì²˜ë¦¬ëŸ‰ í™•ì¥ì„±: {throughput_scalability:.2f} (1.0ì´ ì´ìƒì )")
            print(f"  ì‘ë‹µì‹œê°„ ì•ˆì •ì„±: {response_time_stability:.2f} (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)")
            print(f"  ì„±ê³µë¥  ì•ˆì •ì„±: {success_rate_stability:.2f} (1.0ì´ ì´ìƒì )")
            
            # ì¢…í•© í™•ì¥ì„± ì ìˆ˜
            scalability_score = (
                throughput_scalability * 40 +
                response_time_stability * 30 +
                success_rate_stability * 30
            )
            
            print(f"  ì¢…í•© í™•ì¥ì„± ì ìˆ˜: {scalability_score:.1f}/100")
            
            if scalability_score >= 80:
                print("\\nâœ… í™•ì¥ì„±: EXCELLENT")
            elif scalability_score >= 60:
                print("\\nâš ï¸ í™•ì¥ì„±: GOOD")
            else:
                print("\\nâŒ í™•ì¥ì„±: NEEDS IMPROVEMENT")
            
            return {
                "scalability_score": scalability_score,
                "metrics": scalability_metrics
            }
    
    def run_full_load_test(self):
        """ì „ì²´ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ í¬ì»¤ íŠ¸ë Œë“œ ë¶„ì„ í”Œë«í¼ - ë¶€í•˜ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        load_test_results = []
        
        # ë‹¤ì–‘í•œ ë™ì‹œ ì‚¬ìš©ì ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
        for users in self.concurrent_users:
            result = self.run_concurrent_load_test(users)
            if result:
                load_test_results.append(result)
        
        # í™•ì¥ì„± ë¶„ì„
        scalability_analysis = self.analyze_scalability(load_test_results)
        
        return {
            "load_test_results": load_test_results,
            "scalability_analysis": scalability_analysis
        }

# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
print("âš¡ ë¶€í•˜ í…ŒìŠ¤íŠ¸ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
load_tester = LoadTester()

# í…ŒìŠ¤íŠ¸ ì‹œê°„ ë‹¨ì¶• (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ê¸¸ê²Œ)
load_tester.test_duration = 10  # 10ì´ˆë¡œ ë‹¨ì¶•
load_tester.concurrent_users = [1, 5, 10]  # ì‚¬ìš©ì ìˆ˜ ë‹¨ì¶•

load_results = load_tester.run_full_load_test()

print("\\në¶€í•˜ í…ŒìŠ¤íŠ¸ ë° í™•ì¥ì„± ê²€ì¦: COMPLETED")
`;
    
    const tempScript = path.join(testResultsDir, 'temp_load_test.py');
    fs.writeFileSync(tempScript, loadTestScript);
    
    const result = await runPythonScript(tempScript, [], { timeout: 120000 }); // 2ë¶„ íƒ€ì„ì•„ì›ƒ
    
    expect(result.output).toContain('ë¶€í•˜ í…ŒìŠ¤íŠ¸');
    expect(result.output).toContain('COMPLETED');
    
    // ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
    performanceResults.loadTests.push({
      name: 'load_test_and_scalability',
      success: result.code === 0,
      output: result.output
    });
    
    // ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if (fs.existsSync(tempScript)) {
      fs.unlinkSync(tempScript);
    }
  });
});

// Helper function: Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì§€ì›)
async function runPythonScript(scriptPath, args = [], options = {}) {
  return new Promise((resolve) => {
    const defaultTimeout = options.timeout || 60000; // ê¸°ë³¸ 1ë¶„
    
    const python = spawn('python', [scriptPath, ...args], {
      stdio: ['pipe', 'pipe', 'pipe'],
      cwd: path.dirname(scriptPath),
      ...options
    });
    
    let output = '';
    let error = '';
    let timeoutId;
    
    // íƒ€ì„ì•„ì›ƒ ì„¤ì •
    if (defaultTimeout) {
      timeoutId = setTimeout(() => {
        python.kill('SIGTERM');
        resolve({
          code: -1,
          output: 'Process timed out',
          stdout: '',
          stderr: 'Process timed out'
        });
      }, defaultTimeout);
    }
    
    python.stdout.on('data', (data) => {
      output += data.toString();
    });
    
    python.stderr.on('data', (data) => {
      error += data.toString();
    });
    
    python.on('close', (code) => {
      if (timeoutId) clearTimeout(timeoutId);
      resolve({
        code,
        output: output + error,
        stdout: output,
        stderr: error
      });
    });
    
    python.on('error', (err) => {
      if (timeoutId) clearTimeout(timeoutId);
      resolve({
        code: -1,
        output: err.message,
        stdout: '',
        stderr: err.message
      });
    });
  });
}