#!/usr/bin/env python
"""
í•¸ë“œ ê°ì§€ ì‹œê°í™” ë„êµ¬
ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€ ê³¼ì •ì„ ì‹œê°í™”í•˜ì—¬ ë””ë²„ê¹… ë° íŠœë‹ì— í™œìš©
"""
import cv2
import numpy as np
import json
from pathlib import Path
import argparse
from typing import Dict, List, Any

from hand_boundary_detector import HandBoundaryDetector, MotionTracker, ObjectDetector

class HandDetectionVisualizer:
    """í•¸ë“œ ê°ì§€ ê³¼ì • ì‹œê°í™”"""
    
    def __init__(self, show_debug=True):
        self.detector = HandBoundaryDetector()
        self.show_debug = show_debug
        
        # ì‹œê°í™” ìƒ‰ìƒ ì •ì˜
        self.colors = {
            'motion': (0, 255, 255),      # ë…¸ë€ìƒ‰ - ëª¨ì…˜ ì˜ì—­
            'cards': (0, 255, 0),         # ë…¹ìƒ‰ - ì¹´ë“œ
            'chips': (255, 0, 0),         # íŒŒë€ìƒ‰ - ì¹©
            'roi': (255, 255, 0),         # ì‹œì•ˆìƒ‰ - ROI
            'hand_start': (0, 0, 255),    # ë¹¨ê°„ìƒ‰ - í•¸ë“œ ì‹œì‘
            'hand_end': (255, 0, 255),    # ë§ˆì  íƒ€ìƒ‰ - í•¸ë“œ ì¢…ë£Œ
            'text': (255, 255, 255)       # í°ìƒ‰ - í…ìŠ¤íŠ¸
        }
        
        # ìƒíƒœ ì¶”ì 
        self.frame_count = 0
        self.current_hand_info = None
        self.detection_history = []
    
    def visualize_video(self, video_path: str, output_path: str = None, save_frames: bool = False):
        """ë¹„ë””ì˜¤ ë¶„ì„ê³¼ ë™ì‹œì— ì‹œê°í™”"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps} FPS, {total_frames} í”„ë ˆì„")
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))
        else:
            out = None
        
        # í”„ë ˆì„ ì €ì¥ ë””ë ‰í† ë¦¬
        if save_frames:
            frames_dir = Path("debug_frames")
            frames_dir.mkdir(exist_ok=True)
        
        # ì²« ë²ˆì§¸ í”„ë ˆì„ìœ¼ë¡œ ROI ì„¤ì •
        ret, first_frame = cap.read()
        if ret:
            self.detector.object_detector.set_regions(first_frame.shape)
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        hand_id = 1
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = self.frame_count / fps
            
            # ì›ë³¸ í”„ë ˆì„ ë³µì‚¬
            display_frame = frame.copy()
            debug_frame = np.zeros_like(frame)
            
            # ëª¨ì…˜ ì¶”ì  ë° ê°ì²´ ê°ì§€
            motion_info = self.detector.motion_tracker.update(frame)
            cards = self.detector.object_detector.detect_cards(frame)
            chips = self.detector.object_detector.detect_chips(frame)
            
            # í•¸ë“œ ê°ì§€ ë¡œì§
            if self.detector.current_hand_start is None:
                is_start, confidence, indicators = self.detector._detect_hand_start(
                    frame, motion_info, current_time
                )
                
                if is_start:
                    self.detector.current_hand_start = {
                        'hand_id': hand_id,
                        'frame': self.frame_count,
                        'time': current_time,
                        'confidence': confidence,
                        'indicators': indicators
                    }
                    self.current_hand_info = {
                        'status': 'in_progress',
                        'start_time': current_time,
                        'hand_id': hand_id
                    }
                    print(f"í•¸ë“œ {hand_id} ì‹œì‘ ê°ì§€: {current_time:.2f}ì´ˆ (ì‹ ë¢°ë„: {confidence:.1f})")
            else:
                # ìµœì†Œ 30ì´ˆ í›„ë¶€í„° ì¢…ë£Œ ê°ì§€
                if current_time - self.detector.current_hand_start['time'] > 30:
                    is_end, confidence, indicators = self.detector._detect_hand_end(
                        frame, motion_info, current_time
                    )
                    
                    if is_end:
                        duration = current_time - self.detector.current_hand_start['time']
                        print(f"í•¸ë“œ {hand_id} ì¢…ë£Œ ê°ì§€: {current_time:.2f}ì´ˆ (ê¸¸ì´: {duration:.1f}ì´ˆ)")
                        
                        self.current_hand_info = {
                            'status': 'completed',
                            'end_time': current_time,
                            'duration': duration
                        }
                        
                        self.detector.current_hand_start = None
                        hand_id += 1
            
            # ì‹œê°í™” ìˆ˜í–‰
            self._draw_motion_regions(display_frame, debug_frame, motion_info)
            self._draw_detected_objects(display_frame, debug_frame, cards, chips)
            self._draw_roi_regions(display_frame, debug_frame)
            self._draw_hand_status(display_frame, current_time)
            self._draw_detection_info(debug_frame, motion_info, cards, chips, current_time)
            
            # ë‘ í”„ë ˆì„ì„ ë‚˜ë€íˆ ê²°í•©
            combined_frame = np.hstack([display_frame, debug_frame])
            
            # ë¹„ë””ì˜¤ ì €ì¥
            if out:
                out.write(combined_frame)
            
            # í”„ë ˆì„ ì €ì¥
            if save_frames and self.frame_count % 30 == 0:  # 1ì´ˆë§ˆë‹¤
                frame_path = frames_dir / f"frame_{self.frame_count:06d}.jpg"
                cv2.imwrite(str(frame_path), combined_frame)
            
            # ì‹¤ì‹œê°„ í‘œì‹œ (ì„ íƒì‚¬í•­)
            if self.show_debug and self.frame_count % 5 == 0:  # 5í”„ë ˆì„ë§ˆë‹¤ í‘œì‹œ
                resized = cv2.resize(combined_frame, (1280, 360))
                cv2.imshow('Hand Detection Debug', resized)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord(' '):  # ìŠ¤í˜ì´ìŠ¤ë°”ë¡œ ì¼ì‹œì •ì§€
                    cv2.waitKey(0)
            
            self.frame_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if self.frame_count % 300 == 0:
                progress = (self.frame_count / total_frames) * 100
                print(f"ì§„í–‰ë¥ : {progress:.1f}%")
        
        # ì •ë¦¬
        cap.release()
        if out:
            out.release()
        cv2.destroyAllWindows()
        
        print(f"ì‹œê°í™” ì™„ë£Œ: {self.frame_count} í”„ë ˆì„ ì²˜ë¦¬")
        if output_path:
            print(f"ê²°ê³¼ ë¹„ë””ì˜¤ ì €ì¥: {output_path}")
    
    def _draw_motion_regions(self, display_frame, debug_frame, motion_info):
        """ëª¨ì…˜ ì˜ì—­ ì‹œê°í™”"""
        for region in motion_info['motion_regions']:
            x, y, w, h = region['bbox']
            
            # ì›ë³¸ í”„ë ˆì„ì— ë°˜íˆ¬ëª… ë°•ìŠ¤
            overlay = display_frame.copy()
            cv2.rectangle(overlay, (x, y), (x + w, y + h), self.colors['motion'], -1)
            cv2.addWeighted(display_frame, 0.8, overlay, 0.2, 0, display_frame)
            
            # ë””ë²„ê·¸ í”„ë ˆì„ì— ìœ¤ê³½ì„ 
            cv2.rectangle(debug_frame, (x, y), (x + w, y + h), self.colors['motion'], 2)
            
            # ì¤‘ì‹¬ì  í‘œì‹œ
            center = region['center']
            cv2.circle(debug_frame, center, 5, self.colors['motion'], -1)
            
            # ì˜ì—­ ì •ë³´ í…ìŠ¤íŠ¸
            cv2.putText(debug_frame, f"A:{region['area']}", 
                       (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'])
    
    def _draw_detected_objects(self, display_frame, debug_frame, cards, chips):
        """ê°ì§€ëœ ê°ì²´ ì‹œê°í™”"""
        # ì¹´ë“œ ì‹œê°í™”
        for card in cards:
            if 'bbox' in card:
                x, y, w, h = card['bbox']
                cv2.rectangle(display_frame, (x, y), (x + w, y + h), self.colors['cards'], 2)
                cv2.rectangle(debug_frame, (x, y), (x + w, y + h), self.colors['cards'], 2)
                
                # ì¹´ë“œ ì •ë³´
                info_text = f"Card({card.get('confidence', 0):.1f})"
                cv2.putText(debug_frame, info_text, (x, y - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['cards'])
        
        # ì¹© ì‹œê°í™”
        for chip in chips:
            center = chip['center']
            radius = chip['radius']
            color = self.colors['chips']
            
            cv2.circle(display_frame, center, radius, color, 2)
            cv2.circle(debug_frame, center, radius, color, 2)
            
            # ì¹© ì •ë³´
            chip_text = f"{chip['color']}"
            cv2.putText(debug_frame, chip_text, (center[0] - 20, center[1] - radius - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)
    
    def _draw_roi_regions(self, display_frame, debug_frame):
        """ROI ì˜ì—­ ì‹œê°í™”"""
        # íŒŸ ì˜ì—­
        if self.detector.object_detector.pot_region:
            pot = self.detector.object_detector.pot_region
            cv2.rectangle(debug_frame, (pot['x1'], pot['y1']), 
                         (pot['x2'], pot['y2']), self.colors['roi'], 1)
            cv2.putText(debug_frame, "POT", (pot['x1'], pot['y1'] - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['roi'])
        
        # í”Œë ˆì´ì–´ ì˜ì—­
        for player_id, region in self.detector.object_detector.player_regions.items():
            cv2.rectangle(debug_frame, (region['x1'], region['y1']), 
                         (region['x2'], region['y2']), self.colors['roi'], 1)
            cv2.putText(debug_frame, f"P{player_id}", 
                       (region['x1'], region['y1'] - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['roi'])
    
    def _draw_hand_status(self, display_frame, current_time):
        """í˜„ì¬ í•¸ë“œ ìƒíƒœ í‘œì‹œ"""
        status_text = []
        
        if self.current_hand_info:
            if self.current_hand_info['status'] == 'in_progress':
                duration = current_time - self.current_hand_info['start_time']
                status_text.append(f"Hand {self.current_hand_info['hand_id']} - {duration:.1f}s")
                color = self.colors['hand_start']
            else:
                status_text.append(f"Hand Completed - {self.current_hand_info['duration']:.1f}s")
                color = self.colors['hand_end']
        else:
            status_text.append("Waiting for Hand Start")
            color = self.colors['text']
        
        # ì‹œê°„ ì •ë³´
        status_text.append(f"Time: {current_time:.1f}s")
        status_text.append(f"Frame: {self.frame_count}")
        
        # í™”ë©´ ìƒë‹¨ì— ìƒíƒœ í‘œì‹œ
        for i, text in enumerate(status_text):
            y_pos = 30 + i * 25
            cv2.putText(display_frame, text, (10, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
    
    def _draw_detection_info(self, debug_frame, motion_info, cards, chips, current_time):
        """ê°ì§€ ì •ë³´ í‘œì‹œ"""
        info_lines = [
            f"Motion Areas: {len(motion_info['motion_regions'])}",
            f"Total Motion: {motion_info['total_motion_area']}",
            f"Cards: {len(cards)}",
            f"Chips: {len(chips)}",
        ]
        
        # ëª¨ì…˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        dealing_score = self.detector.motion_tracker.analyze_motion_pattern('dealing')
        collection_score = self.detector.motion_tracker.analyze_motion_pattern('collection')
        
        info_lines.extend([
            f"Dealing Score: {dealing_score:.2f}",
            f"Collection Score: {collection_score:.2f}",
        ])
        
        # í™”ë©´ í•˜ë‹¨ì— ì •ë³´ í‘œì‹œ
        frame_height = debug_frame.shape[0]
        for i, line in enumerate(info_lines):
            y_pos = frame_height - 20 - (len(info_lines) - i - 1) * 20
            cv2.putText(debug_frame, line, (10, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'])

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í•¸ë“œ ê°ì§€ ì‹œê°í™” ë„êµ¬')
    parser.add_argument('video_path', help='ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', help='ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--save-frames', action='store_true', help='ë””ë²„ê·¸ í”„ë ˆì„ ì €ì¥')
    parser.add_argument('--no-display', action='store_true', help='ì‹¤ì‹œê°„ í‘œì‹œ ë¹„í™œì„±í™”')
    
    args = parser.parse_args()
    
    if not Path(args.video_path).exists():
        print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.video_path}")
        return
    
    print(f"ğŸ¥ í•¸ë“œ ê°ì§€ ì‹œê°í™” ì‹œì‘: {args.video_path}")
    
    visualizer = HandDetectionVisualizer(show_debug=not args.no_display)
    
    try:
        visualizer.visualize_video(
            video_path=args.video_path,
            output_path=args.output,
            save_frames=args.save_frames
        )
        print("âœ… ì‹œê°í™” ì™„ë£Œ")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()