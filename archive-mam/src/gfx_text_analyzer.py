"""
GFX í…ìŠ¤íŠ¸ ë¶„ì„ê¸° - OCR ê¸°ë°˜ GFX ì˜¤ë²„ë ˆì´ ê°ì§€
í¬ì»¤ ë°©ì†¡ì˜ GFX ì˜¤ë²„ë ˆì´ì— í¬í•¨ëœ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ ê°ì§€ ìˆ˜í–‰
"""

import cv2
import numpy as np
import pytesseract
import re
from typing import List, Dict, Tuple, Optional, Set
import json
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from collections import Counter, defaultdict
import string

# Tesseract ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ì„¤ì • (Windows ê¸°ë³¸ ê²½ë¡œ)
# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

@dataclass
class TextFeature:
    """í…ìŠ¤íŠ¸ íŠ¹ì§• ì •ë³´"""
    text: str
    confidence: float
    bbox: Tuple[int, int, int, int]
    char_count: int
    word_count: int
    digit_ratio: float
    symbol_ratio: float
    area_ratio: float

@dataclass
class GFXTextProfile:
    """GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„"""
    common_keywords: Set[str]
    typical_patterns: List[str]
    text_density_range: Tuple[float, float]
    avg_confidence: float
    char_distribution: Dict[str, float]
    spatial_distribution: Dict[str, List[Tuple[int, int]]]

class GFXTextAnalyzer:
    """GFX í…ìŠ¤íŠ¸ ë¶„ì„ê¸° - OCR ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        
        # í¬ì»¤ GFXì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œ
        self.poker_keywords = {
            # ê¸°ë³¸ ìš©ì–´
            'pot', 'call', 'raise', 'fold', 'check', 'all', 'in', 'allin',
            'bet', 'blind', 'ante', 'stack', 'chips', 'bb', 'sb',
            
            # ì¹´ë“œ ê´€ë ¨
            'hole', 'cards', 'flop', 'turn', 'river', 'showdown',
            'straight', 'flush', 'full', 'house', 'pair', 'high',
            
            # ì•¡ì…˜/ê²°ê³¼
            'wins', 'loses', 'split', 'side', 'main', 'eliminate',
            'double', 'triple', 'quad', 'royal',
            
            # ìˆ«ì/ê¸°í˜¸ íŒ¨í„´
            '$', 'â‚©', 'â‚¬', 'Â£', '%', 'k', 'm', 'bb', 'x'
        }
        
        # ì¼ë°˜ ê²Œì„ í™”ë©´ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œ (ì œì™¸ ëŒ€ìƒ)
        self.game_keywords = {
            'player', 'table', 'seat', 'join', 'leave', 'wait',
            'lobby', 'tournament', 'cash', 'game', 'level',
            'time', 'break', 'pause', 'resume', 'exit'
        }
        
        # OCR ì„¤ì •
        self.ocr_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz$â‚¬Â£â‚©%.,:-+xKMB'
        
        # í…ìŠ¤íŠ¸ íŒ¨í„´ (ì •ê·œì‹)
        self.patterns = {
            'money': r'[\$â‚¬Â£â‚©]\s*[\d,]+(?:\.\d{2})?[KMB]?',
            'percentage': r'\d+(?:\.\d+)?%',
            'ratio': r'\d+:\d+',
            'card': r'[AKQJT2-9][hdcs]',
            'time': r'\d{1,2}:\d{2}(?::\d{2})?',
            'big_blind': r'\d+(?:\.\d+)?\s*BB',
        }
        
        # í•™ìŠµëœ GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„
        self.gfx_profile = None
        
        if config_path and Path(config_path).exists():
            self.load_config(config_path)
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def load_config(self, config_path: str):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            if 'gfx_text_profile' in config:
                profile_data = config['gfx_text_profile']
                self.gfx_profile = GFXTextProfile(**profile_data)
                
            self.logger.info(f"GFX í…ìŠ¤íŠ¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config_path}")
            
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def preprocess_for_ocr(self, frame: np.ndarray) -> List[np.ndarray]:
        """OCRì„ ìœ„í•œ í”„ë ˆì„ ì „ì²˜ë¦¬ (ì—¬ëŸ¬ ë²„ì „ ìƒì„±)"""
        preprocessed = []
        
        # 1. ì›ë³¸ ê·¸ë ˆì´ìŠ¤ì¼€ì¼
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame.copy()
        preprocessed.append(gray)
        
        # 2. ëŒ€ë¹„ í–¥ìƒ
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        preprocessed.append(enhanced)
        
        # 3. ì´ì§„í™” (OTSU)
        _, binary1 = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        preprocessed.append(binary1)
        
        # 4. ì´ì§„í™” (ì ì‘ì )
        binary2 = cv2.adaptiveThreshold(enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY, 11, 2)
        preprocessed.append(binary2)
        
        # 5. ì—£ì§€ ê°•ì¡°
        edges = cv2.Canny(enhanced, 50, 150)
        preprocessed.append(edges)
        
        return preprocessed
    
    def extract_text_features(self, frame: np.ndarray) -> List[TextFeature]:
        """í”„ë ˆì„ì—ì„œ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        frame_area = frame.shape[0] * frame.shape[1]
        
        try:
            # ì—¬ëŸ¬ ì „ì²˜ë¦¬ ë²„ì „ìœ¼ë¡œ OCR ìˆ˜í–‰
            preprocessed_frames = self.preprocess_for_ocr(frame)
            
            all_detections = []
            
            for proc_frame in preprocessed_frames:
                try:
                    # OCR ë°ì´í„° ì¶”ì¶œ
                    data = pytesseract.image_to_data(
                        proc_frame, 
                        config=self.ocr_config,
                        output_type=pytesseract.Output.DICT
                    )
                    
                    # ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
                    for i in range(len(data['text'])):
                        if int(data['conf'][i]) > 30:  # ì‹ ë¢°ë„ 30 ì´ìƒ
                            text = data['text'][i].strip()
                            if len(text) >= 2:  # 2ê¸€ì ì´ìƒ
                                bbox = (
                                    int(data['left'][i]),
                                    int(data['top'][i]),
                                    int(data['width'][i]),
                                    int(data['height'][i])
                                )
                                all_detections.append({
                                    'text': text,
                                    'confidence': int(data['conf'][i]),
                                    'bbox': bbox
                                })
                                
                except Exception as e:
                    self.logger.debug(f"OCR ì²˜ë¦¬ ì‹¤íŒ¨ (ì „ì²˜ë¦¬ ë²„ì „): {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±° ë° íŠ¹ì§• ê³„ì‚°
            unique_texts = self._remove_duplicate_detections(all_detections)
            
            for detection in unique_texts:
                text = detection['text']
                confidence = detection['confidence']
                bbox = detection['bbox']
                
                # í…ìŠ¤íŠ¸ íŠ¹ì§• ê³„ì‚°
                char_count = len(text)
                word_count = len(text.split())
                
                # ìˆ«ì ë¹„ìœ¨
                digit_count = sum(1 for c in text if c.isdigit())
                digit_ratio = digit_count / char_count if char_count > 0 else 0
                
                # ê¸°í˜¸ ë¹„ìœ¨
                symbol_count = sum(1 for c in text if c in string.punctuation)
                symbol_ratio = symbol_count / char_count if char_count > 0 else 0
                
                # ì˜ì—­ ë¹„ìœ¨
                text_area = bbox[2] * bbox[3]
                area_ratio = text_area / frame_area
                
                feature = TextFeature(
                    text=text,
                    confidence=confidence / 100.0,  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    bbox=bbox,
                    char_count=char_count,
                    word_count=word_count,
                    digit_ratio=digit_ratio,
                    symbol_ratio=symbol_ratio,
                    area_ratio=area_ratio
                )
                
                features.append(feature)
                
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return features
    
    def _remove_duplicate_detections(self, detections: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µëœ í…ìŠ¤íŠ¸ ê²€ì¶œ ì œê±°"""
        if not detections:
            return []
        
        # í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹í™”
        groups = defaultdict(list)
        
        for detection in detections:
            text = detection['text'].lower()
            x, y = detection['bbox'][:2]
            # ê·¼ì ‘í•œ ìœ„ì¹˜ì˜ ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë£¹í™”
            key = (text, x // 10, y // 10)  # 10í”½ì…€ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
            groups[key].append(detection)
        
        # ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ì‹ ë¢°ë„ê°€ ë†’ì€ ê²ƒ ì„ íƒ
        unique_detections = []
        for group in groups.values():
            best = max(group, key=lambda x: x['confidence'])
            unique_detections.append(best)
        
        return unique_detections
    
    def analyze_poker_relevance(self, features: List[TextFeature]) -> Dict:
        """í…ìŠ¤íŠ¸ íŠ¹ì§•ì˜ í¬ì»¤ ê´€ë ¨ì„± ë¶„ì„"""
        if not features:
            return {
                'poker_score': 0.0,
                'keyword_matches': [],
                'pattern_matches': [],
                'total_confidence': 0.0
            }
        
        keyword_matches = []
        pattern_matches = []
        total_confidence = 0.0
        
        for feature in features:
            text_lower = feature.text.lower()
            total_confidence += feature.confidence
            
            # í¬ì»¤ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in self.poker_keywords:
                if keyword in text_lower:
                    keyword_matches.append({
                        'keyword': keyword,
                        'text': feature.text,
                        'confidence': feature.confidence,
                        'bbox': feature.bbox
                    })
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern_name, pattern in self.patterns.items():
                matches = re.findall(pattern, feature.text, re.IGNORECASE)
                if matches:
                    pattern_matches.append({
                        'pattern': pattern_name,
                        'matches': matches,
                        'text': feature.text,
                        'confidence': feature.confidence
                    })
        
        # í¬ì»¤ ì ìˆ˜ ê³„ì‚°
        keyword_score = len(keyword_matches) * 0.3
        pattern_score = len(pattern_matches) * 0.4
        confidence_score = (total_confidence / len(features)) * 0.3 if features else 0
        
        poker_score = min(keyword_score + pattern_score + confidence_score, 1.0)
        
        return {
            'poker_score': poker_score,
            'keyword_matches': keyword_matches,
            'pattern_matches': pattern_matches,
            'total_confidence': total_confidence / len(features) if features else 0,
            'text_density': len(features)
        }
    
    def compute_text_density_features(self, features: List[TextFeature], 
                                    frame_shape: Tuple[int, int]) -> Dict:
        """í…ìŠ¤íŠ¸ ë°€ë„ ê¸°ë°˜ íŠ¹ì§• ê³„ì‚°"""
        if not features:
            return {
                'text_density': 0.0,
                'avg_text_size': 0.0,
                'text_distribution': 'empty',
                'dominant_regions': []
            }
        
        height, width = frame_shape[:2]
        
        # í…ìŠ¤íŠ¸ ë°€ë„ ê³„ì‚°
        total_text_area = sum(f.bbox[2] * f.bbox[3] for f in features)
        frame_area = width * height
        text_density = total_text_area / frame_area
        
        # í‰ê·  í…ìŠ¤íŠ¸ í¬ê¸°
        avg_text_size = total_text_area / len(features)
        
        # ê³µê°„ì  ë¶„í¬ ë¶„ì„ (ê·¸ë¦¬ë“œ ê¸°ë°˜)
        grid_size = 4
        grid_width = width // grid_size
        grid_height = height // grid_size
        
        grid_counts = np.zeros((grid_size, grid_size))
        
        for feature in features:
            x, y = feature.bbox[:2]
            grid_x = min(x // grid_width, grid_size - 1)
            grid_y = min(y // grid_height, grid_size - 1)
            grid_counts[grid_y, grid_x] += 1
        
        # ì§€ë°°ì ì¸ ì˜ì—­ ì°¾ê¸°
        max_count = np.max(grid_counts)
        dominant_regions = []
        
        if max_count > 0:
            for i in range(grid_size):
                for j in range(grid_size):
                    if grid_counts[i, j] >= max_count * 0.7:  # ìµœëŒ€ê°’ì˜ 70% ì´ìƒ
                        dominant_regions.append((j, i))  # (x, y) ìˆœì„œ
        
        # ë¶„í¬ íŒ¨í„´ ë¶„ë¥˜
        if len(dominant_regions) == 1:
            distribution = 'concentrated'
        elif len(dominant_regions) <= 3:
            distribution = 'clustered'
        else:
            distribution = 'distributed'
        
        return {
            'text_density': text_density,
            'avg_text_size': avg_text_size / frame_area,  # ì •ê·œí™”
            'text_distribution': distribution,
            'dominant_regions': dominant_regions,
            'grid_counts': grid_counts.tolist()
        }
    
    def create_gfx_text_profile(self, gfx_samples: List[Dict]) -> GFXTextProfile:
        """GFX ìƒ˜í”Œë“¤ë¡œë¶€í„° í…ìŠ¤íŠ¸ í”„ë¡œí•„ ìƒì„±"""
        all_keywords = []
        all_patterns = []
        all_confidences = []
        char_counter = Counter()
        spatial_positions = defaultdict(list)
        
        for sample in gfx_samples:
            if 'frame_data' not in sample:
                continue
            
            frame = sample['frame_data']
            features = self.extract_text_features(frame)
            analysis = self.analyze_poker_relevance(features)
            
            # í‚¤ì›Œë“œ ìˆ˜ì§‘
            for match in analysis['keyword_matches']:
                all_keywords.append(match['keyword'])
            
            # íŒ¨í„´ ìˆ˜ì§‘
            for match in analysis['pattern_matches']:
                all_patterns.append(match['pattern'])
            
            # ì‹ ë¢°ë„ ìˆ˜ì§‘
            all_confidences.append(analysis['total_confidence'])
            
            # ë¬¸ì ë¶„í¬ ìˆ˜ì§‘
            for feature in features:
                for char in feature.text.lower():
                    char_counter[char] += 1
            
            # ê³µê°„ì  ë¶„í¬ ìˆ˜ì§‘
            density_features = self.compute_text_density_features(features, frame.shape)
            for region in density_features['dominant_regions']:
                spatial_positions[sample.get('label', 'unknown')].append(region)
        
        # ê³µí†µ í‚¤ì›Œë“œ (ë¹ˆë„ ê¸°ë°˜)
        keyword_counter = Counter(all_keywords)
        common_keywords = set(keyword for keyword, count in keyword_counter.most_common(10))
        
        # ì¼ë°˜ì ì¸ íŒ¨í„´
        pattern_counter = Counter(all_patterns)
        typical_patterns = [pattern for pattern, count in pattern_counter.most_common(5)]
        
        # í…ìŠ¤íŠ¸ ë°€ë„ ë²”ìœ„
        if all_confidences:
            min_conf = min(all_confidences)
            max_conf = max(all_confidences)
            text_density_range = (min_conf, max_conf)
            avg_confidence = sum(all_confidences) / len(all_confidences)
        else:
            text_density_range = (0.0, 0.0)
            avg_confidence = 0.0
        
        # ë¬¸ì ë¶„í¬ ì •ê·œí™”
        total_chars = sum(char_counter.values())
        char_distribution = {
            char: count / total_chars 
            for char, count in char_counter.most_common(50)
        } if total_chars > 0 else {}
        
        profile = GFXTextProfile(
            common_keywords=common_keywords,
            typical_patterns=typical_patterns,
            text_density_range=text_density_range,
            avg_confidence=avg_confidence,
            char_distribution=char_distribution,
            spatial_distribution=dict(spatial_positions)
        )
        
        self.gfx_profile = profile
        self.logger.info(f"GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„ ìƒì„± ì™„ë£Œ: {len(common_keywords)}ê°œ í‚¤ì›Œë“œ")
        
        return profile
    
    def classify_frame_by_text(self, frame: np.ndarray, 
                             visual_score: float = 0.5) -> Dict:
        """í…ìŠ¤íŠ¸ ë¶„ì„ê³¼ ì‹œê°ì  ë¶„ì„ì„ ê²°í•©í•œ í”„ë ˆì„ ë¶„ë¥˜"""
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
        text_features = self.extract_text_features(frame)
        
        # í¬ì»¤ ê´€ë ¨ì„± ë¶„ì„
        poker_analysis = self.analyze_poker_relevance(text_features)
        
        # í…ìŠ¤íŠ¸ ë°€ë„ íŠ¹ì§•
        density_features = self.compute_text_density_features(text_features, frame.shape)
        
        # GFX í”„ë¡œí•„ê³¼ ë¹„êµ (í•™ìŠµëœ ê²½ìš°)
        profile_score = 0.0
        if self.gfx_profile:
            profile_score = self._compare_with_profile(text_features, poker_analysis)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ + ì‹œê°ì )
        text_score = (
            poker_analysis['poker_score'] * 0.4 +
            (poker_analysis['total_confidence'] / 100) * 0.3 +
            density_features['text_density'] * 100 * 0.2 +  # ë°€ë„ ì ìˆ˜ ì¡°ì •
            profile_score * 0.1
        )
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_score = (text_score * 0.6 + visual_score * 0.4)
        
        # GFX ì—¬ë¶€ íŒì •
        is_gfx = final_score > 0.5
        
        return {
            'is_gfx': is_gfx,
            'confidence': final_score,
            'text_score': text_score,
            'visual_score': visual_score,
            'poker_analysis': poker_analysis,
            'density_features': density_features,
            'text_features_count': len(text_features),
            'method': 'text+visual'
        }
    
    def _compare_with_profile(self, features: List[TextFeature], 
                            analysis: Dict) -> float:
        """í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ í•™ìŠµëœ í”„ë¡œí•„ê³¼ ë¹„êµ"""
        if not self.gfx_profile or not features:
            return 0.0
        
        score = 0.0
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        matched_keywords = set(match['keyword'] for match in analysis['keyword_matches'])
        keyword_overlap = len(matched_keywords & self.gfx_profile.common_keywords)
        if self.gfx_profile.common_keywords:
            score += (keyword_overlap / len(self.gfx_profile.common_keywords)) * 0.4
        
        # ì‹ ë¢°ë„ ë²”ìœ„ ì ìˆ˜
        avg_conf = analysis['total_confidence']
        if (self.gfx_profile.text_density_range[0] <= avg_conf <= 
            self.gfx_profile.text_density_range[1]):
            score += 0.3
        
        # ë¬¸ì ë¶„í¬ ìœ ì‚¬ë„
        if self.gfx_profile.char_distribution:
            feature_chars = Counter()
            for feature in features:
                for char in feature.text.lower():
                    feature_chars[char] += 1
            
            total_chars = sum(feature_chars.values())
            if total_chars > 0:
                similarity = 0.0
                for char, count in feature_chars.most_common(20):
                    char_ratio = count / total_chars
                    profile_ratio = self.gfx_profile.char_distribution.get(char, 0)
                    similarity += min(char_ratio, profile_ratio)
                
                score += similarity * 0.3
        
        return min(score, 1.0)
    
    def save_profile(self, output_path: str):
        """GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„ ì €ì¥"""
        if not self.gfx_profile:
            raise ValueError("ì €ì¥í•  GFX í”„ë¡œí•„ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # Setì„ listë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)
        profile_dict = asdict(self.gfx_profile)
        profile_dict['common_keywords'] = list(self.gfx_profile.common_keywords)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(profile_dict, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    def load_profile(self, profile_path: str):
        """GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„ ë¡œë“œ"""
        with open(profile_path, 'r', encoding='utf-8') as f:
            profile_dict = json.load(f)
        
        # listë¥¼ setìœ¼ë¡œ ë³€í™˜
        profile_dict['common_keywords'] = set(profile_dict['common_keywords'])
        
        self.gfx_profile = GFXTextProfile(**profile_dict)
        self.logger.info(f"GFX í…ìŠ¤íŠ¸ í”„ë¡œí•„ ë¡œë“œ ì™„ë£Œ: {profile_path}")


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GFX í…ìŠ¤íŠ¸ ë¶„ì„ê¸°')
    parser.add_argument('image_path', help='ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--profile', '-p', help='GFX í”„ë¡œí•„ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--debug', '-d', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # í…ìŠ¤íŠ¸ ë¶„ì„ê¸° ìƒì„±
        analyzer = GFXTextAnalyzer()
        
        # í”„ë¡œí•„ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if args.profile and Path(args.profile).exists():
            analyzer.load_profile(args.profile)
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        frame = cv2.imread(args.image_path)
        if frame is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.image_path}")
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰
        features = analyzer.extract_text_features(frame)
        poker_analysis = analyzer.analyze_poker_relevance(features)
        density_features = analyzer.compute_text_density_features(features, frame.shape)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š GFX í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:")
        print(f"â€¢ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìˆ˜: {len(features)}")
        print(f"â€¢ í¬ì»¤ ê´€ë ¨ì„± ì ìˆ˜: {poker_analysis['poker_score']:.3f}")
        print(f"â€¢ í…ìŠ¤íŠ¸ ë°€ë„: {density_features['text_density']:.6f}")
        print(f"â€¢ í‰ê·  ì‹ ë¢°ë„: {poker_analysis['total_confidence']:.1f}%")
        
        if poker_analysis['keyword_matches']:
            print(f"\nğŸ¯ ë§¤ì¹­ëœ í¬ì»¤ í‚¤ì›Œë“œ:")
            for match in poker_analysis['keyword_matches'][:5]:
                print(f"  - '{match['keyword']}' in '{match['text']}' (ì‹ ë¢°ë„: {match['confidence']:.1f}%)")
        
        if poker_analysis['pattern_matches']:
            print(f"\nğŸ” ë§¤ì¹­ëœ íŒ¨í„´:")
            for match in poker_analysis['pattern_matches'][:3]:
                print(f"  - {match['pattern']}: {match['matches']}")
        
        # ê²°ê³¼ ì €ì¥
        if args.output:
            result_data = {
                'image_path': args.image_path,
                'text_features': [asdict(f) for f in features],
                'poker_analysis': poker_analysis,
                'density_features': density_features
            }
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {args.output}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())